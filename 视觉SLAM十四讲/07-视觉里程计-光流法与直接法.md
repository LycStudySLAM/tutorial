# 视觉里程计-光流法与直接法

以下内容为高翔《视觉SLAM十四讲》第二版的学习笔记

## 回顾

上一讲我们基于特征点法对视觉里程计进行了简单的实现，以特征点在两幅图像的投影位置为基础，根据相机信息不同，分别通过对极几何，PnP，ICP三类方法对相机运动进行了估计，并据此给出了特征点的空间点位置（路标）。尽管基于特征点法的视觉里程计的确实现了功能，但很明显还有一些问题：①特征点的提取耗时较长，限制了帧率的提升。②丢弃了除特征点外图像具有的大量其他信息。③面对不具有明显纹理的地方（比如走廊，墙面），特征点数量会过少，导致匹配点不足或是足够但较少导致估计误差较大。

为了解决上面提到的问题，我们有两种思路：

①保留特征点这一概念，提取关键点，不计算描述子，而是使用**光流法**跟踪特征点的运动（相当于完成了计算描述子与匹配的过程）。光流法的所需时间少于计算描述子的时间。该方法仍然需要提取FAST关键点，估计相机运动仍然需要上一讲中对极几何，PnP，ICP等问题的求解过程。本质上，光流法与特征点法的目标都是**最小化重投影误差**。

②提取关键点，根据像素灰度信息同时估计相机运动和点的投影。该方法称为**直接法**，其目标是**最小化光度误差**。直接法无需特征点法的提取特征点、计算描述子、匹配特征点的过程，并且只需要场景有一定的明暗变化（不是接近完全相同的梯度）就可以工作。

## 光流法

光流是一种描述像素随时间在图像之间运动的方法。计算部分像素运动的称为稀疏光流，计算全部像素运动的称为稠密光流。用于SLAM的主要就是稀疏光流，其中最经典的是LK（Lucas-Kanade）光流。

![07-1](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\07-1.png)

LK光流认为相机的图像会随时间变化，图像可以看作时间的函数，其中I表示灰度，(x,y)表示像素坐标，t表示时间。函数如下
$$
I(x,y,t)
$$
现在考虑一个固定的空间点，在t时刻时图像投影在(x,y)，显而易见，随着相机的运动，投影位置会发生变化，现在我们希望估计出这个空间点在下一时刻的投影在何处（不知道读者有没有发现，这一步代替了特征点法中计算描述子与匹配的步骤，完成了匹配过程）。这里需要先提出光流法的基本假设：

①**灰度不变假设**：同一空间点的像素灰度值，在各个图像中是固定不变的。即
$$
I(x+dx,y+dy,t+dt)=I(x,y,t)\\
dt为两帧图像相差的时间,dx,dy为空间点投影位置的变化量
$$
不得不说这是一个很强的假设，不同的材料的反射率，相机曝光参数和外界光照强度都可以使这个假设不成立，不过考虑到我们只需要两帧之间的变化，一帧时间内的变化也较为有限，暂且认为这个假设成立。

对上式左侧进行泰勒展开并保留一阶项
$$
I(x+dx,y+dy,t+dt)\approx I(x,y,t)+
\frac{\partial I}{\partial x}dx+
\frac{\partial I}{\partial y}dy+
\frac{\partial I}{\partial t}dt
$$
基于之前的灰度不变假设，有
$$
\frac{\partial I}{\partial x}dx+
\frac{\partial I}{\partial y}dy+
\frac{\partial I}{\partial t}dt=0\\
两边同除dt,\;
\frac{\partial I}{\partial x}\frac{dx}{dt}+
\frac{\partial I}{\partial y}\frac{dy}{dt}=
-\frac{\partial I}{\partial t}\\
记I_x=\frac{\partial I}{\partial x},I_y=\frac{\partial I}{\partial y}为I的x,y方向的梯度,
I_t=\frac{\partial I}{\partial t}为灰度对时间的变化率\\
记u=\frac{dx}{dt},v=\frac{dy}{dt}为x轴,y轴方向上的运动速度\\
则有:
\begin{bmatrix}I_x&I_y\end{bmatrix}
\begin{bmatrix}u\\v\end{bmatrix}=-I_t
$$
现在，我们计算的目标正是像素的运动速度u,v。但是目前仅有一个方程，无法完成这个两变量方程的求解。这就要引入第二个假设。

②**相同运动假设**：同一窗口的所有像素具有相同的运动。即我们选定一个窗口，其内的所有像素运动是一致的。

据此，我们可以构建下面的方程组。
$$
对w\times w的窗口,\;
\begin{bmatrix}I_x&I_y\end{bmatrix}_k
\begin{bmatrix}u\\v\end{bmatrix}=-I_{t_k},k=1,2,...w^2
$$
我们记
$$
A=\begin{bmatrix}[I_x\;I_y]_1\\...\\\;[I_x\;I_y]_k\end{bmatrix},
b=\begin{bmatrix}I_{t_1}\\...\\I_{t_k}\end{bmatrix},
$$
则有
$$
A\begin{bmatrix}u\\v\end{bmatrix}=-b\\
\rightarrow\begin{bmatrix}u\\v\end{bmatrix}^*=-(A^TA)^{-1}A^Tb
$$
最后一步是超定线性方程最小二乘解的结果，在05-非线性优化一讲中也曾提到过。

## 直接法

考虑空间点P和两个时刻的相机及其投影

![07-2](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\07-2.png)

我们的目标是求解第一帧到第二帧的位姿变换，以第一帧为参考系，求出第二帧的R，t。数学建模如下
$$
记点P=[X,Y,Z]^T,投影的像素坐标p1,p2\\
p_1=\begin{bmatrix}u\\v\\1\end{bmatrix}_1=\frac{1}{Z_1}KP,\;
p_2=\begin{bmatrix}u\\v\\1\end{bmatrix}_2=\frac{1}{Z_2}K(RP+t)=\frac{1}{Z_2}K(TP)_{1:3}
$$
最后一步是齐次坐标与非齐次坐标的变换，相信大家都对投影方程非常熟悉了，这里不多赘述。

特征点法中，我们通过匹配描述子得到同一空间点的两个投影之间的匹配关系，但采用直接法，我们无法知道第二帧的哪个像素对应到第一帧的这个投影点，这时，考虑到我们之前提到的灰度不变假设，我们理应**寻找一个合适的位姿变换**，**使得第一帧的投影点经过这样的欧式变换后，对应的第二帧上的点与第一帧的投影点的灰度误差最小**。我们定义下面这个误差项为**光度误差**。
$$
e=I_1(p_1)-I_2(p_2)
$$
对于多个点，我们可以构建最小二乘问题
$$
\min_TJ(T)=\sum_{i=1}^Ne_i^Te_i,\;e_i=I_1(p_{1,i})-I_2(p_{2,i})
$$
优化的目标是得到使光度误差最小的位姿T。根据惯例，我们需要求解误差项关于位姿的一阶导数。

首先，我们先定义两个中间变量，以便采用链式法则来简化求导的过程。
$$
q=TP,u=\frac{1}{Z_2}Kq\\
其中q是P在第二个相机坐标系下的坐标,u是投影到第二帧上的像素坐标
$$
我们仍然采用李代数的左扰动模型，先列出待求导的关系式
$$
e(T)=I_1(p_1)-I_2(u)
$$
则有
$$
\frac{\partial e}{\partial \delta\xi}=
-\frac{\partial I_2}{\partial u}
\frac{\partial u}{\partial q}
\frac{\partial q}{\partial \delta\xi}
$$
后两项我们再上一讲光束法平差部分详细求解过，现直接将结果列出
$$
q=[X_c,Y_c,Z_c]^T\\
\frac{\partial u}{\partial q}=
\begin{bmatrix}
\frac{f_x}{Z_c}&0&-\frac{f_xX_c}{Z_c^2}\\
0&\frac{f_y}{Z_c}&-\frac{f_yY_c}{Z_c^2}
\end{bmatrix}\\
\frac{\partial q}{\partial \delta\xi}=\begin{bmatrix}I&-P_c^{\wedge}\end{bmatrix}\\
\frac{\partial u}{\partial \delta\xi}=
\frac{\partial u}{\partial q}\frac{\partial q}{\partial \delta\xi}=
\begin{bmatrix}
\frac{f_x}{Z_c}&0&-\frac{f_xX_c}{Z_c^2}&
-\frac{f_xX_cY_c}{Z_c^2}&f_x+\frac{f_xX_c^2}{Z_c^2}&-\frac{f_xY_c}{Z_c}\\
0&\frac{f_y}{Z_c}&-\frac{f_yY_c}{Z_c^2}&
-f_y-\frac{f_yY_c^2}{Z_c^2}&\frac{f_yX_cY_c}{Z_c^2}&-\frac{f_yX_c}{Z_c}
\end{bmatrix}
$$
第一项则是光流法中提到的对于位置u的灰度像素梯度。综上所述，雅可比矩阵为
$$
J=-\frac{\partial I_2}{\partial u}\frac{\partial u}{\partial \delta\xi}
$$
读者务必注意，我们这里提到的空间点P是深度已知的，读者可以把它当做是RGB-D或是双目相机获得的数据。对于P的选取，我们有三种方式：①P选取自稀疏关键点，通过关键点提取完成，速度非常快。②P选取自具有像素梯度的像素，可以完成半稠密重建。③P来自所有像素，消耗资源大，可以完成稠密重建，但一般不用在SLAM中，在三维重建中也许更为合适。



## 实践模块

特别说明一下，这一章需要用到OpenCV4，如果之前安装OpenCV3的读者可以根据下面的方式进行修改以完成编译与运行。

①以4.5.4版本为例，从https://github.com/opencv/opencv/releases/tag/4.5.4进行源码下载与编译

②在ch8下的CMakeLists.txt中的

```cmake
find_package(OpenCV 4 REQUIRED)
```

之前添加下面的指令

```cmake
#example:"/home/${your_name}/Downloads/opencv-4.5.4/build"换为你的OpenCV build路径
set(OpenCV_DIR "/home/${your_name}/Downloads/opencv-4.5.4/build")
```

③由于OpenCV版本的更迭，如果遇到下面的问题，将CV_GRAY2BGR改为cv::COLOR_GRAY2BGR即可。

![07-3](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\07-3.png)

接下来就开始分析代码。

### 光流法

我们首先看看主函数写了些什么。解释我都写在注释当中了。

```C++
// 读取图像,img1,img2分别为第一帧和第二帧,它们都是灰度图
Mat img1 = imread(file_1, 0);
Mat img2 = imread(file_2, 0);

// 提取第一帧中的GFTT关键点(至于为什么高翔没有采用之前的FAST关键点,我也不知道)
vector<KeyPoint> kp1;
Ptr<GFTTDetector> detector = GFTTDetector::create(500, 0.01, 20); //最多提取500个关键点
detector->detect(img1, kp1);

// 现在在第二帧图像上进行跟踪
// 首先采用单层光流法
vector<KeyPoint> kp2_single;
vector<bool> success_single;
OpticalFlowSingleLevel(img1, img2, kp1, kp2_single, success_single);

// 再采用多层光流法
vector<KeyPoint> kp2_multi;
vector<bool> success_multi;
chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
OpticalFlowMultiLevel(img1, img2, kp1, kp2_multi, success_multi, true);
chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
auto time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
cout << "optical flow by gauss-newton: " << time_used.count() << endl;

// 使用OpenCV提供的LK光流法
vector<Point2f> pt1, pt2;
for (auto &kp: kp1) pt1.push_back(kp.pt);
vector<uchar> status;
vector<float> error;
t1 = chrono::steady_clock::now();
cv::calcOpticalFlowPyrLK(img1, img2, pt1, pt2, status, error);
t2 = chrono::steady_clock::now();
time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
cout << "optical flow by opencv: " << time_used.count() << endl;

// 显示，观察效果
Mat img2_single;
cv::cvtColor(img2, img2_single, cv::COLOR_GRAY2BGR);
//对于对应点进行遍历
for (int i = 0; i < kp2_single.size(); i++) {
    //如果成功跟踪
    if (success_single[i]) {
        //画出跟踪到的点,并且将原来第一帧的点与其连线
        cv::circle(img2_single, kp2_single[i].pt, 2, cv::Scalar(0, 250, 0), 2);
        cv::line(img2_single, kp1[i].pt, kp2_single[i].pt, cv::Scalar(0, 250, 0));
    }
}

//与上面的过程完全一致
Mat img2_multi;
cv::cvtColor(img2, img2_multi, cv::COLOR_GRAY2BGR);
for (int i = 0; i < kp2_multi.size(); i++) {
    if (success_multi[i]) {
        cv::circle(img2_multi, kp2_multi[i].pt, 2, cv::Scalar(0, 250, 0), 2);
        cv::line(img2_multi, kp1[i].pt, kp2_multi[i].pt, cv::Scalar(0, 250, 0));
    }
}

//与上面的过程完全一致
Mat img2_CV;
cv::cvtColor(img2, img2_CV, cv::COLOR_GRAY2BGR);
for (int i = 0; i < pt2.size(); i++) {
    if (status[i]) {
        cv::circle(img2_CV, pt2[i], 2, cv::Scalar(0, 250, 0), 2);
        cv::line(img2_CV, pt1[i], pt2[i], cv::Scalar(0, 250, 0));
    }
}
//显示
cv::imshow("tracked single level", img2_single);
cv::imshow("tracked multi level", img2_multi);
cv::imshow("tracked by opencv", img2_CV);
cv::waitKey(0);
```

从主函数我们可以看出，整个过程使用了OpenCV的LK光流法，自己写的单层光流法和多层光流法，完成了匹配的过程就像特征点法完成了描述子的计算后匹配一样。同时对OpenCV的LK光流法和自己写的多层光流法进行了计时，现在，我们主要看自己实现的单层光流OpticalFlowSingleLevel和多层光流OpticalFlowMultiLevel。

```C++
void OpticalFlowSingleLevel(
    const Mat &img1,
    const Mat &img2,
    const vector<KeyPoint> &kp1,
    vector<KeyPoint> &kp2,
    vector<bool> &success,
    bool inverse, bool has_initial)
{
    kp2.resize(kp1.size());
    success.resize(kp1.size());
    //创建了一个OpticalFlowTracker对象
    OpticalFlowTracker tracker(img1, img2, kp1, kp2, success, inverse, has_initial);
    //调用OpenCV的并行for循环（这个for循环不保证有序，但光流对这些点处理本身无需有序）,加速运行
    parallel_for_(
        //确定循环的Range并绑定循环工作的函数,将Range通过占位符传入函数中
        Range(0, kp1.size()),
        std::bind(&OpticalFlowTracker::calculateOpticalFlow, &tracker, placeholders::_1)
    );
}

class OpticalFlowTracker {
public:
    OpticalFlowTracker(
        const Mat &img1_,
        const Mat &img2_,
        const vector<KeyPoint> &kp1_,
        vector<KeyPoint> &kp2_,
        vector<bool> &success_,
        bool inverse_ = true, bool has_initial_ = false) :
        img1(img1_), img2(img2_), kp1(kp1_), kp2(kp2_), success(success_), inverse(inverse_),
        has_initial(has_initial_) {}

    void calculateOpticalFlow(const Range &range);

private:
    const Mat &img1;
    const Mat &img2;
    const vector<KeyPoint> &kp1;
    vector<KeyPoint> &kp2;
    vector<bool> &success;
    bool inverse = true;
    bool has_initial = false;
};

void OpticalFlowTracker::calculateOpticalFlow(const Range &range) {
    // parameters
    int half_patch_size = 4;
    int iterations = 10;
    for (size_t i = range.start; i < range.end; i++) {
        auto kp = kp1[i];
        // dx,dy 为待估计的值
        double dx = 0, dy = 0; 
        // 计算dx,dy的值
        if (has_initial) {
            dx = kp2[i].pt.x - kp.pt.x;
            dy = kp2[i].pt.y - kp.pt.y;
        }

        double cost = 0, lastCost = 0;
        bool succ = true;

        // 高斯牛顿法
        Eigen::Matrix2d H = Eigen::Matrix2d::Zero();
        Eigen::Vector2d g = Eigen::Vector2d::Zero();
        Eigen::Vector2d J;  // 雅可比矩阵
        for (int iter = 0; iter < iterations; iter++) {
            //初始化过程
            if (inverse == false) {
                H = Eigen::Matrix2d::Zero();
                g = Eigen::Vector2d::Zero();
            } else {
                g = Eigen::Vector2d::Zero();
            }

            cost = 0;
			
            // 计算误差项和雅可比矩阵
            //GetPixelValue为内联的方法，用于获取某一个位置的灰度
            for (int x = -half_patch_size; x < half_patch_size; x++)
                for (int y = -half_patch_size; y < half_patch_size; y++) {
                    double error = GetPixelValue(img1, kp.pt.x + x, kp.pt.y + y) -
                                   GetPixelValue(img2, kp.pt.x + x + dx, kp.pt.y + y + dy);
                    if (inverse == false) {
                        //这里面其实是在求像素梯度,这正是雅可比矩阵的构成
                        J = -1.0 * Eigen::Vector2d(
                            0.5 * (GetPixelValue(img2, kp.pt.x + dx + x + 1, kp.pt.y + dy + y) -
                                   GetPixelValue(img2, kp.pt.x + dx + x - 1, kp.pt.y + dy + y)),
                            0.5 * (GetPixelValue(img2, kp.pt.x + dx + x, kp.pt.y + dy + y + 1) -
                                   GetPixelValue(img2, kp.pt.x + dx + x, kp.pt.y + dy + y - 1))
                        );
                    } else if (iter == 0) {
                        J = -1.0 * Eigen::Vector2d(
                            0.5 * (GetPixelValue(img1, kp.pt.x + x + 1, kp.pt.y + y) -
                                   GetPixelValue(img1, kp.pt.x + x - 1, kp.pt.y + y)),
                            0.5 * (GetPixelValue(img1, kp.pt.x + x, kp.pt.y + y + 1) -
                                   GetPixelValue(img1, kp.pt.x + x, kp.pt.y + y - 1))
                        );
                    }
					//计算
                    g += -error * J;
                    cost += error * error;
                    if (inverse == false || iter == 0) {
                        // also update H
                        H += J * J.transpose();
                    }
                }

            // 计算更新量
            Eigen::Vector2d update = H.ldlt().solve(g);
            
            // 设置退出迭代条件
            if (std::isnan(update[0])) {
                cout << "update is nan" << endl;
                succ = false;
                break;
            }

            if (iter > 0 && cost > lastCost) {
                break;
            }
            
            // 更新
            dx += update[0];
            dy += update[1];
            lastCost = cost;
            succ = true;

            if (update.norm() < 1e-2) {
                break;
            }
        }
		//保存结果
        success[i] = succ;
        kp2[i].pt = kp.pt + Point2f(dx, dy);
    }
}
```

这里有必要说明一下，代码和之前的分析采用的方法并不一致，这里的最小二乘问题是
$$
\min_{dx,dy}\sum_{i=1}^N(I_1(x_i,y_i)-I_2(x_i+dx,y_i+dy))
$$
通过优化迭代得到使得这一误差项最小的像素运动(dx,dy)，这就完成了关键点之间的匹配。

这之中还有提到**反向光流法**，其将梯度用第一张图像的梯度来代替，这一方法节省了梯度计算所需要的资源。



再看多层光流法。所谓多层光流法，是为了解决相机运动过慢或过快导致两帧图像差异在像素上的体现太不明显或者太明显这一问题，它的基础思路正是图像金字塔，我们之前采用图像金字塔解决过关键点提取的问题，相信读者对这一概念十分熟悉。

```C++
void OpticalFlowMultiLevel(
    const Mat &img1,
    const Mat &img2,
    const vector<KeyPoint> &kp1,
    vector<KeyPoint> &kp2,
    vector<bool> &success,
    bool inverse) {

    // parameters
    int pyramids = 4; 							//金字塔层数
    double pyramid_scale = 0.5;					//分辨率变化
    double scales[] = {1.0, 0.5, 0.25, 0.125};	//分辨率

    // 构建金字塔,主要是cv::resize和线性插值
    chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
    vector<Mat> pyr1, pyr2;//图像金字塔
    for (int i = 0; i < pyramids; i++) {
        if (i == 0) {
            pyr1.push_back(img1);
            pyr2.push_back(img2);
        } else {
            Mat img1_pyr, img2_pyr;
            cv::resize(pyr1[i - 1], img1_pyr,
                       cv::Size(pyr1[i - 1].cols * pyramid_scale, pyr1[i - 1].rows * pyramid_scale));
            cv::resize(pyr2[i - 1], img2_pyr,
                       cv::Size(pyr2[i - 1].cols * pyramid_scale, pyr2[i - 1].rows * pyramid_scale));
            pyr1.push_back(img1_pyr);
            pyr2.push_back(img2_pyr);
        }
    }
    chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
    auto time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
    cout << "build pyramid time: " << time_used.count() << endl;

    // 计算金字塔顶层的关键点位置
    vector<KeyPoint> kp1_pyr, kp2_pyr;
    for (auto &kp:kp1) {
        auto kp_top = kp;
        kp_top.pt *= scales[pyramids - 1];
        kp1_pyr.push_back(kp_top);
        kp2_pyr.push_back(kp_top);
    }

    //对每一层金字塔进行单层光流法
    for (int level = pyramids - 1; level >= 0; level--) {
        success.clear();
        t1 = chrono::steady_clock::now();
        OpticalFlowSingleLevel(pyr1[level], pyr2[level], kp1_pyr, kp2_pyr, success, inverse, true);
        t2 = chrono::steady_clock::now();
        auto time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
        cout << "track pyr " << level << " cost time: " << time_used.count() << endl;

        if (level > 0) {
            for (auto &kp: kp1_pyr)
                kp.pt /= pyramid_scale;
            for (auto &kp: kp2_pyr)
                kp.pt /= pyramid_scale;
        }
    }

    //最终层,将关键点对应放入容器
    for (auto &kp: kp2_pyr)
        kp2.push_back(kp);
}
```

最后，我们来看一看光流法的结果

![07-4](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\07-4.png)

可以看出，OpenCV光流与手写光流法的结果还是比较接近的。

单层光流则出现了一些像素移动较大的点，一定程度上会使得准确度下降。

![07-5](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\07-5.png)

最后，我们看一下性能比较的结果。可以看出OpenCV用时多于手写多层光流法，不过都在5ms之下，远快于计算描述子及匹配的过程，这说明**光流法可以用于加速基于特征点法的视觉里程计**。

![07-6](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\07-6.png)

### 直接法

```C++
typedef vector<Eigen::Vector2d, Eigen::aligned_allocator<Eigen::Vector2d>> VecVector2d;
typedef Eigen::Matrix<double, 6, 6> Matrix6d;
typedef Eigen::Matrix<double, 2, 6> Matrix26d;
typedef Eigen::Matrix<double, 6, 1> Vector6d;

/// 用于并行化计算雅可比矩阵的类
class JacobianAccumulator {
public:
    JacobianAccumulator(
        const cv::Mat &img1_,
        const cv::Mat &img2_,
        const VecVector2d &px_ref_,
        const vector<double> depth_ref_,
        Sophus::SE3d &T21_) :
        img1(img1_), img2(img2_), px_ref(px_ref_), depth_ref(depth_ref_), T21(T21_) {
        projection = VecVector2d(px_ref.size(), Eigen::Vector2d(0, 0));
    }

    /// 在range范围内计算雅可比矩阵
    void accumulate_jacobian(const cv::Range &range);

    /// 高斯牛顿法中的H
    Matrix6d hessian() const { return H; }

    /// 高斯牛顿法中的g
    Vector6d bias() const { return b; }

    /// 总的代价值
    double cost_func() const { return cost; }

    /// 投影点集
    VecVector2d projected_points() const { return projection; }

    /// 重置H,g和cost
    void reset() {
        H = Matrix6d::Zero();
        b = Vector6d::Zero();
        cost = 0;
    }

private:
    const cv::Mat &img1;
    const cv::Mat &img2;
    const VecVector2d &px_ref;
    const vector<double> depth_ref;
    Sophus::SE3d &T21;
    VecVector2d projection; // 投影点集

    std::mutex hessian_mutex;
    Matrix6d H = Matrix6d::Zero();
    Vector6d b = Vector6d::Zero();
    double cost = 0;
};

// 雅可比矩阵的计算
void JacobianAccumulator::accumulate_jacobian(const cv::Range &range) {

    // parameters
    const int half_patch_size = 1;
    int cnt_good = 0;
    Matrix6d hessian = Matrix6d::Zero();
    Vector6d bias = Vector6d::Zero();
    double cost_tmp = 0;

    for (size_t i = range.start; i < range.end; i++) {

        // 由第一帧投影点,相机内参,深度->计算第一个相机坐标系下的空间点坐标
        Eigen::Vector3d point_ref =
            depth_ref[i] * Eigen::Vector3d((px_ref[i][0] - cx) / fx, (px_ref[i][1] - cy) / fy, 1);
        // 按照当前估计的位姿变换到第二帧相机下,得到第二个相机坐标系下的空间点坐标
        Eigen::Vector3d point_cur = T21 * point_ref;
        // 深度信息无效则跳过
        if (point_cur[2] < 0)
            continue;

        // 由投影方程倒推到点在第二个相机坐标系下的像素平面坐标
        float u = fx * point_cur[0] / point_cur[2] + cx, v = fy * point_cur[1] / point_cur[2] + cy;
        // 如果倒推结果不再视场内就跳过
        if (u < half_patch_size || u > img2.cols - half_patch_size || v < half_patch_size ||
            v > img2.rows - half_patch_size)
            continue;

        // 这里就成功获得了第二帧基于当前估计位姿变换的投影点
        projection[i] = Eigen::Vector2d(u, v);
        //X,Y,Z为第二个相机坐标系下的空间点坐标
        double X = point_cur[0], Y = point_cur[1], Z = point_cur[2],
            Z2 = Z * Z, Z_inv = 1.0 / Z, Z2_inv = Z_inv * Z_inv;
        cnt_good++;

        // 同一窗口内的运动方向是一致的
        for (int x = -half_patch_size; x <= half_patch_size; x++)
            for (int y = -half_patch_size; y <= half_patch_size; y++) {
				//正如我们前文讨论的误差项,对一个窗口x*y内的所有像素做计算
                double error = GetPixelValue(img1, px_ref[i][0] + x, px_ref[i][1] + y) -
                               GetPixelValue(img2, u + x, v + y);
                Matrix26d J_pixel_xi;
                Eigen::Vector2d J_img_pixel;
				//正如我们前文讨论的雅可比矩阵的后两项
                J_pixel_xi(0, 0) = fx * Z_inv;
                J_pixel_xi(0, 1) = 0;
                J_pixel_xi(0, 2) = -fx * X * Z2_inv;
                J_pixel_xi(0, 3) = -fx * X * Y * Z2_inv;
                J_pixel_xi(0, 4) = fx + fx * X * X * Z2_inv;
                J_pixel_xi(0, 5) = -fx * Y * Z_inv;

                J_pixel_xi(1, 0) = 0;
                J_pixel_xi(1, 1) = fy * Z_inv;
                J_pixel_xi(1, 2) = -fy * Y * Z2_inv;
                J_pixel_xi(1, 3) = -fy - fy * Y * Y * Z2_inv;
                J_pixel_xi(1, 4) = fy * X * Y * Z2_inv;
                J_pixel_xi(1, 5) = fy * X * Z_inv;
				
                //正如我门前文讨论的雅可比矩阵的第一项，这是像素梯度的求解
                //以右一像素-左一像素的二分之一作为x方向的梯度,y方向的梯度也是类似的
                J_img_pixel = Eigen::Vector2d(
                    0.5 * (GetPixelValue(img2, u + 1 + x, v + y) - GetPixelValue(img2, u - 1 + x, v + y)),
                    0.5 * (GetPixelValue(img2, u + x, v + 1 + y) - GetPixelValue(img2, u + x, v - 1 + y))
                );

                // 将前一项和后两项相乘，得到最终的雅可比矩阵
                Vector6d J = -1.0 * (J_img_pixel.transpose() * J_pixel_xi).transpose();

                //由于最小二乘问题是对多个误差项求和，所以将雅可比矩阵与其转置相乘结果加入H,对b和cost也是一样
                hessian += J * J.transpose();
                bias += -error * J;
                cost_tmp += error * error;
            }
    }

    if (cnt_good) {
        // set hessian, bias and cost
        unique_lock<mutex> lck(hessian_mutex);
        H += hessian;
        b += bias;
        cost += cost_tmp / cnt_good;
    }
}
```

这里看起来是直接自己手写一个优化器。（其实我并不理解为什么不使用g2o或者Ceres，这样在后续工程上会更实际一点，不过之后工程章节中我会用g2o和Ceres对此进行实现）

接下来看主函数

```C++
//这里用的是双目相机的数据
cv::Mat left_img = cv::imread(left_file, 0);//左相机的图
cv::Mat disparity_img = cv::imread(disparity_file, 0);//视差图

// 随机选取一些点，用于稍后的直接法(我本人更倾向于选择一定量关键点+一定量随机点来完成)
cv::RNG rng;
int nPoints = 2000;
int boarder = 20;
VecVector2d pixels_ref;
vector<double> depth_ref;

// generate pixels in ref and load depth data
for (int i = 0; i < nPoints; i++) {
    // 避免随机到过于边缘的点，以免不再下一帧的视场内
    int x = rng.uniform(boarder, left_img.cols - boarder);  
    int y = rng.uniform(boarder, left_img.rows - boarder);
    // 获取真实深度
    int disparity = disparity_img.at<uchar>(y, x);
    double depth = fx * baseline / disparity; 
    //将投影点和深度放入容器中
    depth_ref.push_back(depth);
    pixels_ref.push_back(Eigen::Vector2d(x, y));
}

// 对000001.png~000005.png五张图片分别估计
boost::format fmt_others("./%06d.png");
Sophus::SE3d T_cur_ref;

for (int i = 1; i < 6; i++) {
    cv::Mat img = cv::imread((fmt_others % i).str(), 0);
    DirectPoseEstimationSingleLayer(left_img, img, pixels_ref, depth_ref, T_cur_ref);//测试单层直接法
    DirectPoseEstimationMultiLayer(left_img, img, pixels_ref, depth_ref, T_cur_ref);//测试多层直接法
}
```

主函数看起来十分清晰，就是将投影点和空间点以及第一帧和第二帧图片传入，最后得到结果位姿，现在的重点就是单层直接法和多层直接法的两个函数的实现了。

```C++
//单层直接法
void DirectPoseEstimationSingleLayer(
    const cv::Mat &img1,
    const cv::Mat &img2,
    const VecVector2d &px_ref,
    const vector<double> depth_ref,
    Sophus::SE3d &T21) {

    const int iterations = 10;
    double cost = 0, lastCost = 0;
    auto t1 = chrono::steady_clock::now();
    // 生成一个优化器类
    JacobianAccumulator jaco_accu(img1, img2, px_ref, depth_ref, T21);

    for (int iter = 0; iter < iterations; iter++) {
        jaco_accu.reset();//重置
        // 并行化计算，与光流法这一步的工作十分类似
        // 计算出更新用的雅可比矩阵
        cv::parallel_for_(
            cv::Range(0, px_ref.size()),
           	std::bind(&JacobianAccumulator::accumulate_jacobian, &jaco_accu, std::placeholders::_1)
        );
        // 获取高斯牛顿法中的H,g
        Matrix6d H = jaco_accu.hessian();
        Vector6d b = jaco_accu.bias();

        // 求解增量方程并更新
        Vector6d update = H.ldlt().solve(b);;
        T21 = Sophus::SE3d::exp(update) * T21;
        cost = jaco_accu.cost_func();

        // 判断退出迭代的条件
        if (std::isnan(update[0])) {
            cout << "update is nan" << endl;
            break;
        }
        if (iter > 0 && cost > lastCost) {
            cout << "cost increased: " << cost << ", " << lastCost << endl;
            break;
        }
        if (update.norm() < 1e-3) {
            break;
        }

        lastCost = cost;
        cout << "iteration: " << iter << ", cost: " << cost << endl;
    }
	
    //优化结果
    cout << "T21 = \n" << T21.matrix() << endl;
    auto t2 = chrono::steady_clock::now();
    auto time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
    cout << "direct method for single layer: " << time_used.count() << endl;
	
    // 显示
    cv::Mat img2_show;
    cv::cvtColor(img2, img2_show, cv::COLOR_GRAY2BGR);
    VecVector2d projection = jaco_accu.projected_points();// 获取结果
    for (size_t i = 0; i < px_ref.size(); ++i) {
        auto p_ref = px_ref[i];	// 第一帧的点
        auto p_cur = projection[i];	//估计出第二帧的投影点
        if (p_cur[0] > 0 && p_cur[1] > 0) {
            //当估计的点在视场中时,模仿光流法的连线操作
            cv::circle(img2_show, cv::Point2f(p_cur[0], p_cur[1]), 2, cv::Scalar(0, 250, 0), 2);
            cv::line(img2_show, cv::Point2f(p_ref[0], p_ref[1]), cv::Point2f(p_cur[0], p_cur[1]),
                     cv::Scalar(0, 250, 0));
        }
    }
    cv::imshow("current", img2_show);
    cv::waitKey();
}

//多层直接法
void DirectPoseEstimationMultiLayer(
    const cv::Mat &img1,
    const cv::Mat &img2,
    const VecVector2d &px_ref,
    const vector<double> depth_ref,
    Sophus::SE3d &T21) {

    // 构建金字塔，与多层光流法的步骤一样
    int pyramids = 4;
    double pyramid_scale = 0.5;
    double scales[] = {1.0, 0.5, 0.25, 0.125};

    vector<cv::Mat> pyr1, pyr2;
    for (int i = 0; i < pyramids; i++) {
        if (i == 0) {
            pyr1.push_back(img1);
            pyr2.push_back(img2);
        } else {
            cv::Mat img1_pyr, img2_pyr;
            cv::resize(pyr1[i - 1], img1_pyr,
                       cv::Size(pyr1[i - 1].cols * pyramid_scale, pyr1[i - 1].rows * pyramid_scale));
            cv::resize(pyr2[i - 1], img2_pyr,
                       cv::Size(pyr2[i - 1].cols * pyramid_scale, pyr2[i - 1].rows * pyramid_scale));
            pyr1.push_back(img1_pyr);
            pyr2.push_back(img2_pyr);
        }
    }

    //先保存当前层的内参
    double fxG = fx, fyG = fy, cxG = cx, cyG = cy;
    for (int level = pyramids - 1; level >= 0; level--) {
        VecVector2d px_ref_pyr; // 用于保存当前层的点集
        for (auto &px: px_ref) {
            px_ref_pyr.push_back(scales[level] * px);
        }
        // 内参也要达到缩放的倍率
        fx = fxG * scales[level];
        fy = fyG * scales[level];
        cx = cxG * scales[level];
        cy = cyG * scales[level];
        //调用单层直接法
        DirectPoseEstimationSingleLayer(pyr1[level], pyr2[level], px_ref_pyr, depth_ref, T21);
    }
}
```

我们来看一下结果

![07-7](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\07-7.png)

这里给出了**最小光度误差下**的对应点变化关系。

![07-8](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\07-8.png)

这里可以看出对每一个单层直接法都估计出了位姿变换T，可以看出不同层的用时和估计结果也略有不同，可以根据一定的规则对金字塔的结果进行筛选，选择最优解作为最终解。

经过实践模块，我们更清晰的理解了光流法和直接法。光流法通过光流跟踪关键点的变化，替代计算描述子并匹配的特征点法匹配过程，加速了特征点法；直接法则另辟蹊径，通过最小化光度误差同时获得对应点和位姿变换的关系。直接法完全基于最小二乘问题，令人想到：如果图片的非凸性很强，直接法会轻易的落进局部最小值，无法进行进一步优化。两种方法都有一定的使用限制，不过对于大部分工程情况都可以满足，对于少部分情况将在后面的工程章节中具体说明。


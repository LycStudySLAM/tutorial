# 非线性优化

以下内容为高翔《视觉SLAM十四讲》第二版的学习笔记

## 回顾

在第一讲-初识SLAM中我们曾经提到SLAM的本质问题，并使用数学工具将其建模两个方程：**运动方程**和**观测方程**。之前三讲分别介绍了位姿的描述方式、位姿优化的数学工具以及相机的观测模型，用于表达视觉SLAM中的运动方程与观测方程。但无论如何，传感器的精度有限，两个方程不可能完全成立，而只能是近似成立。为了尽可能准确的定位，状态估计在所难免。这一讲介绍非线性优化相关的基础知识，以更好的解决上面提到的状态估计问题。

## 状态估计

重新回顾一下运动方程和观测方程，并且假设噪声先验是高斯分布
$$
x_k=f(x_{k-1},u_k)+w_k\\
z_{k,j}=h(y_j,x_k)+v_{k,j}\\
w\sim N(0,R_k),v\sim N(0,Q_{k,j})\\
其中x为位姿,y为路标(观测到的三维空间点的一部分),w,v为噪声，u,z分别为运动传感器和测量传感器数据\\
$$
现在我们的目标是通过传感器数据u和z，估计位姿x和路标y，这是一个状态估计问题。

处理这个状态估计可以大致分为两种：

①**递归状态估计**。首先持有当前时刻的估计状态，再根据新的时刻产生的数据将其更新。采取这种方式称为滤波器，比如卡尔曼滤波器，拓展卡尔曼滤波器，粒子滤波器等。（这一部分可以在经典著作《概率机器人》中学习）

②**批量状态估计**。不同于递归状态估计，批量状态估计将一段时间内的数据保存，再据此估计整段时间内的状态。

可以设想的是，递归状态估计由于其一阶马尔科夫假设，只与前一时刻的数据有关，如果之前的估计有误，会累积在整个过程的误差当中，这一点一定程度地降低了算法的鲁棒性（尽管实践上递归状态估计表现仍然不错）。而相对来说，批量状态估计可以在更大的范围（或者说窗口）达到最优化。极端情况下，甚至可以将全部数据先保存，再统一处理（这有点类似三维重建的课题），不过这种情况自然就和需求实时性的SLAM没太大关系了。一般来说，批量状态估计仅对当前时刻附近的状态进行优化。

### 批量状态估计

现在来介绍批量状态估计的数学模型，记位姿和路标集如下
$$
x=\{x_1,x_2,...,x_N\},y=\{y_1,y_2,...,y_M\}\\
$$
同样地，我们用u来表示运动传感器数据集，z来表示测量传感器的数据集，这些都是在1到N时刻产生的数据。那我们刚才提到的估计问题就是求
$$
p(x,y|z,u)\\
$$
如果运动传感器数据未知（比如说我们只有相机而没有里程计或imu时），上面的条件概率变为
$$
p(x,y|z)\\
$$
根据概率论的知识，可以有
$$
p(x,y|z,u)=\frac{p(z,u|x,y)p(x,y)}{p(z,u)}\propto p(z,u|x,y)p(x,y)\\
$$
如果你也看过《概率机器人》，你应该很清楚的知道第一个等号是由贝叶斯公式推知。正比于的原因及正比于号后的概率分布解释如下
$$
p(z,u)是确定的概率分布，由传感器提供，但包含传感器误差\\
p(z,u|x,y)称为似然，所谓似然，在这里表达了在怎样的位姿下观测，最有可能得到现在的传感器数据\\
p(x,y)称为先验，它是在观测之前位姿和观测的概率分布\\
而我们需要求的p(x,y|z,u)称为后验，它是经过新的传感器更新后，位姿和观测的概率分布\\
$$
上面这一段解释需要读者好好体会。如果还是没能理解，可以参考书籍《概率机器人》递推状态估计一章。

为了得到最优的状态估计，我们可以求解使后验概率最大的状态（这称为**最大后验概率估计**）。如果我们没有先验信息，也可以求解使似然最大的状态（这称为**最大似然估计**），分别记作如下
$$
(x,y)_{MAP}^{*}=argmax\;p(x,y|z,u)=argmax\;p(z,u|x,y)p(x,y)\\
(x,y)_{MLE}^{*}=argmax\;p(z,u|x,y)\\
$$

### 最小二乘

刚才已经为状态估计问题完成了数学建模，现在的问题是：如何求解最大后验/最大似然？这需要引入最小二乘的概念。

首先，还是先再次列出观测模型
$$
z_{k,j}=h(y_j,x_k)+v_{k,j},\quad v\sim N(0,Q_{k,j})\\
$$
从而有
$$
p(z_{k,j}|x_k,y_j)\sim N(h(y_j,x_k),Q_{k,j})\\
$$
又因为
$$
对于x\sim N(\mu,\Sigma),有
p(x)=det(2\pi\Sigma)^{-\frac{1}{2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))
$$
我们要求最大似然，只需要让最大似然函数的负对数最小（因为对数函数是递增的），负对数形式表示如下
$$
-\ln(p(x))=\frac{1}{2}\ln(det(2\pi\Sigma))+\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)
$$
因为第一项与x无关，故可以略去。综合上面的数学模型，我们可以知道求最大似然估计实际是在求
$$
(x_k,y_j)_{MLE}^*=argmin\;((z_{k,j}-h(y_j,x_k))^TQ_{k,j}^{-1}(z_{k,j}-h(y_j,x_k)))
$$
可以看出，该式等价于最小化噪声项的一个二次型。

有了上面的基础，我们再来讨论批量估计的情况，批量估计时，我们需要优化若干个时刻的运动和观测数据，而一般地，我们假设这些数据之间**相互独立**，从而可以有
$$
p(z,u|x,y)=\prod_k p(u_k|x_{k-1},x_k)\prod_{k,j}p(z_{k,j}|x_k,y_j)\\
e_{u,k}=x_k-f(x_{k-1},u_k),e_{z,k,j}=z_{k,j}-h(x_k,y_j)\\
$$
我们求的最大似然估计是使一段时刻内所有运动和观测的似然乘积最大的状态，误差项如第二式所示，它们是二次型的及基本构成。

转换成负对数的形式，我们正是在求
$$
min\;J(x,y)=\sum_k e_{u,k}^TR_k^{-1}e_{u,k}+\sum_k\sum_j e_{z,k,j}^TQ_{k,j}^{-1}e_{z,k,j}
$$
这里就构成了一个**最小二乘问题**。我们的目标就是找到这些二次型和为极小值时的状态，这是一个典型的**非线性优化**过程。

观察这个最小二乘问题，我们还能发现这个问题有一些特定的结构。首先，尽管总体的状态变量维数很高，但每个误差项并不复杂，仅仅与附近的两个位姿以及对应的路标相关，也就是说，整个问题中其中的矩阵是十分稀疏的。其次，采用李代数作为优化数学工具的优势有所体现，如果做这个优化还要考虑旋转矩阵正交的约束，会是一个令人头疼的问题。最后，用二次型表达也会带来一些问题，当测量越准确（协方差矩阵越小）时，其逆越大，在整个误差项中的占比越重，后续会讨论到这个问题。

### 举例：批量状态估计

不得不说，如果没有系统学习过状态估计的概念，上面的式子都会显得十分抽象，所以在这里通过一个简单的例子将上面的公式具象化。

假设在一条直线上有一个动点，它观测的是自己的位置。它的运动方程和观测方程如下
$$
x_k=x_{k-1}+u_k+w_k,w_k\sim N(0,Q_k)\\
z_k=x_k+n_k,n_k\sim N(0,R_k)
$$
我们假设要获得三个时刻的最优估计，则批量状态变量x，批量观测z，批量运动u如下
$$
x=[x_0,x_1,x_2,x_3]^T,z=[z_1,z_2,z_3]^T,u=[u_1,u_2,u_3]^T\\
$$
此时的条件概率和误差项如下
$$
p(u_k|x_k,x_{k-1})=N(x_k-x_{k-1},Q_k)\\
p(z_k|x_k)=N(x_k,R_k)\\
e_{u,k}=x_k-x_{k-1}-u_k\\
e_{z,k}=z_k-x_k
$$
目标函数则是
$$
min(\sum_{k=1}^3 e_{u,k}^TQ_k^{-1}e_{u,k}+\sum_{k=1}^3 e_{z,k}^TR_k^{-1}e_{z,k})
$$
由于系统为线性系统，我们可以将此问题写得更为简便一些
$$
y=[u,z]^T,令y-Hx=e\sim N(0,\Sigma)\\
则H=\begin{bmatrix}1&-1&0&0\\0&1&-1&0\\0&0&1&-1\\0&1&0&0\\0&0&1&0\\0&0&0&1\end{bmatrix},
\Sigma=diag(Q_1,Q_2,Q_3,R_1,R_2,R_3)\\
问题可以写成:argmin\;e^T\Sigma^{-1}e,经过之后的分析可以得到唯一解:(H^T\Sigma^{-1}H)^{-1}H^T\Sigma^{-1}y
$$

## 非线性最小二乘

先考虑一个最简单的最小二乘问题
$$
\min_xF(x)=\frac{1}{2}||f(x)||_2^2
$$
对于简单的线性函数，我们可以采取求导函数并令其为0来确定其极值，即
$$
x=[x_1,x_2,...,x_n]\\
F：R^{n}\rightarrow R,\frac{dF}{dx}=
\begin{bmatrix}
\frac{\partial F(x)}{\partial x_1}&
\frac{\partial F(x)}{\partial x_2}&
...&
\frac{\partial F(x)}{\partial x_n}
\end{bmatrix}=0
$$
极值可能是极大值，极小值和鞍点处的值，比较这些值即可。如果f为简单的线性函数，这就是一个线性最小二乘问题，不过，对于非线性最小二乘问题，有时f的结构会比较复杂，导致求导比较困难，而且需要知道目标函数的全局性质，这在无先验的情况下也是困难的，因此，这个时候我们通常采用**迭代**的方法，即从一个**初始值**出发，将优化变量向一个方向移动一个步长，使得**目标函数值下降**，直到**下降的值小于某一阈值**就停止迭代，可以将这个过程写作如下
$$
1.给定某个初始值x_0\\
2.对于第k次迭代,寻找一个增量\Delta x_k,使得||f(x_k+\Delta x_k)||_2^2达到极小值\\
3.若\Delta x_k足够小,则停止,否则,令x_{k+1}=x_k+\Delta x_k,重复步骤2
$$
这个过程让求解导函数为0的问题变为了不断寻找下降增量的问题。并且由于非线性函数的线性化，增量的计算将较为简单，并且如此一来，我们关注的是函数的局部性质而非全局性质。接下来介绍一些寻找该增量的方法（这部分属于数值优化的课题）

### 一阶和二阶梯度法

**对于第k次迭代**，直接对目标函数F进行泰勒展开
$$
F(x_k+\Delta x_k)\approx F(x_k)+J(x_k)^T\Delta x_k+\frac{1}{2}\Delta x_k^TH(x_k)\Delta x_k
$$
其中，J，H分别为向量x的一阶导数和二阶导数，分别称为**雅可比矩阵**和**黑塞矩阵**
$$
x=[x_1,x_2,...,x_n]\\
J(x)=\begin{bmatrix}
\frac{\partial F(x)}{\partial x_1}&
\frac{\partial F(x)}{\partial x_2}&
...&
\frac{\partial F(x)}{\partial x_n}
\end{bmatrix}^T,
H(x)=\begin{bmatrix}\frac{\partial^2 f(x)}{\partial x_1^2}&...&\frac{\partial^2 f(x)}{\partial x_1x_n}\\...&...&...\\\frac{\partial^2 f(x)}{\partial x_nx_1}&...&\frac{\partial^2 f(x)}{\partial x_n^2}\end{bmatrix}
$$
千万不要忘记我们本质上在**用迭代与线性化的方法**替代**求导函数并令其为零的方法**，本质上，我们的目标仍然是令
$$
\frac{dF}{dx}=
\begin{bmatrix}
\frac{\partial F(x)}{\partial x_1}&
\frac{\partial F(x)}{\partial x_2}&
...&
\frac{\partial F(x)}{\partial x_n}
\end{bmatrix}=0
$$
所以增量必须要让导函数向0的方向前进。

最简单的方法，自然是直接取增量为一阶导数反向的梯度，并且通过一个步长λ来调节,即
$$
\Delta x^*=-\lambda J(x_k)
$$
这种方法称为**最速下降法**，也称为一阶梯度法。

当然，我们还可以利用二阶导数的信息，将上面的二阶泰勒展开式**对增量求导**并令其为0。
$$
J(x_k)+H(x_k)^T\Delta x_k=0
$$
这一方程称为**增量方程**。我们认为对于每一次迭代这个式子都是有意义的，所以写作
$$
H\Delta x=-J
$$
这种方法称为**牛顿法**，也称为二阶梯度法。

这里需要做一些解释（**这段解释非常重要**）：

①我们的目标是使目标函数最小。

②我们采取了线性化的方式，将原来非线性的目标函数线性化成了一个二次函数。

③线性化后我们的目标是使这个二次函数最小，也就是向这个二次函数的一阶导数为0的方向前进。

④具体到计算，我们需要找到一个增量，使得这个二次函数最小，也就是说构成了一个增量是自变量，二次函数值是因变量的映射。

⑤如此一来，最简单的方法就是对增量求一阶导并令其为0，这时的增量方向正是满足我们的目标的方向。

⑥我们的处理使得我们不需要全局的计算非线性函数的一阶导，而是分成了若干个二次函数的一阶导。

这两种方法原理上都是可行的，但各自有各自的问题。最速下降法由于过于贪心，容易走出锯齿路线（因为沿当前迭代的一阶导数的反向方向前进，如果没有控制好步长，容易过冲），导致迭代次数的增加。牛顿法从原理上来说没太大问题，效果也比较好，最大的缺陷是二阶导数H计算量较大，特别是向量较长导致H规模较大时，因此我们往往倾向于避免H的计算。

### 高斯牛顿法

高斯牛顿法在牛顿法的基础上进行了一些改进，当然，最大的改进就是不需要求解H。下面详细介绍高斯牛顿法。

重新把最小二乘问题写作如下
$$
\min_xF(x)=\frac{1}{2}||f(x)||_2^2
$$
首先，将函数f而不是目标函数F泰勒展开
$$
f(x+\Delta x)\approx f(x)+J(x)^T\Delta x,J(x)^T=\frac{df}{dx}
$$
最小二乘问题变为
$$
\Delta x^*=arg\min_{\Delta x}(\frac{1}{2}||f(x)+J(x)^T\Delta x||^2)\\
\frac{1}{2}||f(x)+J(x)^T\Delta x||^2=\frac{1}{2}(f(x)+J(x)^T\Delta x)^T(f(x)+J(x)^T\Delta x)\\=
\frac{1}{2}(||f(x)||_2^2+2f(x)J(x)^T\Delta x+\Delta x^TJ(x)J(x)^T\Delta x)
$$
我们根据牛顿法中我提到的解释的思想，对增量求一阶导并令其为0，即
$$
J(x)f(x)+J(x)J(x)^T\Delta x=0\\
令H(x)=J(x)J(x)^T,g(x)=-J(x)f(x),有\\
H\Delta x=g
$$
我们把这个方程称为**高斯牛顿方程**。

可以看到，高斯牛顿法中不再需要计算H，而是通过J来实现对H的近似。高斯牛顿法的步骤可以记作如下
$$
1.给定初始值x_0\\
2.对于第k次迭代,求出当前的误差项f(x_k)和雅可比矩阵J(x_k)\\
3.求解增量方程H\Delta x=g\\
4.若\Delta x_k足够小则停止，否则重复步骤2,3
$$
高斯牛顿法看起来已经相当不错，但仍然有很大的缺陷。为了求解增量方程，我们需要求这里表示的H的逆，而由J表示的H往往只有半正定性，也就是说使用该算法时会出现H是奇异或变态的矩阵导致算法不收敛。就算H并非奇异或病态，如果我们求出的步长太大，也会导致泰勒展开近似准确性的下降。

尽管如此，高斯牛顿法的思路还是不错的，在此基础上解决步长问题的就有阻尼牛顿法。

### 阻尼牛顿法

为了解决泰勒展开近似因步长而导致的准确性下降问题，阻尼牛顿法采取了一个因子来表示近似的好坏程度
$$
\rho =\frac{f(x+\Delta x)-f(x)}{J(x)^T\Delta x}
$$
其步骤如下
$$
1.给定初始值x_0,初始优化半径\mu\\
2.对于第k次迭代，在高斯牛顿法的基础上加上信赖区域\\
\min_{\Delta x_k}\frac{1}{2}||f(x_k)+J(x_k)^T\Delta x_k||^2,s.t. ||D\Delta x_k||\leq\mu\\
3.计算\rho,若\rho>\frac{3}{4},\mu=2\mu;\rho<\frac{1}{4},\mu=0.5\mu\\
4.更新并判断算法是否收敛
$$
这相当于在求解拉格朗日函数
$$
L(\Delta x_k,\lambda)=\frac{1}{2}||f(x_k)+J(x_k)^T\Delta x_k||^2+\frac{\lambda}{2}(||D\Delta x_k||-\mu)
$$
它的核心是求解
$$
(H+\lambda D^TD)\Delta x_k=g
$$

### 其它讨论

对于非线性优化问题而言，设置初始值十分重要，否则迭代次数会增加。

除此之外，非线性优化还有一个问题，对于非凸的优化问题可能会陷入局部极小值而不是全局的最小值。



## 实践模块

安装ceres-solver:https://github.com/ceres-solver/ceres-solver/releases/tag/1.14.0，http://ceres-solver.org/

安装g2o特定版本:https://github.com/RainerKuemmerle/g2o/tree/9b41a4ea5ade8e1250b9c1b279f3a9c098811b5a

编译工程项目不多赘述，直接开始实践环节：曲线拟合问题

假设我们有这样一条曲线（在SLAM中我们可以把它当做是状态方程）
$$
y=\exp(ax^2+bx+c)+w,w\sim N(0,\sigma^2)
$$
现在我们有N个数据点x,y（在SLAM中我们可以把它当做是传感器数据），我们需要估计参数a,b,c（在SLAM中我们可以把它当做待估计的相机位姿）。这是一个最小二乘问题。其中误差项为
$$
e_i=y_i-\exp(ax_i^2+bx_i+c)\\
$$
则最小二乘问题可以表示为
$$
\min_{a,b,c}\frac{1}{2}\sum_{i=1}^Ne_i^T(\sigma^2)^{-1}e_i\\
$$

由于这个问题是一维的标量(而不是向量)，我们可以写成
$$
\min_{a,b,c}\frac{1}{2}\sum_{i=1}^N(\sigma^2)^{-1}||e_i||^2=
\min_{a,b,c}\frac{1}{2}\sum_{i=1}^N(\sigma^2)^{-1}||y_i-\exp(ax_i^2+bx_i+c)||^2
$$

### 手写高斯牛顿法

我们首先求出误差函数的一阶导
$$
\frac{\partial e_i}{\partial a}=-x_i^2\exp(ax_i^2+bx_i+c)\\
\frac{\partial e_i}{\partial b}=-x_i\exp(ax_i^2+bx_i+c)\\
\frac{\partial e_i}{\partial c}=-\exp(ax_i^2+bx_i+c)\\
J=\begin{bmatrix}\frac{\partial e_i}{\partial a}&
\frac{\partial e_i}{\partial b}&
\frac{\partial e_i}{\partial c}\end{bmatrix}^T\\
$$
这里的最小二乘是多个最小二乘问题之和，各个误差之间独立（求导上没有任何关系），增量方程为
$$
\sum_{i=1}^N(\sigma^2)^{-1}(J_iJ_i^T)\Delta x=\sum_{i=1}^N(J_i(\sigma^2)^{-1}e_i)
$$
下面分析代码及结果（分析已经写在注释当中）

```C++
double ar = 1.0, br = 2.0, cr = 1.0;         // 真实参数值
double ae = 2.0, be = -1.0, ce = 5.0;        // 估计参数初始值
int N = 100;                                 // 数据点数量
double w_sigma = 1.0;                        // 噪声Sigma值
double inv_sigma = 1.0 / w_sigma;
cv::RNG rng;                                 // OpenCV随机数产生器

//生成数据
vector<double> x_data, y_data;      // 数据
for (int i = 0; i < N; i++) {
    double x = i / 100.0;
    x_data.push_back(x);
    y_data.push_back(exp(ar * x * x + br * x + cr) + rng.gaussian(w_sigma * w_sigma));
}

// 开始Gauss-Newton迭代
int iterations = 100;    // 迭代次数
double cost = 0;		 // 本次迭代的cost
double lastCost = 0;     //上一次迭代的cost

chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
for (int iter = 0; iter < iterations; iter++) {

    Matrix3d H = Matrix3d::Zero();             
    Vector3d g = Vector3d::Zero();          
    cost = 0;

    for (int i = 0; i < N; i++) {
        double xi = x_data[i], yi = y_data[i];  // 第i个数据点
        double error = yi - exp(ae * xi * xi + be * xi + ce);//计算误差函数值
        
        //计算雅可比矩阵
        Vector3d J; // 雅可比矩阵
        J[0] = -xi * xi * exp(ae * xi * xi + be * xi + ce);
        J[1] = -xi * exp(ae * xi * xi + be * xi + ce);
        J[2] = -exp(ae * xi * xi + be * xi + ce);
        
		//计算H和g
        H += inv_sigma * inv_sigma * J * J.transpose();
        g += -inv_sigma * inv_sigma * error * J;
		
        //计算所有点误差函数的和
        cost += error * error;
    }

    // 求解线性增量方程 Hx=g
    Vector3d dx = H.ldlt().solve(g);
    if (isnan(dx[0])) {
        cout << "result is nan!" << endl;
        break;
    }

    //如果本次cost大于或等于上次，表示已经收敛，结束迭代
    if (iter > 0 && cost >= lastCost) {
        cout << "cost: " << cost << ">= last cost: " << lastCost << ", break." << endl;
        break;
    }

    //更新优化量
    ae += dx[0];
    be += dx[1];
    ce += dx[2];

    lastCost = cost;

    cout << "total cost: " << cost << ", \t\tupdate: " << dx.transpose() <<
        "\t\testimated params: " << ae << "," << be << "," << ce << endl;
}

//输出用时及最终优化量结果
chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
cout << "solve time cost = " << time_used.count() << " seconds. " << endl;
cout << "estimated abc = " << ae << ", " << be << ", " << ce << endl;
```

迭代的过程和结果如下图所示，可以看出迭代速度很快，但似乎陷入了局部极小值。

![05-1](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\05-1.png)

### 使用Ceres

Ceres求解最小二乘问题一般地形式如下
$$
\min_x \frac{1}{2} \sum_i \rho_i||f_i(x_i)||^2,s.t.l_j<x_j<u_j\\
其中x_1,...,x_n是优化变量(ceres中称为参数块),f_i是代价函数(ceres中称为残差块)，\rho_i是核函数,u,l是优化变量的上下界
$$
为了使用Ceres完成最小二乘问题的求解，我们需要做：

①定义每个**参数块**。参数块通常为平凡的向量，在SLAM中可能是李代数，四元数这样的向量。

②定义**残差块的计算方式**。残差块通常需要关联若干个参数块，并对其进行一些计算并返回残差值，Ceres再对其进行求平方和。

③残差块往往还需要**自定义雅可比矩阵的计算**。如果要使用其自动求导功能，那么需要按照特定的格式书写。

④将所有的参数块和残差块加入Ceres的**Problem对象**当中，调用**Solve函数**求解，求解器还可以做一些配置，如停止条件等。

下面我们来分析代码与结果（解释都写在注释当中了）

```C++
// 代价函数的计算模型
//定义残差块的类
//operator()为拟函数，ceres可以调用这个函数将矩阵传入
struct CURVE_FITTING_COST {
  CURVE_FITTING_COST(double x, double y) : _x(x), _y(y) {}
    
  const double _x, _y;    // x,y数据
  // 残差块的计算方式
  template<typename T>
  bool operator()(const T *const abc,T *residual) const
  {
    //其中T(_x),T(_y)是强制类型转换为类型T
    residual[0] = T(_y) - ceres::exp(abc[0] * T(_x) * T(_x) + abc[1] * T(_x) + abc[2]); // y-exp(ax^2+bx+c)
    return true;
  }
};

//与手写高斯牛顿法一致，先生成含噪声的数据
double ar = 1.0, br = 2.0, cr = 1.0;         // 真实参数值
double ae = 2.0, be = -1.0, ce = 5.0;        // 估计参数值
int N = 100;                                 // 数据点
double w_sigma = 1.0;                        // 噪声Sigma值
double inv_sigma = 1.0 / w_sigma;
cv::RNG rng;                                 // OpenCV随机数产生器

vector<double> x_data, y_data;      // 数据
for (int i = 0; i < N; i++) {
    double x = i / 100.0;
    x_data.push_back(x);
    y_data.push_back(exp(ar * x * x + br * x + cr) + rng.gaussian(w_sigma * w_sigma));
}

//参数块，一个double数组
double abc[3] = {ae, be, ce};

// 构建Problem对象
ceres::Problem problem;
for (int i = 0; i < N; i++) {
    problem.AddResidualBlock(     // 向问题中添加误差项
        // 使用自动求导AutoDiff
        //模板参数：误差类型（我们定义的残差块），一维误差项，三维优化项
        new ceres::AutoDiffCostFunction<CURVE_FITTING_COST, 1, 3>(
            new CURVE_FITTING_COST(x_data[i], y_data[i])
        ),
        nullptr,            // 核函数，这里不使用，为空
        abc                 // 待估计参数
    );
}

// 配置求解器
ceres::Solver::Options options;     // 这里有很多配置项可以填
options.linear_solver_type = ceres::DENSE_NORMAL_CHOLESKY;  // 增量方程如何求解
options.minimizer_progress_to_stdout = true;   // 输出到cout

ceres::Solver::Summary summary;                // 优化信息
chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
ceres::Solve(options, &problem, &summary);  // 调用Solve函数开始优化
chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
cout << "solve time cost = " << time_used.count() << " seconds. " << endl;

// 输出结果
cout << summary.BriefReport() << endl;
cout << "estimated a,b,c = ";
for (auto a:abc) cout << a << " ";
cout << endl;
```

代码运行结果如下。可以看出，与手写高斯牛顿法的结果基本是一致的

![05-2](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\05-2.png)

### 使用g2o

g2o是一个基于图优化的库。图优化理论是一种将非线性优化和图论结合起来的理论。

图优化中的图由若干个边和若干个定点组成，用顶点表示优化变量，用边表示误差项。以SLAM的两个方程为例，我们可以构建这样一个图优化问题。

![05-3](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\05-3.png)

对于上面所说的非线性最小二乘问题，我们也可以构建一个图优化问题。

![05-4](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\05-4.png)

g2o需要用户所做的事主要是下面几个步骤

①定义**顶点**和**边**的类型

②构建**图**

③选择**优化算法**

④调用g2o进行优化，返回结果

还是根据代码来分析（解释都写在注释中了）

```C++
// 图的顶点
//模板参数：优化变量维度和数据类型
class CurveFittingVertex : public g2o::BaseVertex<3, Eigen::Vector3d> {
public:
  EIGEN_MAKE_ALIGNED_OPERATOR_NEW
      
  //并且顶点有私有变量_estimate,其类型模板给定的Eigen::Vector3d
  // 重置
  virtual void setToOriginImpl() override {
    _estimate << 0, 0, 0;
  }

  // 更新
  virtual void oplusImpl(const double *update) override {
    _estimate += Eigen::Vector3d(update);
  }

  // 存盘和读盘：留空
  virtual bool read(istream &in) {}

  virtual bool write(ostream &out) const {}
};


// 图的边  
// 模板参数：观测值维度，类型，连接顶点类型
class CurveFittingEdge : public g2o::BaseUnaryEdge<1, double, CurveFittingVertex> {
public:
  EIGEN_MAKE_ALIGNED_OPERATOR_NEW

  CurveFittingEdge(double x) : BaseUnaryEdge(), _x(x) {}

  // 计算曲线模型误差
  virtual void computeError() override {
    const CurveFittingVertex *v = static_cast<const CurveFittingVertex *> (_vertices[0]);
    const Eigen::Vector3d abc = v->estimate();//返回顶点的优化变量值
    _error(0,0) = _measurement - std::exp(abc(0,0) * _x * _x + abc(1,0) * _x + abc(2,0));
  }

  // 计算雅可比矩阵
  virtual void linearizeOplus() override {
    const CurveFittingVertex *v = static_cast<const CurveFittingVertex *> (_vertices[0]);
    const Eigen::Vector3d abc = v->estimate();//返回顶点的优化变量值
    double y = exp(abc[0] * _x * _x + abc[1] * _x + abc[2]);
    _jacobianOplusXi[0] = -_x * _x * y;
    _jacobianOplusXi[1] = -_x * y;
    _jacobianOplusXi[2] = -y;
  }

  virtual bool read(istream &in) {}

  virtual bool write(ostream &out) const {}

public:
  double _x;  // x 值， y 值为 _measurement
};


//生成含噪声的数据
double ar = 1.0, br = 2.0, cr = 1.0;         // 真实参数值
double ae = 2.0, be = -1.0, ce = 5.0;        // 估计参数值
int N = 100;                                 // 数据点
double w_sigma = 1.0;                        // 噪声Sigma值
double inv_sigma = 1.0 / w_sigma;
cv::RNG rng;                                 // OpenCV随机数产生器

vector<double> x_data, y_data;      // 数据
for (int i = 0; i < N; i++) {
    double x = i / 100.0;
    x_data.push_back(x);
    y_data.push_back(exp(ar * x * x + br * x + cr) + rng.gaussian(w_sigma * w_sigma));
}


// 构建图优化，先设定g2o
// 重命名solver的类型名。每个误差项优化变量维度为3，误差值维度为1
typedef g2o::BlockSolver<g2o::BlockSolverTraits<3, 1>> BlockSolverType;
// 线性求解器类型
typedef g2o::LinearSolverDense<BlockSolverType::PoseMatrixType> LinearSolverType; 

// 梯度下降方法，可以从GN, LM, DogLeg 中选
auto solver = new g2o::OptimizationAlgorithmGaussNewton(
    g2o::make_unique<BlockSolverType>(g2o::make_unique<LinearSolverType>()));

g2o::SparseOptimizer optimizer;     // 图模型
optimizer.setAlgorithm(solver);   // 设置求解器
optimizer.setVerbose(true);       // 打开调试输出

// 往图中增加顶点，初始化顶点变量，并设置ID
CurveFittingVertex *v = new CurveFittingVertex();
v->setEstimate(Eigen::Vector3d(ae, be, ce));
v->setId(0);
optimizer.addVertex(v);

// 往图中增加边，将x值作为初始化量初始化边，设置ID，设置连接的顶点，设置观测数值
for (int i = 0; i < N; i++) {
    CurveFittingEdge *edge = new CurveFittingEdge(x_data[i]);
    edge->setId(i);
    edge->setVertex(0, v);                
    edge->setMeasurement(y_data[i]);
    // 设置信息矩阵：协方差矩阵之逆
    edge->setInformation(Eigen::Matrix<double, 1, 1>::Identity() * 1 / (w_sigma * w_sigma));
    optimizer.addEdge(edge);
}

// 执行优化
cout << "start optimization" << endl;
chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
optimizer.initializeOptimization();
optimizer.optimize(10);
chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
cout << "solve time cost = " << time_used.count() << " seconds. " << endl;

// 输出优化值
Eigen::Vector3d abc_estimate = v->estimate();
cout << "estimated model: " << abc_estimate.transpose() << endl;
```

不得不说，使用g2o还是相当复杂的，而且g2o没有一个很好的官方文档，只能找到github上仓库的readme和一些简单的使用教程进行基础的学习。不过到此为止，我们还只是为后面真正SLAM需要的理论打下基础，具体怎么将这些工具使用在SLAM上，后续还会有具体的教程。下面来看一下g2o的优化结果。

![05-5](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\05-5.png)

不出所料，与前两种方式的结果基本是一致的。

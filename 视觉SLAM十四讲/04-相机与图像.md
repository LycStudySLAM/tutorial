# 相机与图像

以下内容为高翔《视觉SLAM十四讲》第二版的学习笔记

## 回顾

前面两讲介绍了位姿的表达方式以及优化时必要的数学工具。有了前两讲的工具和模型，就可以描述相机在三维空间中的刚体运动以及优化运动估计。本讲主要描述相机模型，着重解决**空间中的一点如何投影到相机像素平面**这一过程的数学描述。

## 相机模型

相机完成了**将三维的空间点映射到二维图像平面**这样一个过程。关于相机模型，最简单的是针孔模型（这一点与小孔成像的原理是一致的），不过相机内部还有透镜，所以还需要一个畸变模型来描述光线通过透镜发生的变化。大部分普通的相机都可以通过针孔和畸变模型来描述这一过程，并将这一过程中的参数称为相机的内参数。

### 针孔模型

![04-1](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\04-1.png)

针孔相机模型如上图所示。可以看到三维空间点P通过针孔相机模型投影到成像平面上的点p'。现在对这一模型进行数学建模。

首先，记O-x-y-z坐标系为相机坐标系，O为相机光心（也是针孔模型中的针孔），三维空间中一点P通过光心投影到物理成像平面上，成像点为P'（注意，这里的p'还是物理平面上实际的点，而不是像素坐标）,记
$$
P:[X,Y,Z]^T,P':[X',Y',Z']^T\\
$$
并且记光心到物理成像平面的距离为f（焦距），则根据上图的相似三角形关系，有
$$
\frac{Z}{f}=-\frac{X}{X'}=-\frac{Y}{Y'}\\
$$
其中负号表示像是倒立的（这一点大家也应该很熟悉），但我们使用时相机都是正像，为了符合实际相机，我们可以将物理成像平面虚拟地移到光心前，如下图所示，这样就符合实际相机产出图像的模型（尽管实际的工作原理并非如此）。

![04-2](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\04-2.png)

这样一来，我们就得到了这样的针孔模型
$$
\frac{Z}{f}=\frac{X}{X'}=\frac{Y}{Y'}\\
X'=f\frac{X}{Z}\\
Y'=f\frac{Y}{Z}
$$
到这一步，我们已经完成了**三维空间一点到物理成像平面一点**的转换，描述它们的单位都是真实尺度下的单位，比如米，毫米等等。但相机输出的是像素坐标，所以中间还有从**物理成像平面一点P’到像素平面坐标**的转换过程。

现在，设物理成像平面o-x'-y'的同一平面上固定像素平面o'-u-v，而像素坐标系的一般定义是原点o'放在图像左上角（而不是中心），u轴与x'轴平行，v轴与y'轴平行。所以，物理成像平面与像素平面的坐标变换相差一个平移和一个缩放，可以描述为
$$
u=\alpha X'+c_x\\
v=\beta Y'+c_y\\
$$
这时，我们把刚才的工作总结一下，直接从三维空间一点到像素平面则是
$$
u=f_x\frac{X}{Z}+c_x\\
v=f_y\frac{y}{Z}+c_y\\
其中，f_x=\alpha f,f_y=\beta f
$$
为了表达的简洁，将其写为矩阵形式
$$
Z\begin{bmatrix}u\\v\\1\end{bmatrix}=\begin{bmatrix}f_x&0&c_x\\0&f_y&c_y\\0&0&1\end{bmatrix}
\begin{bmatrix}X\\Y\\Z\end{bmatrix}=KP\\
$$
将
$$
K=\begin{bmatrix}f_x&0&c_x\\0&f_y&c_y\\0&0&1\end{bmatrix}
$$
记作**相机内参数矩阵**。

（确定相机的内参这一工作称为**标定**，如果做过这个方面的工作，应该非常熟悉棋盘格标定的过程了。）

考虑到相机在移动这一问题，我们自然能想到，相对世界坐标系，如果世界坐标系到相机坐标系的旋转矩阵是R，平移矩阵是t，变换矩阵是T，根据我们前两讲的知识，我们可以得到
$$
ZP_{uv}=Z\begin{bmatrix}u\\v\\1\end{bmatrix}=\begin{bmatrix}f_x&0&c_x\\0&f_y&c_y\\0&0&1\end{bmatrix}
\begin{bmatrix}X\\Y\\Z\end{bmatrix}=KP_c=K(RP_w+t)=KTP_w\\
其中P_{uv}是像素平面齐次坐标，P_c是相机坐标系下的三维点坐标，P_w是世界坐标系下的同一三维点坐标\\
上面的式子最后一步隐含了一个齐次坐标到非齐次坐标的转换,因为TP_w是一个四维齐次坐标
$$
一般地，我们将相机相对世界坐标系的位姿R，t记为相机的**外参数**。

上面这个式子还可以从**另一个角度**来看
$$
P_{uv}=\begin{bmatrix}u\\v\\1\end{bmatrix}=\begin{bmatrix}f_x&0&c_x\\0&f_y&c_y\\0&0&1\end{bmatrix}
\begin{bmatrix}\frac{X}{Z}\\\frac{Y}{Z}\\1\end{bmatrix}\\
称\begin{bmatrix}\frac{X}{Z}\\\frac{Y}{Z}\\1\end{bmatrix}为归一化坐标
$$
归一化其位于归一化平面上，归一化平面可以看作相机前方Z=1平面的一个点，其左乘内参直接得到像素坐标。从这个式子可以看出，相机坐标系下的[X,Y,Z]同时乘一个非零常数，归一化坐标都一致，从而像素坐标一致，这说明**点的深度在投影的过程中丢失了**。

### 畸变模型

为了更广的视场，相机前通常都会安装透镜。透镜自身的形状及其与成像平面的不完全平行都会带来光线在投影过程中的变化。

由透镜形状引起的变化称为**径向畸变**，体现出来的效果是直线不再是直的，而变成了曲线，且越靠近像素平面边缘越明显。畸变主要分为两大类：**桶形畸变**和**枕形畸变**

![04-3](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\04-3.png)

由透镜与成像平面不完全平行带来的变化称为**切向畸变**

![04-4](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\04-4.png)

现在为两类畸变进行数学建模
$$
记归一化平面上的一点p,其直角坐标为[x,y]^T,极坐标为[r,\theta]^T\\
$$
经过径向畸变后，有
$$
x_{distorted}=x(1+k_1r^2+k_2r^4+k_3r^6)\\
y_{distorted}=y(1+k_1r^2+k_2r^4+k_3r^6)
$$
经过切向畸变后，有
$$
x_{distorted}=x+2p_1xy+p_2(r^2+2x^2)\\
y_{distorted}=y+p_1(r^2+2y^2)+2p_2xy
$$
综合起来，得到畸变模型
$$
x_{distorted}=x(1+k_1r^2+k_2r^4+k_3r^6)+2p_1xy+p_2(r^2+2x^2)\\
y_{distorted}=y(1+k_1r^2+k_2r^4+k_3r^6)+p_1(r^2+2y^2)+2p_2xy
$$
其中k，p为畸变系数。选取畸变模型的时候，不一定要将上面所有的畸变系数都选择，可以灵活选择其中一部分（假如系统要求的精度没有那么高）。

### 总结：针孔模型和畸变模型

刚才介绍了两个模型，已经有不少坐标系了，相信大家第一次看到这里的时候都已经理不清楚了，这里对针孔和畸变模型整体总结一下。

**成像过程：**世界坐标系下一点到像素平面的坐标点。整个过程如下。

①通过外参数，将坐标变换到相机坐标系下一点。
$$
\begin{bmatrix}X\\Y\\Z\end{bmatrix}=P_c=RP_w+t=TP_w,最后一步涉及齐次坐标与非齐次坐标的变换
$$
②将相机坐标系下一点归一化Z坐标，变换到归一化平面上一点
$$
\begin{bmatrix}X\\Y\\Z\end{bmatrix}\rightarrow\begin{bmatrix}\frac{X}{Z}\\\frac{Y}{Z}\\1\end{bmatrix}=
\begin{bmatrix}x\\y\\1\end{bmatrix}
$$
③根据畸变模型，计算归一化平面上坐标发生畸变后的坐标
$$
x_{distorted}=x(1+k_1r^2+k_2r^4+k_3r^6)+2p_1xy+p_2(r^2+2x^2)\\
y_{distorted}=y(1+k_1r^2+k_2r^4+k_3r^6)+p_1(r^2+2y^2)+2p_2xy
$$
④通过内参数，变换到像素坐标上
$$
P_{uv}=\begin{bmatrix}u\\v\\1\end{bmatrix}=\begin{bmatrix}f_x&0&c_x\\0&f_y&c_y\\0&0&1\end{bmatrix}
\begin{bmatrix}x_{distorted}\\y_{distorted}\\1\end{bmatrix}\\
$$
**涉及的坐标系：**世界坐标系，相机坐标系，归一化平面，像素平面。（物理成像平面隐含在了内参当中）



## 双目相机模型

刚才介绍的单目相机模型已经告诉我们，单目相机仅凭像素坐标无法获取空间点的真实深度，因为光心到归一化平面点的连线及其延长线上所有的点都会投影到同一个像素上。

![04-5](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\04-5.png)

这里特别提醒一下：不要忘记我们了解相机模型的目的是什么。不仅仅是外部世界的一个点通过怎样的方式投影到像素上被我们获取，更重要的是，我们要根据相机得到的像素结果，重新获得三维空间点在世界坐标系下的坐标，这样才能完成定位与建图的目标。

言归正传，生活经验告诉我们，我们可以通过移动一个单目相机，来看出一个空间点究竟远或近。如果我们直接就有两个相机呢？接下来就介绍双目相机获取深度的原理。

![04-6](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\04-6.png)
$$
O_L,O_R为左右相机光心,b为基线长度,f为焦距,u_L(此处为正数),u_R(此处为负数)为空间点p投影到左右相机的坐标
$$
根据相似关系，可以简单地得到
$$
\frac{z}{z-f}=\frac{b-u_L+u_R}{b}\rightarrow z=\frac{fb}{d},d=u_L-u_R
$$
称d为左右相机的视差，在这里也是左右相机对同一点成像后横坐标之差

这个过程看似简洁，但也反映了双目相机的两个问题：可测深度有理论最大值，因为视差最小为一个像素。计算量大，如果要估计每个像素，首先要进行匹配，还要对每一对匹配点进行计算，做到实时性比较困难。

## RGB-D相机模型

RGB-D相机可以按原理分为两大类：红外结构光和ToF。两种类型都是通过向外发射光束，再根据返回的光束与发射光束对比，并经过一定的算法处理，从而得到距离信息。

相对双目相机来说，RGB-D获取深度对于我们这些传感器的用户而言就简单多了。因为RGB-D相机已经把彩色图和深度图配准了，我们可以直接使用图像信息和距离信息。当然，有好处也有缺陷，因为发射光束到透明材料反射效果差，以及室外强光会产生干扰，这限制了RGB-D相机的使用场景。



## 图像

本讲的标题是相机与图像，上面已经将相机模型做了基本的介绍，现在简单介绍图像在计算机中的格式（毕竟做图像相关的工作对这方面肯定是很熟悉的）。图像大抵先分为灰度图和彩色图。所谓灰度图就是单通道图像，每个像素由一个通道表达，其值是投影到该像素的亮度，一般每个通道由8位整数表达（0-255）。彩色图则是三通道图像，包含RGB三个分量，每个分量由8位整数表达。有些时候图像还有透明度通道，需要32位整数表达RGBA四个通道。深度图一般由16位整数表达。

我们处理图像一般使用OpenCV。OpenCV提供了C++，Python的API，非常方便我们在ROS中进行使用。OpenCV的默认通道是BGR（而不是RGB），需要多加注意。



## 实践板块

安装OpenCV我不打算赘述，如果读者已经安装了ROS，那么会有自带的OpenCV；如果读者想使用特定的OpenCV版本，可以到OpenCV的官方github（https://github.com/opencv/opencv/releases）上进行对应版本的下载。安装就如同其他CMake工程一样。如果读者还没有安装依赖项，请参考《视觉SLAM十四讲》书中的依赖项进行安装。至于slambook中的工程编译，也如之前的CMake工程一样，不再赘述。

### imageBasics.cpp

文件中主要介绍了图像的读取，主要谈及了一个问题：cv::Mat用赋值或者拷贝初始化都是浅拷贝，要想深拷贝务必使用其提供的clone()方法。更多的OpenCV API使用见官方教程https://docs.opencv.org/master/d9/df8/tutorial_root.html

### undistortedImage.cpp

文件示例了一个去畸变过程，先看看效果左边是去畸变之后的，比较右边的畸变程度，可以说修正得很不错了。

![04-7](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\04-7.png)

再看看代码（解释都写为注释了）

```C++
// 下面的参数都可以直接通过标定获得
// 畸变参数
double k1 = -0.28340811, k2 = 0.07395907, p1 = 0.00019359, p2 = 1.76187114e-05;
// 内参
double fx = 458.654, fy = 457.296, cx = 367.215, cy = 248.375;

// 计算去畸变后图像的内容
for (int v = 0; v < rows; v++) {
    for (int u = 0; u < cols; u++) {
        //下面这一部分，通过畸变模型，得到一个关于无畸变(u,v)到有畸变(u_distorted,v_distorted)的映射
        double x = (u - cx) / fx, y = (v - cy) / fy;//由像素坐标(u,v)到归一化平面(x,y)
        double r = sqrt(x * x + y * y);//计算参数r
        //根据畸变模型算出归一化平面上畸变后的结果
        double x_distorted = x * (1 + k1 * r * r + k2 * r * r * r * r) + 2 * p1 * x * y + p2 * (r * r + 2 * x * x);
        double y_distorted = y * (1 + k1 * r * r + k2 * r * r * r * r) + p1 * (r * r + 2 * y * y) + 2 * p2 * x * y;
        //通过内参方程算出像素平面上畸变后的结果
        double u_distorted = fx * x_distorted + cx;
        double v_distorted = fy * y_distorted + cy;

        //一一对应，无畸变图像的点(u,v)对应有畸变图像的点(u_distorted,v_distorted)
        // 赋值 (最近邻插值)
        if (u_distorted >= 0 && v_distorted >= 0 && u_distorted < cols && v_distorted < rows) {
            image_undistort.at<uchar>(v, u) = image.at<uchar>((int) v_distorted, (int) u_distorted);
        } else {
            image_undistort.at<uchar>(v, u) = 0;
        }
    }
}
```

### stereoVision.cpp

由双目的灰度图计算视差图，分析代码如下

```C++
// 内参
double fx = 718.856, fy = 718.856, cx = 607.1928, cy = 185.2157;
// 基线
double b = 0.573;

// 读取图像
cv::Mat left = cv::imread(left_file, 0);
cv::Mat right = cv::imread(right_file, 0);
// 下面这个参数意义，参考https://blog.csdn.net/weixin_43042467/article/details/108199785
// 或者参考官方文档的解释
cv::Ptr<cv::StereoSGBM> sgbm = cv::StereoSGBM::create(
0, 96, 9, 8 * 9 * 9, 32 * 9 * 9, 1, 63, 10, 100, 32);    // 神奇的参数
cv::Mat disparity_sgbm, disparity;
sgbm->compute(left, right, disparity_sgbm);//将左右相机所得图片输入，计算出视差图disparity_sgbm
disparity_sgbm.convertTo(disparity, CV_32F, 1.0 / 16.0f);//格式转换
cv::imshow("disparity", disparity / 96.0);//归一化显示视差图
cv::waitKey(0);
```

让我们简单比较一下左右相机的图像和生成的视差图，可以看出，越近的地方值越大（越靠近白色），反之亦然。

![04-8](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\04-8.png)

![04-9](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\04-9.png)

![04-10](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\04-10.png)

再分析一下点云图的代码

```C++
// 生成点云
vector<Vector4d, Eigen::aligned_allocator<Vector4d>> pointcloud;

//计算左边相机所有像素点的深度并显示
for (int v = 0; v < left.rows; v++)
    for (int u = 0; u < left.cols; u++) {
        //剔除无效数据
        if (disparity.at<float>(v, u) <= 0.0 || disparity.at<float>(v, u) >= 96.0) continue;

        // 前三维为xyz,第四维为颜色
        Vector4d point(0, 0, 0, left.at<uchar>(v, u) / 255.0); 

        // 根据双目模型计算 point 的位置
        //计算归一化平面的x，y，并根据双目模型得到深度
        double x = (u - cx) / fx;
        double y = (v - cy) / fy;
        double depth = fx * b / (disparity.at<float>(v, u));
        //由归一化平面到相机坐标系，并将其放入容器中
        point[0] = x * depth;
        point[1] = y * depth;
        point[2] = depth;

        pointcloud.push_back(point);
    }
// 画出点云
showPointCloud(pointcloud);
```

效果图如下（还可以拖动看到别的角度的点云，这里就不展示了）

![04-11](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\04-11.png)

### joinMap.cpp

演示了如何根据相机的内外参以及其图像生成点云地图。原理就是我们这一讲讨论的式子
$$
ZP_{uv}=Z\begin{bmatrix}u\\v\\1\end{bmatrix}=\begin{bmatrix}f_x&0&c_x\\0&f_y&c_y\\0&0&1\end{bmatrix}
\begin{bmatrix}X\\Y\\Z\end{bmatrix}=KP_c=K(RP_w+t)=KTP_w\\
$$
只不过我们现在准备反其道而行之，已知像素坐标和内外参反推世界坐标系下空间点的坐标。

该例程根据相机内参，五个时刻下的位姿及拍下的五张彩色图及其对应的深度图，重建三维场景。

还是先分析代码（解释都在注释当中了）

```C++
typedef vector<Sophus::SE3d, Eigen::aligned_allocator<Sophus::SE3d>> TrajectoryType;
typedef Eigen::Matrix<double, 6, 1> Vector6d;
vector<cv::Mat> colorImgs, depthImgs;    // 彩色图和深度图
TrajectoryType poses;         // 相机位姿
ifstream fin("./pose.txt"); //位姿文件

for (int i = 0; i < 5; i++) {
    //读取彩色图和深度图
    boost::format fmt("./%s/%d.%s"); //图像文件格式
    colorImgs.push_back(cv::imread((fmt % "color" % (i + 1) % "png").str()));
    depthImgs.push_back(cv::imread((fmt % "depth" % (i + 1) % "pgm").str(), -1)); // 使用-1读取原始图像
	//读取位姿并将其转换成Sophus表示的变换矩阵SE3d
    double data[7] = {0};
    for (auto &d:data)
        fin >> d;
    Sophus::SE3d pose(Eigen::Quaterniond(data[6], data[3], data[4], data[5]),
                      Eigen::Vector3d(data[0], data[1], data[2]));
    poses.push_back(pose);
}

// 计算点云并拼接
// 相机内参 
double cx = 325.5;
double cy = 253.5;
double fx = 518.0;
double fy = 519.0;
double depthScale = 1000.0;
//点云存储的数据结构，并申请足够的空间，以免多次扩容
vector<Vector6d, Eigen::aligned_allocator<Vector6d>> pointcloud;
pointcloud.reserve(1000000);

for (int i = 0; i < 5; i++) {
    cout << "转换图像中: " << i + 1 << endl;
    cv::Mat color = colorImgs[i];
    cv::Mat depth = depthImgs[i];
    Sophus::SE3d T = poses[i];
    for (int v = 0; v < color.rows; v++)
        for (int u = 0; u < color.cols; u++) {
            unsigned int d = depth.ptr<unsigned short>(v)[u]; // 深度值
            if (d == 0) continue; // 为0表示没有测量到
            Eigen::Vector3d point;
            point[2] = double(d) / depthScale;//计算真实的深度
            //根据内参矩阵由像素平面反推到相机坐标系
            point[0] = (u - cx) * point[2] / fx;
            point[1] = (v - cy) * point[2] / fy;
            //根据外参由相机坐标系推到世界坐标系
            Eigen::Vector3d pointWorld = T * point;

            //申请六维向量存储三维空间点，前三维存放位置，后三维存放颜色
            Vector6d p;
            p.head<3>() = pointWorld;
            p[5] = color.data[v * color.step + u * color.channels()];   // blue
            p[4] = color.data[v * color.step + u * color.channels() + 1]; // green
            p[3] = color.data[v * color.step + u * color.channels() + 2]; // red
            pointcloud.push_back(p);
        }
}
//获得点云数量并显示
cout << "点云共有" << pointcloud.size() << "个点." << endl;
showPointCloud(pointcloud);
```

最后上一张效果图

![04-12](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\04-12.png)


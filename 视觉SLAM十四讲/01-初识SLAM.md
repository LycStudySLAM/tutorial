# 初识SLAM

以下内容为高翔《视觉SLAM十四讲》第二版的学习笔记

## 什么是SLAM？

SLAM的全称是Simultaneous Localization and Mapping，中文译作**同时定位与地图构建**。指搭载传感器的主体在**没有环境先验信息**的情况下，在运动的过程中建立环境的模型，同时估计自身的运动。

简单来说，SLAM需要通过传感器感知外部信息，经过一系列算法得到两个重要信息：自身的位置以及外在的环境。总结下来，SLAM需要解决两大问题：定位与建图。

## 传感器

为了解决定位和建图两大问题，传感器是必须的。传感器首先可以笼统的分为两类，一类是安装在执行SLAM的机构上的（如机器人），另一类则安装在环境中。显然，为了我们做出来的机器人适用于不依赖特殊场景的环境，安装在环境中的传感器首先不在考虑的范围内，直接考虑安装在SLAM执行机构上的传感器。

对于当下的SLAM而言，常用的传感器包括：RGB相机，RGB-D相机，双目相机，二维激光雷达，固态雷达，三维激光雷达，事件相机，IMU，轮式里程计，GPS等等。本书主要讨论视觉SLAM，而且我主要研究手持传感器的室内定位为主，所以主要的传感器为单目RGB相机，RGB-D相机，双目相机，事件相机，IMU。下面简单介绍一下各个传感器。

#### 单目RGB相机（Monocular）

只有一个摄像头，以一定帧率曝光，采集图像。由于只有一个摄像头，无法获取场景的深度。简单来说，就是近且大与远且小的物体，在单目相机看来没有区别。当然，我们可以换一个视角，这样就能知道到底是近且大还是远且小，但是在运动的过程中，我们仍只能获得物体的相对远近，而不是一个真实世界的数值，它们之间差一个比例因子，称为尺度。所以，单目相机直接用于SLAM有两大问题：运动情况下才能确定深度，具有尺度不确定性。



为了解决无法在无运动情况下获取深度，又有以下两类相机。

#### 双目相机（Stereo）

有两个摄像头，两个摄像头相对位置固定，两者距离称为基线（Baseline），可以在标定内外参数后经三角测量计算出视场中各像素的距离，但是计算过程十分消耗资源。

#### RGB-D相机

有一个摄像头和一个用于测量深度的传感器，相机原始返回数据即为RGB图像及每一像素相对相机坐标系的距离。省去了双目相机的计算过程，但是测量距离，噪声等性能差于双目相机，且比较难用于室外。



为了适用于更为苛刻的场景，如高速运动，同时用于室内外环境，又有以下一类相机。

#### 事件相机（Event）

与传统相机不一致，事件相机仅返回事件，一个事件即视场中某一点的亮度变化超过阈值时，返回变化时间，像素坐标，以及亮度增大或是减小。其响应时间短，动态范围较大，可以在高速场景及曝光变化较大的场景下也有很好的表现。同时也无法直接获得深度，需要配合其他传感器进行融合使用。



除此之外，还有测量自身运动的传感器。

#### 惯性测量单元（IMU）

测量运动的加速度和角速度。

## 经典视觉SLAM框架

经典的视觉SLAM框架分为四部分：前端视觉里程计（Visual Odometry），后端（非线性）优化（Optimization），回环检测（Loop Closure Detection），建图（Mapping），当然要完成整个过程，还需要传感器硬件驱动，即传感器信息的获得，以及数据预处理，不过直接使用数据集也是可以的。除此之外，框架前端还应该有一个读取传感器信息的接口。

#### 视觉里程计

通过相邻几帧图像，根据几何关系，估计出相机的运动同时可以恢复三维的场景。视觉里程计就如常见的里程计一样，只提供相邻时刻的运动，而与过去很长一段时间的运动没有关系。可以想象通过视觉里程计我们可以得到相机每一时刻的位姿，解决了定位问题，再根据每一个位姿下各点相对相机坐标系的坐标，就可以得到世界坐标系下各点坐标，解决了建图问题。当然，这只是美好的想象，因为运动估计存在误差，如果长时间会产生很大的累积漂移，导致定位和建图都与真实值有很大出入。

#### 后端（非线性）优化

由于噪声的存在，前端视觉里程计不能完成整个SLAM任务，需要后端优化来消除累积误差。后端主要是表达出定位与建图的不确定性，再采用滤波与非线性优化算法来估计出状态的均值和方差。

（状态不确定性如何表示？如何获得最优估计？）

#### 回环检测

主要用于解决累积误差问题。将新读入的图像与之前的图像进行比对，如果相似度很高，则认为是同一个地方，并把这个信息交给后端优化，后端优化则可以把这两个图像之间所有时刻的位姿和建图进行优化。

（工程上保留信息多长时间用于回环检测？如果长期保存，如何解决内存问题？信息是图像本身还是特征？）

#### 建图

主要分为度量地图和拓扑地图。

度量地图强调精确地表示地图中物体的位置关系。可以简单分为稀疏地图和稠密地图。稀疏地图仅仅用一些路标组成（可以理解为特征点构成的地图），稠密地图由图像看到的所有信息组成。若是定位，只需要稀疏地图，而导航则需要稠密地图。

拓扑地图只关心两点之间是否连通，由一个图（计算机中图论的图）组成。

## SLAM数学表达

### 运动方程

$$
x_k=f(x_{k-1},u_k,w_k)
$$
x表示每一个位姿，k表示运动产生的第k个位姿，u表示运动传感器数据，w为噪声。

这就是一个基本的里程计模型，由上一时刻的位姿与这一时刻的（含噪声的）传感器数据进行计算，得到最可靠的这一时刻的位姿。本质上由于增添了噪声，位姿为一随机模型，可以假设为高斯分布或是其他的一些合理的分布。

### 观测方程

$$
z_{k,j}=h(y_j,x_k,v_{k,j})
$$
其中y表示当前地图中的路标，x仍然表示位姿，v表示观测时带来的传感器误差。z则是这一位姿下测量传感器对这一路标的（含有噪声）的观测。很明显，这也是一个随机模型。

---

对于不同方式计算，或是不同传感器输入，亦或是不同的维度，上述的两个方程可以被参数化成不同的方程，但是总的来说，都可以是这样表示。本质上来说，这两个方程组成了一个状态估计问题：如何根据运动传感器和测量传感器的输入，得到每一时刻的定位x和地图路标y？这就是SLAM的核心问题：同步定位与建图。

（具体如何参数化？如何滤波和非线性优化？滤波算法和优化算法有哪些？有没有成熟的算法库？）



## 实践板块

为了方便，我使用虚拟机安装ubuntu18.04.5，并且安装了g++，cmake，ROS

cmake的简单使用参考博客：https://blog.csdn.net/afei__/article/details/81201039

深入使用有官方教程：https://cmake.org/cmake/help/latest/guide/tutorial/index.html

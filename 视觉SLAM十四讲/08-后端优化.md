# 后端优化

以下内容为高翔《视觉SLAM十四讲》第二版的学习笔记

## 回顾

前面两讲我们了解了构建视觉里程计这一部分的常用方法，之前我们曾提到视觉里程计只能构建一个短时间的轨迹和地图，因为它只与前一帧或几帧相关，得到的位姿变换必然具有一定的误差，而长时间的运行会使这个误差越来越大，以至于轨迹和地图都出现较大的漂移。为了获得长时间内的最优轨迹与地图，我们需要引入**后端优化**这一部分。

## 状态估计

在05-非线性优化一讲中，我们曾提到过状态估计这一概念，现在我们重新列出SLAM问题的运动方程和观测方程
$$
记位姿x_0,x_1,...,x_N,路标y_1,...y_M\\
\begin{cases}
x_k=f(x_{k-1},u_k)+w_k\\
z_{k,j}=h(y_j,x_k)+v_{k,j}
\end{cases}
$$
注意：①在SLAM问题中由于关键点众多，所以一般而言观测方程远多于运动方程。②我们可能没有运动测量的装置，如此一来我们就仅有观测方程而没有运动方程。（事实上，到现在为止之前的讨论都是基于观测方程，因为我们的确没有测量运动的装置）

因为每一个位姿x和路标y都受噪声影响，所以x,y并不是确定的值而是一个服从某种概率分布的随机变量。高斯滤波中，我们将待估计的位姿x和路标y以及噪声都认为是服从高斯分布的，因为高斯分布由均值和方差（协方差矩阵）两个参数确定，所以我们仅仅需要存储其均值和协方差矩阵并对其进行更新即可。现在的问题是：我们怎么**根据运动数据u和观测数据z来估计和更新状态变量位姿x和路标y**？

我们首先直观的描述一下运动和观测这两个动作对上述讨论的估计量的影响。当只有运动方程时，我们明白自身每一步的运动是怎样的，但由于没有观测且运动数据有噪声，时间越长，我们对自身位置估计的不确定性就越大。当有观测方程时，由于能不断的观测到外部场景，我们可以根据之前的方法，使得自身位置估计的不确定性减小。下图形象的描述了这一现象。

![08-1](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\08-1.png)

现在我们对这个过程进行数学建模，
$$
定义状态变量x_k=\{x_k,y_1,...,y_M\},\\此时有运动方程和观测方程
\begin{cases}
x_k=f(x_{k-1},u_k)+w_k\\
z_{k}=h(x_k)+v_{k}
\end{cases}
$$
对于第k时刻的状态变量，我们用k之前所有的信息对其进行估计，认为其概率分布为
$$
p(x_k|x_{0:k-1},u_{1:k},z_{1:k})
$$
我们首先按照贝叶斯公式，可以有
$$
p(x_k|x_{0:k-1},u_{1:k},z_{1:k})\propto p(z_k|x_k)p(x_k|x_{0:k-1},u_{1:k},z_{1:k-1})
$$
可以看出先验部分是在观测之前的状态量估计的概率分布，似然描述的是“什么样的当前状态最有可能观察到这样的观测”这样一个问题，后验则是通过测量值更新后状态量估计的概率分布。先验又至少与k-1时刻的状态x密切相关，对于连续而非离散的状态量，有
$$
p(x_k|x_{0:k-1},u_{1:k},z_{1:k-1})=
\int p(x_k|x_{k-1},x_{0:k-2},u_{1:k},z_{1:k-1})
p(x_{k-1}|x_{0:k-2},u_{1:k},z_{1:k-1})dx_{k-1}
$$
对这一步的处理，可以分为两类算法：一种是**基于递归状态估计的滤波器算法**，它只考虑k-1时刻对k时刻的影响而不再考虑更早时刻的影响；另一种是将k时刻前所有时刻全部考虑，构成**以非线性优化为主体的优化框架**，目前主流的方法是非线性优化方法，它在精度和鲁棒性上都胜过了滤波器算法。

接下来，我会介绍高斯滤波算法和非线性优化算法，由于主流方法是非线性优化算法，我将非线性优化章节放在前面介绍，在实践模块之后再介绍滤波器算法，对滤波器算法不感兴趣的读者可以跳过滤波器算法一节。

## BA与图优化

我们再次把投影模型的全过程列出来。考虑一个世界坐标系下的空间点p的投影过程
$$
将世界坐标系变换到相机坐标系:P_c=[X_c,Y_c,Z_c]^T\\
将相机坐标系空间点投影到归一化平面:p_c=[u_c,v_c,1]^T=[\frac{X_c}{Z_c},\frac{Y_c}{Z_c},1]^T\\
暂且考虑径向畸变,得到畸变后的坐标:
\begin{cases}
u_c'=u_c(1+k_1r_c^2+k_2r_c^4)\\
v_c'=v_c(1+k_1r_c^2+k_2r_c^4)
\end{cases}\\
根据内参,计算像素平面上的像素坐标:
\begin{cases}
u_s=f_xu_c'+c_x\\
v_s=f_yv_c'+c_y
\end{cases}
$$
我们之前将其记为观测方程
$$
z=h(x,y)
$$
现在，我们将其参数化。x代表相机位姿，可以由变换矩阵T表示；y代表路标，这里可以用三维空间点p表示，观测数据z则是最后投影到像素平面上的结果，也就是说我们可以定义观测误差项e如下
$$
e=z-h(T,p)\\
z=[u_s,v_s]为投影点像素坐标,T为变换矩阵,p为空间点
$$
现在，我们每个位姿产生的若干个观测的全部观测误差求和，那么整体的误差项（也即最小二乘的代价函数）为
$$
\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^n||e_{ij}||=
\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^n||z_{i,j}-h(T_i,p_j)||
$$
对这个最小二乘问题的求解，将同时对位姿和空间点估计做调整。

按照最小二乘问题的求解方式，我们已经确定了误差项，下一步就是求误差项的一阶导数（也即雅可比矩阵）



## 滤波器算法

现在我们来介绍曾是主流算法的滤波器算法，本讲只包含高斯滤波中的KF和EKF算法，如有对高斯滤波及粒子滤波感兴趣的读者可以阅读《概率机器人》中的 第三章：高斯滤波 和 第四章：非参数滤波。

递归状态估计的一大假设是一阶马尔科夫性假设，即其认为该时刻的状态仅与上一时刻状态相关，与之前的时刻无关。根据上文状态估计一节我们推导的式子，引入一阶马尔科夫假设进行化简
$$
p(x_k|x_{k-1},x_{0:k-2},u_{1:k},z_{1:k-1})=p(x_k|x_{k-1},u_k)\\
p(x_{k-1}|x_{0:k-2},u_{1:k},z_{1:k-1})=p(x_{k-1}|x_{0:k-2},u_{1:k-1},z_{1:k-1})
$$
我们不妨记
$$
bel(x_k)=p(x_k|x_{0:k-1},z_{1:k},u_{1:k})\\
\overline{bel}(x_k)=p(x_k|x_{0:k-1},z_{1:k-1},u_{1:k})
$$
上面的过程正是经典的贝叶斯滤波算法
$$
\overline{bel}(x_k)=\int p(x_k|x_{k-1},u_k)bel(x_{k-1})dx_{k-1}&\\
bel(x_k)=\eta p(z_k|x_k)\overline{bel}(x_k)&\\
其中,\eta是待确定的常数,我们暂且用此表示正比关系。
$$
上面的两式就是状态更新的两个步骤：**控制更新**和**测量更新**。如果读者对这部分不熟悉，建议阅读《概率机器人》第二章：递推状态估计

众所周知，贝叶斯滤波算法并不是实际的算法，我们需要将一定的概率分布放入算法中才能工作，比如我们之前讨论的的高斯分布，下面我们将高斯分布带入式中进行分析。

### 线性高斯系统与KF

$$
线性高斯系统的运动方程和观测方程如下
\begin{cases}
x_t=A_tx_{t-1}+B_tu_t+w_t\\
z_t=C_tx_t+v_t
\end{cases}\\
w\sim N(0,R),\;v\sim N(0,Q)\\
$$

由于推导十分复杂，我先给出结果，接下来是详细的推导，需要读者有一定的线性代数和概率论知识。
$$
\overline{\mu_t}=A_t\mu_{t-1}+B_tu_t&\\
\overline{\Sigma_t}=A_t\Sigma_{t-1}A_t^T+R_t&\\
 K_t=\overline{\Sigma_t}C_t^T(C_t\overline{\Sigma_t}C_t^T+Q_t)^{-1}&\\
\mu_t=\overline{\mu_t}+K_t(Z_t-C_t\overline{\mu_t})&\\
\Sigma_t=(I-K_tC_t)\overline{\Sigma_t}&\\
$$

#### 控制更新步骤推导

**贝叶斯到高斯**

首先，贝叶斯滤波定位控制更新步骤如下
$$
\overline{bel}(x_t)=\int p(x_t|x_{t-1},u_t)bel(x_{t-1})dx_{t-1}
$$
代入线性高斯系统的表示，则
$$
\overline{bel}(x_t)=\eta\int 
exp(-\frac{1}{2}(x_t-A_tx_{t-1}-B_tu_t)^TR_t^{-1}(x_t-A_tx_{t-1}-B_tu_t))
exp(-\frac{1}{2}(x_{t-1}-\mu_{t-1})^T\Sigma_t^{-1}(x_{t-1}-\mu_{t-1}))
dx_{t-1}
$$
将指数项相加记为一符号如下
$$
\overline{bel}(x_t)=\eta\int exp|-L_t|dx_{t-1}\\
L_t=\frac{1}{2}(x_t-A_tx_{t-1}-B_tu_t)^TR_t^{-1}(x_t-A_tx_{t-1}-B_tu_t)+
\frac{1}{2}(x_{t-1}-\mu_{t-1})^T\Sigma_t^{-1}(x_{t-1}-\mu_{t-1})
$$
此时，为了积分的便利，需要做一个函数分解
$$
L_t=L_t(x_t,x_{t-1})+L_t(x_t)\\
\rightarrow\overline{bel}(x_t)=\eta exp|-L_t(x_t)|\int exp|-L_t(x_t,x_{t-1})|dx_{t-1}
$$
注意，分解的同时包含t，t-1两时刻的状态作为参数的L，其务必在积分后与积分值不依赖与t时刻的x，即
$$
\overline{bel}(x_t)=\eta exp|-L_t(x_t)|\int exp|-L_t(x_t,x_{t-1})|dx_{t-1}=\eta^{'} exp|-L_t(x_t)|
$$

**函数分解**

以其一阶偏导的零点为均值，二阶偏导为方差，构成一个新的高斯分布，因为对于一个关于x的二次型而言，其一阶导零点正是均值，二阶导则是协方差的逆，为了满足高斯分布的需求，应该尝试去构成一个这样的二次型，下面的步骤正是在执行上面的说法。
$$
\frac{\partial L_t}{\partial x_{t-1}}=A_tR_t^{-1}(x_t-A_tx_{t-1}-B_tu_t)+\Sigma_t^{-1}(x_{t-1}-\mu_{t-1})\\
\frac{\partial^2 L_t}{\partial x_{t-1}^2}=A_t^TR_t^{-1}A_t+\Sigma_t^{-1}=\Psi_t^{-1}
$$
令一阶导数为0,化简式，可得
$$
x_{t-1}=\Psi_t(A_tR_t^{-1}(x_t-B_tu_t)+\Sigma_t^{-1}\mu_{t-1})
$$
则定义L函数如下
$$
L_t(x_t,x_{t-1})=\frac{1}{2}
(x_{t-1}-\Psi_t(A_tR_t^{-1}(x_t-B_tu_t)+\Sigma_t^{-1}\mu_{t-1}))^T
\Psi_t^{-1}
(x_{t-1}-\Psi_t(A_tR_t^{-1}(x_t-B_tu_t)+\Sigma_t^{-1}\mu_{t-1}))
$$
其积分为
$$
\int det(2\pi\Psi)^{-\frac{1}{2}}exp|-L_t(x_t,x_{t-1})|dx_{t-1}=1\\
\int exp|-L_t(x_t,x_{t-1})|dx_{t-1}=det(2\pi\Psi)^{\frac{1}{2}}
$$
从而
$$
\overline{bel}(x_t)=\eta^{'} exp|-L_t(x_t)|
$$

**分解后的计算**
$$
L_t(x_t)=L_t-L_t(x_t,x_{t-1})=
\frac{1}{2}(x_t-A_tx_{t-1}-B_tu_t)^TR_t^{-1}(x_t-A_tx_{t-1}-B_tu_t)+\\
\frac{1}{2}(x_{t-1}-\mu_{t-1})^T\Sigma_t^{-1}(x_{t-1}-\mu_{t-1})-\\
\frac{1}{2}
(x_{t-1}-\Psi_t(A_tR_t^{-1}(x_t-B_tu_t)+\Sigma_t^{-1}\mu_{t-1}))^T
\Psi_t^{-1}
(x_{t-1}-\Psi_t(A_tR_t^{-1}(x_t-B_tu_t)+\Sigma_t^{-1}\mu_{t-1}))\\
\Psi_t^{-1}=A_t^TR_t^{-1}A_t+\Sigma_t^{-1}
$$

联立并化简上面两式，可以得到
$$
L_t(x_t)=\frac{1}{2}(x_t-B_tu_t)^TR_t^{-1}(x_t-B_tu_t)+\frac{1}{2}\mu_{t-1}^T\Sigma_{t-1}^{-1}\mu_{t-1}-\\
\frac{1}{2}(A_t^TR_t^{-1}(x_t-B_tu_t)+\Sigma_{t-1}^{-1}\mu_{t-1})^T
(A_t^TR_t^{-1}A_t+\Sigma_t^{-1})^{-1}
(A_t^TR_t^{-1}(x_t-B_tu_t)+\Sigma_{t-1}^{-1}\mu_{t-1})
$$
其中，所有关于t-1时刻的状态量x全部被消去，并且L函数是关于t时刻状态量x的二次型，这意味着基于以前状态的后验函数的确服从正态分布。均值和方差应该分别对应L(x)的极小值和曲率,由于求解这两部分的数学推导过于复杂，且需要基于一些引理，故直接给出。

**重要引理**
$$
(R+PQP^T)^{-1}=R^{-1}-R^{-1}P(Q^{-1}+P^TR^{-1}P)P^TR^{-1}
$$
极小值的求解过程
$$
\frac{\partial L_t}{\partial x_{t}}=(R_t+A_t\Sigma_{t-1}A_t^T)^{-1}(x_t-B_tu_t)-
R_t^{-1}A_t(A_t^TR_t^{-1}A_t+\Sigma_{t-1}^{-1})\Sigma_{t-1}^{-1}\mu_{t-1}=0\\
\rightarrow x_t=B_tu_t+(R_t+A_t\Sigma_{t-1}A_t^T)R_t^{-1}A_t(A_t^TR_t^{-1}A_t+\Sigma_{t-1}^{-1})\Sigma_{t-1}^{-1}\mu_{t-1}\\
(R_t+A_t\Sigma_{t-1}A_t^T)R_t^{-1}A_t=A_t+A_t\Sigma_{t-1}A_t^TR_t^{-1}A_t=A_t(I+\Sigma_{t-1}A_t^TR_t^{-1}A_t)\quad (1)\\
(AB)^{-1}=B^{-1}A^{-1}\rightarrow 
(A_t^TR_t^{-1}A_t+\Sigma_{t-1}^{-1})^{-1}\Sigma_{t-1}^{-1}=(\Sigma_{t-1}(A_t^TR_t^{-1}A_t+\Sigma_{t-1}^{-1}))^{-1}=(I+\Sigma_{t-1}A_t^TR_tA_t)^{-1}\quad(2)\\
(R_t+A_t\Sigma_{t-1}A_t^T)R_t^{-1}A_t(A_t^TR_t^{-1}A_t+\Sigma_{t-1}^{-1})^{-1}\Sigma_{t-1}^{-1}=
A_t(I+\Sigma_{t-1}A_t^TR_t^{-1}A_t)(I+\Sigma_{t-1}A_t^TR_tA_t)^{-1}=A_t\\
\rightarrow x_t=B_tu_t+A_t\mu_{t-1}
$$
从而有t时刻未更新测量的状态x的均值
$$
\overline{\mu_t}=B_tu_t+A_t\mu_{t-1}
$$
曲率的求解过程
$$
\frac{\partial^2 L_t}{\partial x_{t-1}^2}=(A_t^T\Sigma_{t-1}^{-1}A_t+R_t)^{-1}
\rightarrow \overline{\Sigma_t}=A_t^T\Sigma_{t-1}^{-1}A_t+R_t
$$

#### 测量更新步骤推导

**贝叶斯到高斯**
$$
bel(x_t)=\eta p(z_t|x_t)\overline{bel}(x_t)=\eta' 
exp(-\frac{1}{2}(z_t-C_tx_t)^TQ_t^{-1}(z_t-C_tx_t))
exp(-\frac{1}{2}(x_t-\overline{\mu_t})^T\overline{\Sigma_t}^{-1}(x_t-\overline{\mu_t}))
$$

与控制更新同样的思路，我们采用指数项合并，用函数J来表示指数项
$$
bel(x_t)=\eta exp(-J_t)\\
J_t=\frac{1}{2}(z_t-C_tx_t)^TQ_t^{-1}(z_t-C_tx_t)+\frac{1}{2}(x_t-\overline{\mu_t})^T\overline{\Sigma_t}^{-1}(x_t-\overline{\mu_t})
$$

**二次型的计算**

这个函数是x的二次型，所以显然地，bel(x)也是一个高斯分布，同样地，求一阶与二阶偏导数
$$
\frac{\partial J_t}{\partial x_{t}}=-C_t^TQ_t^{-1}(z_t-C_tx_t)+\overline{\Sigma_t}^{-1}(x_t-\overline{\mu_t})\\
\frac{\partial^2 J_t}{\partial x_t^2}=C_t^TQ_t^{-1}C_t+\overline{\Sigma_t}^{-1}
$$
方差就是二阶导（曲率）的逆,这是因为bel是一个高斯分布，那么二次型的二阶导正是协方差的逆
$$
\Sigma_t=(C_t^TQ_t^{-1}C_t+\overline{\Sigma_t}^{-1})^{-1}
$$
均值是一阶导的零点，也是函数的极小值，从正态分布的定义可以看出这一点，因为正态分布式一个二次型，其一阶导的零点正是自变量与均值作差的线性函数，那么，求得该零点即是均值。求均值的过程，可将状态x换为均值μ
$$
C_t^TQ_t^{-1}(z_t-C_t\mu_t)=\overline{\Sigma_t}^{-1}(\mu_t-\overline{\mu_t})
$$
对左式进行变形处理
$$
C_t^TQ_t^{-1}(z_t-C_t\mu_t)=C_t^TQ_t^{-1}(z_t-C_t\mu_t+C_t\overline{\mu_t}-C_t\overline{\mu_t})\\=
C_t^TQ_t^{-1}(z_t-C_t\overline{\mu_t})-C_t^TQ_t^{-1}C_t(\mu_t-\overline{\mu_t})
$$
移项
$$
C_t^TQ_t^{-1}(z_t-C_t\overline{\mu_t})=(C_t^TQ_t^{-1}C_t+\overline{\Sigma_t}^{-1})(\mu_t-\overline{\mu_t})=\Sigma_t^{-1}(\mu_t-\overline{\mu_t})\\
\rightarrow \Sigma_tC_t^TQ_t^{-1}(z_t-C_t\overline{\mu_t})=(\mu_t-\overline{\mu_t})
\rightarrow \mu_t = \overline{\mu_t}+\Sigma_tC_t^TQ_t^{-1}(z_t-C_t\overline{\mu_t})
$$
可以说，根据以上的处理，可以得到两个完备的式子,这是第一种完备的表示法
$$
\Sigma_t=(C_t^TQ_t^{-1}C_t+\overline{\Sigma_t}^{-1})^{-1}\\
\mu_t = \overline{\mu_t}+\Sigma_tC_t^TQ_t^{-1}(z_t-C_t\overline{\mu_t})
$$
这两个式子完整的表达了测量更新的过程

#### 另一种表示法

为了μ并不依赖于Σ可以直接得到，推导另一种表示法如下

首先，记卡尔曼增益矩阵
$$
K_t=\Sigma_tC_t^TQ_t^{-1}
$$
变换如下
$$
K_t=\Sigma_tC_t^TQ_t^{-1}=\Sigma_tC_t^TQ_t^{-1}(C_t^T\overline{\Sigma_t}C_t+Q_t)(C_t^T\overline{\Sigma_t}C_t+Q_t)^{-1}\\=
\Sigma_t(C_t^TQ_t^{-1}C_t^T\overline{\Sigma_t}C_t+C_t^T)(C_t^T\overline{\Sigma_t}C_t+Q_t)^{-1}\\=
\Sigma_t(C_t^TQ_t^{-1}C_t^T\overline{\Sigma_t}C_t+\overline{\Sigma_t}^{-1}\overline{\Sigma_t}C_t^T)(C_t^T\overline{\Sigma_t}C_t+Q_t)^{-1}\\=
\Sigma_t\overline{\Sigma_t}C_t^T(C_t^TQ_t^{-1}C_t^T+\overline{\Sigma_t}^{-1})(C_t^T\overline{\Sigma_t}C_t+Q_t)^{-1}\\
because:\Sigma_t=(C_t^TQ_t^{-1}C_t+\overline{\Sigma_t}^{-1})^{-1}\\
thus:K_t=\overline{\Sigma_t}C_t^T(C_t^T\overline{\Sigma_t}C_t+Q_t)^{-1}
$$
那么均值显然可以表达为
$$
\mu_t = \overline{\mu_t}+K_t(z_t-C_t\overline{\mu_t})
$$
对于方差，根据下面的引理
$$
(R+PQP^T)^{-1}=R^{-1}-R^{-1}P(Q^{-1}+P^TR^{-1}P)P^TR^{-1}
$$
有
$$
\Sigma_t=(C_t^TQ_t^{-1}C_t+\overline{\Sigma_t}^{-1})^{-1}=\overline{\Sigma_t}-\overline{\Sigma_t}C_t^T(Q_t^{-1}+C_t\overline{\Sigma_t}C_t^T)^{-1}C_t\overline{\Sigma_t}\\=
(I-\overline{\Sigma_t}C_t^T(Q_t^{-1}+C_t\overline{\Sigma_t}C_t^T)^{-1}C_t)\overline{\Sigma_t}\\
because:K_t=\overline{\Sigma_t}C_t^T(Q_t^{-1}+C_t\overline{\Sigma_t}C_t^T)^{-1}\\
thus:\Sigma_t=(I-K_tC_t)\overline{\Sigma_t}
$$
这样，就得到了第二种完备的表示法
$$
K_t=\overline{\Sigma_t}C_t^T(C_t^T\overline{\Sigma_t}C_t+Q_t)^{-1}\\
\mu_t = \overline{\mu_t}+K_t(z_t-C_t\overline{\mu_t})\\
\Sigma_t=(I-K_tC_t)\overline{\Sigma_t}
$$

到此为止，我们完成了卡尔曼滤波器的推导。需要注意的是，高斯分布的线性变换仍然是高斯分布，因此我们进行的状态估计是状态的最优无偏估计。但是许多的系统并非线性系统，为了扩展高斯滤波的应用场景，我们引入拓展卡尔曼滤波。

### 非线性高斯系统与EKF

$$
非线性高斯系统的运动方程和观测方程：
\begin{cases}
x_t=g(u_t,x_{t-1})+w_t\\
z_t=h(x_t)+v_t\\
\end{cases}\\
w\sim N(0,R),\;v\sim N(0,Q)
$$

显然地，经过一个非线性系统后，置信度为高斯的先验得到的后验不再是一个高斯分布，这个分布甚至不再是单峰的，为了得到一种近似，同时保持高斯分布的性质，必须将其进行线性化，然后估计出均值和方差，最终获得的是一个近似后验分布的高斯分布。

**泰勒展开线性化**

记非线性函数g的一阶偏导如下
$$
g'(u_t,x_{t-1})=\frac{\partial g(u_t,x_{t-1})}{\partial x_{t-1}}=G_t
$$
从而将非线性函数g泰勒展开如下
$$
g(u_t,x_{t-1})\approx g(u_t,\mu_{t-1})+G_t(x_{t-1}-\mu_{t-1})
$$
则有状态转移概率
$$
p(x_t|x_{t-1},u_t)\approx det(2\pi R_t)^{-\frac{1}{2}}exp(-\frac{1}{2}(
x_t-g(u_t,\mu_{t-1})-G_t(x_{t-1}-\mu_{t-1}))^T
R_t^{-1}
(x_t-g(u_t,\mu_{t-1})-G_t(x_{t-1}-\mu_{t-1})))
$$
对于测量概率，也有类似的近似方案
$$
h'(x_t)=\frac{\partial h(x_t)}{\partial x_t}=H_t\\
h(x_t)\approx h(\mu_t)+H_t(x_t-\mu_t)\\
p(z_t|x_t)=det(2\pi Q_t)^{-\frac{1}{2}}exp(-\frac{1}{2}(
x_t-h(\mu_t)-H_t(x_t-\mu_t))^T
Q_t^{-1}
(x_t-h(\mu_t)-H_t(x_t-\mu_t))
$$

因为拓展卡尔曼滤波的推导仍然十分复杂，所以我仍然选择先给出
$$
\overline{\mu_t}=g(u_t,x_{t-1})\\
\overline{\Sigma_t}=G_t\Sigma_{t-1}G_t^T+R_t\\
K_t=\overline{\Sigma_t}H_t^T(H_t\overline{\Sigma_t}H_t^T+Q_t)^{-1}\\
\mu_t=\overline{\mu_t}+K_t(Z_t-H(\overline{\mu_t}))\\
\Sigma_t=(I-K_tH_t)\overline{\Sigma_t}\\
$$

可以看出，经过线性化之后，控制更新中，均值和之前的求法一致，只不过由线性函数变成了非线性函数g，方差中将关于之前状态的线性参数矩阵A换为了非线性函数g对状态x的一阶导雅可比矩阵G。测量更新中，将线性函数变成了非线性函数H，将关于状态的线性参数矩阵C换为了非线性函数h对状态x的一阶导雅可比矩阵H。总体上来说，变动并不大，而且从线性化的角度来看还是好理解的。

无论如何，一些必要的推导还是需要的，接下来，仿照卡尔曼滤波的推导，将扩展卡尔曼推导如下。

#### 控制更新步骤推导

**贝叶斯到高斯**
$$
\overline{bel}(x_t)=\int p(x_t|x_{t-1},u_t)bel(x_{t-1})dx_{t-1}=\\
\int det(2\pi R_t)^{-\frac{1}{2}}exp(-\frac{1}{2}(
x_t-g(u_t,\mu_{t-1})-G_t(x_{t-1}-\mu_{t-1}))^T
R_t^{-1}
(x_t-g(u_t,\mu_{t-1})-G_t(x_{t-1}-\mu_{t-1})))\\
det(2\pi \Sigma_{t-1})^{-\frac{1}{2}}exp(-\frac{1}{2}
(x_{t-1}-\mu_{t-1}))^T\Sigma_{t-1}^{-1}(x_{t-1}-\mu_{t-1}))dx_{t-1}\\
\rightarrow\overline{bel}(x_t)=\eta\int exp|-L_t|dx_{t-1}\\
L_t=\frac{1}{2}(x_t-g(u_t,\mu_{t-1})-G_t(x_{t-1}-\mu_{t-1}))^TR_t^{-1}
(x_t-g(u_t,\mu_{t-1})-G_t(x_{t-1}-\mu_{t-1}))+\\
\frac{1}{2}(x_{t-1}-\mu_{t-1}))^T\Sigma_{t-1}^{-1}(x_{t-1}-\mu_{t-1})
$$

**类似卡尔曼的推导**

依然是对其做函数分解，过程与卡尔曼的推导一致，由极小值和曲率确定t-1时刻状态x的二次型，使t-1时刻状态x积分值与t时刻x无关，收到常数项中。另一部分同样求极小值和曲率，得到的结果就是新的二次型的均值以及方差的逆，这部分与卡尔曼推导的过程基本一致，都是对二次型的求导，略去计算步骤，直接给出下面的结果
$$
L_t=L_t(x_t,x_{t-1})+L_t(x_t)\\
L_t(x_t,x_{t-1})=\frac{1}{2}(x_{t-1}-\Phi_t(G_tR_t^{-1}(x_t-g(u_t,\mu_{t-1})+G_t\mu_{t-1})+\Sigma_{t-1}^{-1}\mu_{t-1})^T\Phi_t^{-1}\\(x_{t-1}-\Phi_t(G_tR_t^{-1}(x_t-g(u_t,\mu_{t-1})+G_t\mu_{t-1})+\Sigma_{t-1}^{-1}\mu_{t-1})\\
\Phi_t=(G_t^TR_{t-1}G_t+\Sigma_{t-1}^{-1})\\
L_t(x_t)=\frac{1}{2}(x_t-g(u_t,\mu_{t-1})+G_t\mu_{t-1})^TR_t^{-1}(x_t-g(u_t,\mu_{t-1})+G_t\mu_{t-1})+\\
\frac{1}{2}(x_{t-1}-\mu_{t-1}))^T\Sigma_{t-1}^{-1}(x_{t-1}-\mu_{t-1})-
\frac{1}{2}(G_tR_t^{-1}(x_t-g(u_t,\mu_{t-1})+G_t\mu_{t-1})+\Sigma_{t-1}^{-1}\mu_{t-1}))^T\Phi_t\\
(G_tR_t^{-1}(x_t-g(u_t,\mu_{t-1})+G_t\mu_{t-1})+\Sigma_{t-1}^{-1}\mu_{t-1}))
$$
对最后分解得到的L(x)求极小值和曲率，即是均值以及方差的逆,并经过类似卡尔曼推导过程的化简，最终得到
$$
\overline{\mu_t}=g(u_t,x_{t-1})\\
\overline{\Sigma_t}=G_t\Sigma_{t-1}G_t^T+R_t
$$

#### 测量更新步骤推导

**贝叶斯到高斯**
$$
bel(x_t)=\eta p(z_t|x_t)\overline{bel}(x_t)=\eta' 
exp(-\frac{1}{2}(z_t-h(\overline{\mu_t})-H_t(x_t-\overline{\mu_t}))^TQ_t^{-1}(z_t-h(\overline{\mu_t})-H_t(x_t-\overline{\mu_t}))\\
exp(-\frac{1}{2}(x_t-\overline{\mu_t})^T\overline{\Sigma_t}^{-1}(x_t-\overline{\mu_t}))\\
\rightarrow
bel(x_t)=\eta exp(-J_t)\\
J_t=\frac{1}{2}(z_t-h(\overline{\mu_t})-H_t(x_t-\overline{\mu_t}))^TQ_t^{-1}(z_t-h(\overline{\mu_t})-H_t(x_t-\overline{\mu_t}))+\frac{1}{2}(x_t-\overline{\mu_t})^T\overline{\Sigma_t}^{-1}(x_t-\overline{\mu_t})
$$

**类似卡尔曼的推导**

接下来的工作就是对非线性函数J求关于状态x的偏导，其极小值和曲率分别对应着均值以及方差的逆，计算步骤与卡尔曼的推导步骤基本一致，本质上都是二次型的求导，略去计算步骤，直接给出下面的结果
$$
K_t=\overline{\Sigma_t}H_t^T(H_t^T\overline{\Sigma_t}H_t+Q_t)^{-1}\\
\mu_t = \overline{\mu_t}+K_t(z_t-h(\overline{\mu_t}))\\
\Sigma_t=(I-K_tH_t)\overline{\Sigma_t}
$$

### 关于EKF的讨论

早期的SLAM中，EKF占据主要地位，除此之外高斯滤波及粒子滤波等改进算法（都是滤波器算法）在SLAM也有不少的应用。但时至今日，EKF有太多的局限，让我们不得不在大部分SLAM场景下放弃这一算法。下面主要谈谈它的问题。

①滤波器算法几乎都有马尔科夫假设，但是对于回环这种情况，当前帧与若干帧之前确实有关系，也就是说马尔科夫假设未必成立。对于回环漂移现象，滤波器算法难以解决。②EKF对非线性系统上一状态仅仅做了一次线性化，对局部线性化条件不好的非线性系统，这将会带来较大的非线性误差，并且对应到优化中，单次线性化这只是一次迭代过程。③如果把路标放到状态变量中，需要维护的均值和协方差矩阵的规模将非常大，大型场景下使用起来对设备的要求较高。④没有异常检测机制，实用中稳定性较差。

由于这些问题，滤波器算法慢慢淡出主流SLAM方案。本讲对其介绍的主要目的是希望读者对后端优化算法有更多的了解。

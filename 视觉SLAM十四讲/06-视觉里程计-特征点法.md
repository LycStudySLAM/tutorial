# 视觉里程计-特征点法

以下内容为高翔《视觉SLAM十四讲》第二版的学习笔记

## 回顾

在前面的01-05讲笔记中，我们了解了SLAM问题以及经典视觉SLAM框架，如何表示相机的位姿以及用什么数学工具进行优化；还了解了相机模型，即空间中一点如何投影到相机成像平面，再如何转换为我们可以读取的像素信息；最后，我们学习了非线性最小二乘问题的求解，为之后的SLAM优化过程打下了理论基础。

从本讲开始，我们终于要面对关乎SLAM的问题。本讲介绍基于**特征点法**的前端**视觉里程计**。



## 特征点

视觉里程计的核心问题是**根据图像估计相机运动**。特征点法的思路是，选取图像中一些具有特征的点，这些点能在帧间保持不变，从而我们可以在连续的两帧中的都能观察到这些特征点。然后，我们再根据这些点的像素变化，根据几何学估计相机运动。在经典的视觉SLAM模型中，我们称之为**路标**。注意，特征点至少要满足相机运动前后保持相对的稳定。

最简单的一类特征点为角点，但是角点的特征不够稳定（比如说远处看是角点的地方近处看可能不是角点）。学者们在普通的角点基础上进行了一系列构造，提出了诸如SIFT,SURF,ORB的特征模型。这些特征模型具有以下几个性质：①在不同的图像都可以找到。②不同的特征有不同的表达。③同一图像中，特征点数量远小于像素数量。④特征仅与一小片图像区域相关。

![06-1](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-1.png)

特征点由**关键点**和**描述子**两部分组成。关键点一般指特征点在图像中的位置，描述子通常是一个向量，描述了该关键点周围像素的信息。必须说明，描述子的设计原则是**外观相似的特征应该有相似的描述子**。

在SLAM问题中，我们需要非常好的实时性，尽管有SIFT这样较为精确地特征点模型，但是计算量较大，很难达到实时性。适当降低精度和鲁棒性，SLAM中最主流的特征点模型是ORB特征。

### ORB特征

ORB特征基于FAST关键点（一种计算非常快的关键点）进行改进，增添了方向信息，又采用了速度极快的二进制描述子BRIEF，在具有实时性的同时同样具有尺度和旋转不变性。

**FAST关键点：**

FAST关键点是一类角点，其核心思想是：如果一个像素与其邻域的像素差别较大，则可能是角点。具体检测过程如下
$$
1.在图像中选择像素p,假设其亮度为I_p\\
2.设置阈值T\\
3.以像素p为中心，选择半径为3的圆上的16个像素点\\
4.假如选取的圆上有连续N个点的亮度大于I_p+T或者小于I_p-T,认为像素p是特征点
$$
![06-2](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-2.png)

除此之外，还有一些其他必要的操作。比如说N取12时可以先检测第1,5,9,13个点是否满足特征条件，以快速筛选。获取特征点后会出现特征点扎堆的情况，需要对各区域进行**非极大值抑制**来减少相邻特征点的数目。

**Oriented FAST：**

ORB特征在FAST关键点的基础上还进行了改进，添加了尺度和旋转的特征，以保证特征点在缩放或旋转后仍能被检测。

尺度不变性主要由**图像金字塔**进行实现，相信用OpenCV做过图像处理的读者对此并不陌生。我们为图像建立图像金字塔，面对不同尺度下对特征点进行匹配，我们都可以在金字塔中寻找到对应的匹配，从而实现尺度不变性。

![06-3](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-3.png)

旋转不变性在ORB特征点由**灰度质心法**实现，其步骤如下
$$
1.在一块图像块B中,定义图像矩\;m_{p,q}=\sum_{x,y\in B}x^py^qI(x,y),\;p,q=\{0,1\}\\
2.通过矩的定义计算图像的质心C=(\frac{m_{10}}{m_{00}},\frac{m_{01}}{m_{00}})\\
3.连接几何中心O与质心C,得到向量\overrightarrow{OC},定义特征方向为\theta = \arctan(\frac{m_{01}}{m_{10}})
$$
如此一来，FAST角点就有了尺度和方向的描述，鲁棒性得到了一定的提升。

**BRIEF描述子：**

BRIEF描述子是一种二进制描述子，其描述向量由若干个0与1组成。其编码方式是随机选择周围两个像素p，q，如果p的亮度高于q，编码为1，反之为0。我们重复这个过程n次，就可以编码出一个n维描述向量。原始的BRIEF不具有旋转不变性，但经过之前的FAST关键点改良后，我们可以使用方向信息。ORB特征将BRIEF改良为Steer BRIEF，使得描述子也具有旋转不变性。

### 特征匹配

不要忘记我们的目的是根据图像估计相机的运动。既然有了特征点的构造方法，下一步就是前后两帧特征点的匹配了。所谓特征匹配就是得到当前看到的路标与之前看到的路标的对应关系。据此对应关系我们才能根据几何学求解位姿的变化。下面简单介绍特征匹配。

特征匹配最简单的思路自然是**暴力匹配**，即将前后两帧的特征点做二重遍历，一一计算描述子并比较以找到对应关系，但可想而知的是较大的计算量。在SLAM问题中，我们一般使用**快速最近邻算法(FLANN)**，其速度较快，不过适用于较多匹配点的场景。具体的算法在OpenCV中已经提供，此处不展开讨论其细节，感兴趣的读者可以阅读OpenCV的源码或是数字图像处理相关书籍。



### 实践模块1-特征点匹配

因为特征点匹配的实践性很强，因此本讲将实践模块提前，下面分析特征匹配的代码。

#### 利用OpenCV进行特征匹配

先分析代码（解释都写在注释中了）

```C++
//-- 读取图像
if (argc != 3) {
    cout << "usage: feature_extraction img1 img2" << endl;
    return 1;
}
Mat img_1 = imread(argv[1], CV_LOAD_IMAGE_COLOR);
Mat img_2 = imread(argv[2], CV_LOAD_IMAGE_COLOR);
assert(img_1.data != nullptr && img_2.data != nullptr);

//-- 初始化,生成检测器，描述器和匹配器
std::vector<KeyPoint> keypoints_1, keypoints_2;
Mat descriptors_1, descriptors_2;
Ptr<FeatureDetector> detector = ORB::create();
Ptr<DescriptorExtractor> descriptor = ORB::create();
Ptr<DescriptorMatcher> matcher = DescriptorMatcher::create("BruteForce-Hamming");

//-- 第一步:检测 Oriented FAST 角点位置
chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
detector->detect(img_1, keypoints_1);
detector->detect(img_2, keypoints_2);

//-- 第二步:根据角点位置计算 BRIEF 描述子
descriptor->compute(img_1, keypoints_1, descriptors_1);
descriptor->compute(img_2, keypoints_2, descriptors_2);
chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
cout << "extract ORB cost = " << time_used.count() << " seconds. " << endl;

Mat outimg1;
drawKeypoints(img_1, keypoints_1, outimg1, Scalar::all(-1), DrawMatchesFlags::DEFAULT);
imshow("ORB features", outimg1);

//-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离
vector<DMatch> matches;
t1 = chrono::steady_clock::now();
matcher->match(descriptors_1, descriptors_2, matches);
t2 = chrono::steady_clock::now();
time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
cout << "match ORB cost = " << time_used.count() << " seconds. " << endl;

//-- 第四步:匹配点对筛选
// 计算最小距离和最大距离
auto min_max = minmax_element(matches.begin(), matches.end(),
                              [](const DMatch &m1, const DMatch &m2) { return m1.distance < m2.distance; });
double min_dist = min_max.first->distance;
double max_dist = min_max.second->distance;

printf("-- Max dist : %f \n", max_dist);
printf("-- Min dist : %f \n", min_dist);

//当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.
std::vector<DMatch> good_matches;
for (int i = 0; i < descriptors_1.rows; i++) {
    if (matches[i].distance <= max(2 * min_dist, 30.0)) {
        good_matches.push_back(matches[i]);
    }
}

//-- 第五步:绘制匹配结果
Mat img_match;
Mat img_goodmatch;
drawMatches(img_1, keypoints_1, img_2, keypoints_2, matches, img_match);
drawMatches(img_1, keypoints_1, img_2, keypoints_2, good_matches, img_goodmatch);
imshow("all matches", img_match);
imshow("good matches", img_goodmatch);
waitKey(0);
```

结果如下

![06-4](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-4.png)

![06-5](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-5.png)

可以看出，筛选之后的匹配看起来大部分是正确的。再看看用时。

![06-6](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-6.png)

可以看出，整个过程中比较耗时的是ORB特征提取，特征匹配还是相当快的。

#### 手写ORB特征匹配

根据上文提到的ORB特征点的求法，手写ORB特征匹配代码如下(解释都在注释当中了）

```C++
typedef vector<uint32_t> DescType; // 描述子类型
// 读取图片
cv::Mat first_image = cv::imread(first_file, 0);
cv::Mat second_image = cv::imread(second_file, 0);
assert(first_image.data != nullptr && second_image.data != nullptr);

// 提取FAST关键点，其中阈值T为40
chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
vector<cv::KeyPoint> keypoints1;
cv::FAST(first_image, keypoints1, 40);
vector<DescType> descriptor1;
ComputeORB(first_image, keypoints1, descriptor1);//使用自己写的ComputeORB方法，将在下方给出
vector<cv::KeyPoint> keypoints2;
vector<DescType> descriptor2;
cv::FAST(second_image, keypoints2, 40);
ComputeORB(second_image, keypoints2, descriptor2);
//获取提取用时
chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
cout << "extract ORB cost = " << time_used.count() << " seconds. " << endl;

// 计算匹配
vector<cv::DMatch> matches;
t1 = chrono::steady_clock::now();
BfMatch(descriptor1, descriptor2, matches);//使用自己写的匹配方法，将在下方给出
t2 = chrono::steady_clock::now();
time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
cout << "match ORB cost = " << time_used.count() << " seconds. " << endl;
cout << "matches: " << matches.size() << endl;

// 显示匹配结果并保存
cv::Mat image_show;
cv::drawMatches(first_image, keypoints1, second_image, keypoints2, matches, image_show);
cv::imshow("matches", image_show);
cv::imwrite("matches.png", image_show);
cv::waitKey(0);


// ORB特征计算
void ComputeORB(const cv::Mat &img, vector<cv::KeyPoint> &keypoints, vector<DescType> &descriptors) {
  const int half_patch_size = 8;
  const int half_boundary = 16;
  int bad_points = 0;
  //对每个FAST关键点进行操作
  for (auto &kp: keypoints) {
    //去除坏点
    if (kp.pt.x < half_boundary || kp.pt.y < half_boundary ||
        kp.pt.x >= img.cols - half_boundary || kp.pt.y >= img.rows - half_boundary) {
      // outside
      bad_points++;
      descriptors.push_back({});
      continue;
    }

    //灰度质心法
    //计算质心
    float m01 = 0, m10 = 0;
    for (int dx = -half_patch_size; dx < half_patch_size; ++dx) {
      for (int dy = -half_patch_size; dy < half_patch_size; ++dy) {
        uchar pixel = img.at<uchar>(kp.pt.y + dy, kp.pt.x + dx);
        m10 += dx * pixel;
        m01 += dy * pixel;
      }
    }
    // 计算方向向量
    float m_sqrt = sqrt(m01 * m01 + m10 * m10) + 1e-18; // avoid divide by zero
    float sin_theta = m01 / m_sqrt;
    float cos_theta = m10 / m_sqrt;

      
    // 描述子计算
    //生成256位的二进制描述向量
    DescType desc(8, 0);
    for (int i = 0; i < 8; i++) {
      uint32_t d = 0;
      for (int k = 0; k < 32; k++) {
        int idx_pq = i * 32 + k;
        //ORB_pattern是一个256*4的数组，猜测与描述子生成规则有关
        //随机选取两个点p,q
        cv::Point2f p(ORB_pattern[idx_pq * 4], ORB_pattern[idx_pq * 4 + 1]);
        cv::Point2f q(ORB_pattern[idx_pq * 4 + 2], ORB_pattern[idx_pq * 4 + 3]);
        // 添加旋转，使描述子具有旋转不变性
        cv::Point2f pp = cv::Point2f(cos_theta * p.x - sin_theta * p.y, sin_theta * p.x + cos_theta * p.y)
                         + kp.pt;
        cv::Point2f qq = cv::Point2f(cos_theta * q.x - sin_theta * q.y, sin_theta * q.x + cos_theta * q.y)
                         + kp.pt;
        if (img.at<uchar>(pp.y, pp.x) < img.at<uchar>(qq.y, qq.x)) {
          d |= 1 << k;
        }
      }
      desc[i] = d;
    }
    descriptors.push_back(desc);
  }
  cout << "bad/total: " << bad_points << "/" << keypoints.size() << endl;
}


// 暴力匹配算法，两重遍历筛选出最佳匹配
void BfMatch(const vector<DescType> &desc1, const vector<DescType> &desc2, vector<cv::DMatch> &matches) {
  const int d_max = 40;

  for (size_t i1 = 0; i1 < desc1.size(); ++i1) {
    if (desc1[i1].empty()) continue;
    cv::DMatch m{i1, 0, 256};
    for (size_t i2 = 0; i2 < desc2.size(); ++i2) {
      if (desc2[i2].empty()) continue;
      int distance = 0;
      for (int k = 0; k < 8; k++) {
        distance += _mm_popcnt_u32(desc1[i1][k] ^ desc2[i2][k]);
      }
      if (distance < d_max && distance < m.distance) {
        m.distance = distance;
        m.trainIdx = i2;
      }
    }
    if (m.distance < d_max) {
      matches.push_back(m);
    }
  }
}
```

先看一看匹配的图像结果，至少看起来匹配没太大问题。

![06-7](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-7.png)

再看一看用时，可以看出，手写ORB特征匹配相对OpenCV提供的ORB特征匹配速度更快一些。

![06-8](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-8.png)





## 相机运动估计

到此为止，我们已经获得了匹配好的点对（我们假设匹配都是正确的），接下来我们要根据这些点对进行相机运动的估计。根据相机原理的不同，我们可以把问题分为下面三种情况。

1.单目相机，我们需要根据**两组2D点**估计运动，该问题需要用**对极几何**解决。

2.双目或RGB-D相机，我们需要根据**两组3D点**估计运动，该问题通常采用**ICP**解决。

3.为了节省计算资源，我们还可能采用**一组3D点**和**一组2D点**估计运动，该问题采用**PnP**解决。

下面分别介绍这三种情况的解决方案。

### 2D-2D：对极几何

现在，我们要**根据若干对匹配点**，**恢复两帧之间相机运动**。

![06-9](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-9.png)
$$
现在,考虑两帧图像I_1,I_2,相机光心为O_1,O_2,从第一帧到第二帧的旋转矩阵和平移向量为R,t.\\
考虑I_1中有一个特征点p_1,在I_2中对应着特征点p_2,对于正确的匹配,它们是同一空间点P在两个平面上的投影\\
从而，连线O_1p_1与O_2p_2可以相交与点P,O_1O_2P三点构成一个平面,称为极平面\\
O_1O_2的连线与I_1,I_2平面的交点e_1,e_2记为极点,极平面与像平面的交线l_1,l_2记为极线，O_1,O_2称为基线
$$

$$
从第一帧的角度来看,射线\overrightarrow{O_1p_1}上所有的点都是P可能的位置\\
如果不知道点P的真实位置，那么I_2平面上极线l_2都是p_2可能出现的投影位置\\
不过现在我们通过特征匹配确定了p_2在极线上的位置,从而可以推知P的位置
$$

现在我们为这个略显复杂的几何关系进行数学建模。

#### 对极约束

首先设空间点P在第一帧坐标系的坐标如下
$$
P=[X,Y,Z]^T
$$
根据04-相机与图像中描述的针孔模型
$$
ZP_{uv}=Z\begin{bmatrix}u\\v\\1\end{bmatrix}=\begin{bmatrix}f_x&0&c_x\\0&f_y&c_y\\0&0&1\end{bmatrix}
\begin{bmatrix}X\\Y\\Z\end{bmatrix}=KP_c
$$
可以有
$$
s_1p_1=KP,s_2p_2=K(RP+t)\\
其中s对应上述针孔模型的Z,表示深度;K是内参矩阵,R,t是坐标系1到坐标系2旋转矩阵及平移向量
$$

对于用齐次坐标表示的像素点坐标，其乘上任何一个向量表示一个投影关系，我们称之为**尺度意义下相等**，记作
$$
sp\cong p
$$
那么，上面的式子可以有如下的推导
$$
p_1\cong KP,p_2\cong K(RP+t)\\
取归一化平面坐标x_1,x_2,根据针孔模型，x_1=K^{-1}p_1,x_2=K^{-1}p_2\\
从而有:x_2\cong Rx_1+t\\
两边同乘x_2^Tt^{\wedge},x_2^Tt^{\wedge}x_2\cong x_2^Tt^{\wedge}(Rx_1+t)\\
由于t^{\wedge}x_2与x_2,t都垂直，故该向量与x_2做内积为0,t^{\wedge}t显然也为0,所以上式可以化简为\\
x_2^TRx_1=0
$$
我们将式子重新代回到像素平面（刚才是在归一化平面）
$$
p_2^TK^{-T}t^{\wedge}RK^{-1}p_1=0
$$
这个式子称为**对极约束**，该约束中包含了平移和旋转。我们将中间的部分分别记作基础矩阵F和本质矩阵E，写作如下
$$
E=t^{\wedge}R,F=K^{-T}EK^{-1}\\
x_2^TEx_1=p_2^{T}Fp_1=0
$$
现在，我们的问题变为了

①**根据配对点的像素位置求解出本质矩阵E和基础矩阵F**

②**根据本质矩阵E和基础矩阵F解得旋转矩阵R和平移向量t**

由于本质矩阵E和基础矩阵F只需要根据内参做一定的变换，而内参往往由标定已知，所以我们主要讨论**求解本质矩阵E**。

#### 本质矩阵E的求解

首先，我们根据本质矩阵E的构造方式讨论其性质：
$$
1.对极约束是等式为零的约束，也就是说本质矩阵E乘上任一非零常数仍然满足对极约束，这称为E在不同尺度下等价\\
2.根据E=t^{\wedge}R的构造,从数学上可以证明其奇异值向量具有[\sigma,\sigma,0]^T的形式\\
3.由于平移和旋转各有3自由度，而不同尺度下等价丢失一个自由度，从而E实际上有5个自由度
$$
E具有五个自由度，表示我们可以用至少五对点来求解E，不过本质矩阵内在的非线性性质会使真正只用五对点进行求解时十分麻烦，我们发现本质矩阵是一个3×3的矩阵，如果我们仅仅丢失不同尺度下等价这一自由度，而不考虑其表达平移和旋转的冗余自由度，那我们应该用八对点来估计E，这就是经典的**八点法**。

先考虑一对匹配点的情况
$$
设归一化平面上x_1=[u_1,v_1,1]^T,x_2=[u_2,v_2,1]^T,根据对极约束，\\
\begin{bmatrix}u_2&v_2&1\end{bmatrix}
\begin{bmatrix}e_1&e_2&e_3\\e_4&e_5&e_6\\e_7&e_8&e_9\end{bmatrix}
\begin{bmatrix}u_1\\v_1\\1\end{bmatrix}=0\\
若把E展开为向量的形式:
\overrightarrow{e}=\begin{bmatrix}e_1&e_2&e_3&e_4&e_5&e_6&e_7&e_8&e_9\end{bmatrix}^T\\
等价地书写对极约束:
\begin{bmatrix}u_2u_1&u_2v_1&u_2&v_2u_1&v_2v_1&v_2&u_1&v_1&1\end{bmatrix}\cdot\overrightarrow{e}=0
$$
我们用八对点构成的8个这样的方程组合成一个方程组
$$
记\overrightarrow{f_{uv}^i}=
\begin{bmatrix}u_2u_1&u_2v_1&u_2&v_2u_1&v_2v_1&v_2&u_1&v_1&1\end{bmatrix}^i
为第i对匹配特征点构成的向量，则有\\
\begin{bmatrix}f_{uv}^1\\f_{uv}^2\\...\\f_{uv}^8\end{bmatrix}\cdot\overrightarrow{e}=0
$$
这8个方程构成一个线性方程组，如果系数矩阵满秩，那么本质矩阵E中的各元素就可以由这个方程组解出来。

接下来就是**根据刚才解出的本质矩阵E恢复出相机运动的R,t**。这个过程由**奇异值分解**（SVD分解）解决。为了防止读者不了解奇异值分解而完全不理解下列过程，我有必要简要介绍一下奇异值分解。

**相似对角化与奇异值分解**

我们在线性代数课中应该学过相似对角化的概念。如果A是方阵并且由n个线性无关的特征向量，则有
$$
A=W\Sigma W^{-1}\\
其中，\Sigma 为与A相似的对角矩阵，对角元素为特征值，W为与特征值对应的特征向量张成的矩阵
$$
由于W是特征向量张成的矩阵，故而各列向量线性无关，将其进行正交化使其称为一个正交矩阵，便有
$$
W^T=W^{-1}\\
\rightarrow A=W\Sigma W^T
$$
如果矩阵A并不是方阵，而是一个m*n的矩阵，这时就需要用到SVD分解
$$
A=U\Sigma V^T\\
其中，U是m维方阵，\Sigma是m\times n的矩阵，V是n维方阵
$$
并且U,V都是正交矩阵，即
$$
UU^T=VV^T=I
$$
分解得到的三个矩阵分别由下列方式求得。首先有

$$
A^TA\in R^{n\times n}\quad AA^T\in R^{m\times m}
$$
对这两个矩阵分别进行相似对角化，可以分别得到特征向量张成的矩阵正交化后的矩阵U，V。

再根据下式
$$
AV=U\Sigma
$$
即可求得奇异值矩阵。

**求解**

简要介绍了SVD分解，我们回到本质矩阵E的分解问题。我们首先设
$$
E=U\Sigma V^T\\
U,V为正交矩阵,\Sigma为奇异值矩阵\\
前面提到奇异值矩阵的形式为diag(\sigma,\sigma,0),由于E不同尺度下等价，直接令其为diag(1,1,0)
$$
对于任意一个E，可能会有两个t，R与之对应
$$
t_1^{\wedge}=UR_Z(\frac{\pi}{2})\Sigma U^T,R_1=UR_Z^T(\frac{\pi}{2})V^T\\
t_2^{\wedge}=UR_Z(-\frac{\pi}{2})\Sigma U^T,R_2=UR_Z^T(-\frac{\pi}{2})V^T\\
其中R_Z(\frac{\pi}{2})表示沿Z轴旋转\frac{\pi}{2}得到的旋转矩阵
$$
同时，由于-E和E等价，所以对任意一个t取负号得到的本质矩阵也是一致的。从而从E分解到t，R时，会有4个不同的解。

![06-10](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-10.png)

从图示可以看出，对于相机（蓝色线）平面上同样的投影点（红色点），不过我们可以看出只有第一种解对两个相机都具有正深度。实际估计时，我们可以计算出四个解，将其代回
$$
s_1p_1=KP,s_2p_2=K(RP+t)\\
$$
计算两个深度是否为正即可确定哪个解是正确的解。

#### 单应矩阵

除了上面提到的本质矩阵和基础矩阵，二视图几何还有一种常见的矩阵：单应矩阵H。它描述了两个平面之间的映射关系，若场景中的特征点都落在同一平面上，则可以通过单应性进行运动估计。还是先进行数学建模。
$$
设图像I_1,I_2上有一堆匹配好的特征点p_1,p_2,对应的实际空间点P\\
平面方程为n^TP+d=0,即-\frac{n^TP}{d}=1\\
$$
回顾我们引出对极约束时引入的尺度意义下相等概念，有
$$
p_2\cong K(RP+t)\cong K(RP+t(-\frac{n^TP}{d}))\cong K(R-\frac{tn^T}{d})P\\
又因为p_1\cong KP,所以p_2\cong K(R-\frac{tn^T}{d})K^{-1}p_1\\
记H=K(R-\frac{tn^T}{d})K^{-1},则p_2\cong Hp_1
$$
我们记H为**单应矩阵**。

我们依照求解本质矩阵E的思路求解单应矩阵H，首先先考虑一对匹配点的情况。
$$
设归一化平面上x_1=[u_1,v_1,1]^T,x_2=[u_2,v_2,1]^T\\
\begin{bmatrix}u_2\\v_2\\1\end{bmatrix}\cong
\begin{bmatrix}h_1&h_2&h_3\\h_4&h_5&h_6\\h_7&h_8&h_9\end{bmatrix}
\begin{bmatrix}u_1\\v_1\\1\end{bmatrix}\\
注意,该式子是在尺度意义下相等的，也就是说H乘任意非零常数该式子仍然成立，当h_9非零时，不妨令h_9=1\\
基于尺度意义下相等式
\begin{bmatrix}u_2\\v_2\\1\end{bmatrix}\cong
\begin{bmatrix}h_1&h_2&h_3\\h_4&h_5&h_6\\h_7&h_8&h_9\end{bmatrix}
\begin{bmatrix}u_1\\v_1\\1\end{bmatrix}=
\begin{bmatrix}h_1u_1+h_2v_1+h_3\\h_4u_1+h_5v_1+h_6\\h_7u_1+h_8v_1+h_9\end{bmatrix}
以及h_9=1,可以得到如下两步推导\\
尺度意义下相等:u_2=\frac{h_1u_1+h_2v_1+h_3}{h_7u_1+h_8v_1+h_9},v_2=\frac{h_4u_1+h_5v_1+h_6}{h_7u_1+h_8v_1+h_9}
\\消去h_9:
\begin{cases}
h_1u_1+h_2v_1+h_3-h_7u_1u_2-h_8v_1u_2=u_2\\
h_4u_1+h_5v_1+h_6-h_7u_1v_2-h_8v_1v_2=v_2
\end{cases}
\\
$$
如此一来，一组匹配点就可以构造两组非线性相关的约束，从而自由度为8的单应矩阵只需要四对匹配点就可以求解（注意，这四对匹配点都在同一平面上，且前提是这几对匹配点没有共线等退化情况）。我们将H也序列化成向量，并且去除一个自由度
$$
H=[h_1,h_2,...h_8]^T
$$
由刚才推导的式子，我们可以由四对匹配点写成一个线性方程组如下
$$
记u^i,v^i为第i对匹配特征点,则有\\
\begin{bmatrix}
u_1^1&v_1^1&1&0&0&0&-u_1^1u_2^1&-v_1^1u_2^1\\
0&0&0&u_1^1&v_1^1&1&-u_1^1v_2^1&-v_1^1v_2^1\\
u_1^2&v_1^2&1&0&0&0&-u_1^2u_2^2&-v_1^2u_2^2\\
0&0&0&u_1^2&v_1^2&1&-u_1^2v_2^2&-v_1^2v_2^2\\
u_1^3&v_1^3&1&0&0&0&-u_1^3u_2^3&-v_1^3u_2^3\\
0&0&0&u_1^3&v_1^3&1&-u_1^3v_2^3&-v_1^3v_2^3\\
u_1^4&v_1^4&1&0&0&0&-u_1^4u_2^4&-v_1^4u_2^4\\
0&0&0&u_1^4&v_1^4&1&-u_1^4v_2^4&-v_1^4v_2^4
\end{bmatrix}
\begin{bmatrix}
h_1\\h_2\\h_3\\h_4\\h_5\\h_6\\h_7\\h_8
\end{bmatrix}=
\begin{bmatrix}
u_2^1\\v_2^1\\u_2^2\\v_2^2\\u_2^3\\v_2^3\\u_2^4\\v_2^4\\
\end{bmatrix}
$$
由此就可以求解单应矩阵H的元素。与本质矩阵E类似，它也需要进行分解来获得R，t，并且也会返回四组解，此时需要确定平面的法向量方向和空间点深度才能得到正确的一组R，t。

在SLAM中，单应性具有重要意义，因为当出现特征点共面或者相机发生纯旋转（而无平移）时，本质矩阵的自由度下降，即出现了退化情况。此时退化的自由度反而主要由噪声决定，造成影响。为了SLAM过程中的鲁棒性，我们选择同时估计本质矩阵E和单应矩阵H，选择**重投影误差较小**的一个作为最终的**运动估计矩阵**。所谓重投影误差，指的是将最终估计空间点P重新通过针孔模型投影到两视图上，再根据两个投影点与真实点像素误差的范数的平方之和作为重投影误差。

#### 实践模块2-对极几何运动估计

按照惯例，我们还是先分析代码，首先先分析自己定义的三个函数

```C++
//特征匹配的代码，与实践模块1-特征点匹配的代码一致
//过程为 寻找关键点-计算描述子-汉明距离匹配
void find_feature_matches(const Mat &img_1, const Mat &img_2,
                          std::vector<KeyPoint> &keypoints_1,
                          std::vector<KeyPoint> &keypoints_2,
                          std::vector<DMatch> &matches) {
  //-- 初始化
  Mat descriptors_1, descriptors_2;
  // used in OpenCV3
  Ptr<FeatureDetector> detector = ORB::create();
  Ptr<DescriptorExtractor> descriptor = ORB::create();
  // use this if you are in OpenCV2
  // Ptr<FeatureDetector> detector = FeatureDetector::create ( "ORB" );
  // Ptr<DescriptorExtractor> descriptor = DescriptorExtractor::create ( "ORB" );
  Ptr<DescriptorMatcher> matcher = DescriptorMatcher::create("BruteForce-Hamming");
  //-- 第一步:检测 Oriented FAST 角点位置
  detector->detect(img_1, keypoints_1);
  detector->detect(img_2, keypoints_2);

  //-- 第二步:根据角点位置计算 BRIEF 描述子
  descriptor->compute(img_1, keypoints_1, descriptors_1);
  descriptor->compute(img_2, keypoints_2, descriptors_2);

  //-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离
  vector<DMatch> match;
  //BFMatcher matcher ( NORM_HAMMING );
  matcher->match(descriptors_1, descriptors_2, match);

  //-- 第四步:匹配点对筛选
  double min_dist = 10000, max_dist = 0;

  //找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离
  for (int i = 0; i < descriptors_1.rows; i++) {
    double dist = match[i].distance;
    if (dist < min_dist) min_dist = dist;
    if (dist > max_dist) max_dist = dist;
  }

  printf("-- Max dist : %f \n", max_dist);
  printf("-- Min dist : %f \n", min_dist);

  //当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.
  for (int i = 0; i < descriptors_1.rows; i++) {
    if (match[i].distance <= max(2 * min_dist, 30.0)) {
      matches.push_back(match[i]);
    }
  }
}

//实现像素坐标转换到归一化平面的坐标
//两者仅仅差内参矩阵K,直接根据线性方程组关系求解
Point2d pixel2cam(const Point2d &p, const Mat &K) {
  return Point2d
    (
      (p.x - K.at<double>(0, 2)) / K.at<double>(0, 0),
      (p.y - K.at<double>(1, 2)) / K.at<double>(1, 1)
    );
}

//对极几何估计
void pose_estimation_2d2d(std::vector<KeyPoint> keypoints_1,
                          std::vector<KeyPoint> keypoints_2,
                          std::vector<DMatch> matches,
                          Mat &R, Mat &t) {
  // 相机内参矩阵,TUM Freiburg2
  Mat K = (Mat_<double>(3, 3) << 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);

  //-- 把匹配点转换为vector<Point2f>的形式
  vector<Point2f> points1;
  vector<Point2f> points2;

  for (int i = 0; i < (int) matches.size(); i++) {
    points1.push_back(keypoints_1[matches[i].queryIdx].pt);
    points2.push_back(keypoints_2[matches[i].trainIdx].pt);
  }

  //下面的计算要利用OpenCV中的函数
  //-- 计算基础矩阵F
  Mat fundamental_matrix;
  fundamental_matrix = findFundamentalMat(points1, points2, CV_FM_8POINT);
  cout << "fundamental_matrix is " << endl << fundamental_matrix << endl;

  //-- 计算本质矩阵E
  Point2d principal_point(325.1, 249.7);  //相机光心, TUM dataset标定值
  double focal_length = 521;      //相机焦距, TUM dataset标定值
  Mat essential_matrix;
  essential_matrix = findEssentialMat(points1, points2, focal_length, principal_point);
  cout << "essential_matrix is " << endl << essential_matrix << endl;

  //-- 计算单应矩阵
  //-- 但是本例中场景不是平面，单应矩阵意义不大
  Mat homography_matrix;
  homography_matrix = findHomography(points1, points2, RANSAC, 3);
  cout << "homography_matrix is " << endl << homography_matrix << endl;

  //-- 从本质矩阵中恢复旋转和平移信息.(这里没有使用单应矩阵)
  // 此函数仅在Opencv3中提供
  recoverPose(essential_matrix, points1, points2, R, t, focal_length, principal_point);
  cout << "R is " << endl << R << endl;
  cout << "t is " << endl << t << endl;

}
```

再分析主程序

```C++
//-- 读取图像
Mat img_1 = imread(argv[1], CV_LOAD_IMAGE_COLOR);
Mat img_2 = imread(argv[2], CV_LOAD_IMAGE_COLOR);
assert(img_1.data && img_2.data && "Can not load images!");

//完成特征匹配并保存
vector<KeyPoint> keypoints_1, keypoints_2;
vector<DMatch> matches;
find_feature_matches(img_1, img_2, keypoints_1, keypoints_2, matches);
cout << "一共找到了" << matches.size() << "组匹配点" << endl;

//-- 估计两张图像间运动
Mat R, t;
pose_estimation_2d2d(keypoints_1, keypoints_2, matches, R, t);

//-- 验证E=t^R*scale
Mat t_x =
    (Mat_<double>(3, 3) << 0, -t.at<double>(2, 0), t.at<double>(1, 0),
     t.at<double>(2, 0), 0, -t.at<double>(0, 0),
     -t.at<double>(1, 0), t.at<double>(0, 0), 0);

cout << "t^R=" << endl << t_x * R << endl;

//-- 验证对极约束
Mat K = (Mat_<double>(3, 3) << 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);
for (DMatch m: matches) {
    Point2d pt1 = pixel2cam(keypoints_1[m.queryIdx].pt, K);
    Mat y1 = (Mat_<double>(3, 1) << pt1.x, pt1.y, 1);
    Point2d pt2 = pixel2cam(keypoints_2[m.trainIdx].pt, K);
    Mat y2 = (Mat_<double>(3, 1) << pt2.x, pt2.y, 1);
    //对极约束式，满足约束应为0(含噪声时接近0)
    Mat d = y2.t() * t_x * R * y1;
    cout << "epipolar constraint = " << d << endl;
}
```

我们可以看到整个过程和我们理论推导的过程是一致的。我们首先对两张图像进行特征匹配，再依据特征匹配的结果根据八点法求解本质矩阵和基础矩阵（OpenCV函数需要我们提供相机内参）。再由OpenCV的函数由本质矩阵得到旋转矩阵R和平移向量t，并且很好的是函数已经帮我们剔除了深度为负的三组解，得到了正确的那一组解。下面来看看程序的运行结果。

![06-11](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-11.png)

可以看到验证由R，t倒推本质矩阵E的结果与本质矩阵E在尺度意义下相等。再看看各组匹配点的对极约束

![06-12](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-12.png)

显然，各组匹配点的对极约束都十分接近零，说明算法基本是没有什么问题的（因为含噪声的缘故而导致不为零）。

这里还需要说明一些问题：由于本质矩阵E具有尺度等价性，由其分解获得的R，t也具有尺度等价性。由于R作为变换矩阵的自身约束，我们认为平移向量t具有一个尺度，而在分解过程中，t乘上任意一个非零常数分解都是成立的，因此，我们通常归一化t，令其为1。这个时候我们就发现：**单目视觉具有尺度不确定性**。而我们对两张图像进行t归一化处理相当于固定了一个尺度，尽管我们并不知道这个尺度具体代表的真实物理值为多少，但我们以此为一个长度单位，就完成了单目SLAM**不可避免**的**初始化**过程。除此之外，单目SLAM还有一个**初始化纯旋转问题**，如果初始化相机发生的运动是纯旋转，那么t为0时本质矩阵E也为0，我们无法通过本质矩阵求旋转。 我们可以通过单应矩阵求解旋转，但纯旋转时又无法通过三角测量估计特征点的空间位置。总而言之，**单目初始化必须有一定的平移**。

还有一个问题：我们理论介绍的是八点法，但实践中匹配的特征点远不止8对，根据线性代数的知识，我们知道这将把之前的八点法构造的线性方程组变为一个**超定方程组**，而超定方程组未必有解。这时，我们可以采取最小化下面的二次型
$$
\min_e ||Ae||_2^2=\min_e e^TA^TAe
$$
来求解最小二乘意义下的本质矩阵E，不过考虑到误匹配的情况，工程上往往采用**随机采样一致性(RANSAC)方法**来求解。由于这部分在现有的OpenCV函数库中已经实现，故不再展开讨论，感兴趣的读者可以搜索相关资料了解该方法。



### 三角测量

#### 原理

在对极几何部分中，我们了解了如何通过两帧图像估计相机的运动。SLAM问题中，我们还需要得到特征点的真实空间位置，由于单目相机无法直接获取深度，我们还需要通过相机的运动估计特征点的深度。解决这个问题我们通过**三角测量**来完成。

![06-13](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-13.png)

这个场景与对极约束一讲最开始的图示是一致的，现在，我们重新将尺度代回，而不是使用尺度意义下等价
$$
s_1p_1=KP,s_2p_2=K(RP+t),x_1=K^{-1}p_1,x_2=K^{-1}p_2\\
s_2x_2=s_1Rx_1+t
$$
现在，我们已知匹配特征点的归一化平面坐标以及运动估计R，t，要求解特征点在两视图下的深度s。
$$
由图我们可以有很简单的思路:\\
在射线O_1p_1上寻找空间点P,使得P在I_2像素平面投影最接近p_2;亦或是在射线O_2p_2上寻找,使投影最接近p_1
$$
根据这个思路，我们假设先求解图像1中特征点的深度
$$
同时左乘x_2^{\wedge}:s_2x_2^{\wedge}x_2=0=s_1x_2^{\wedge}Rx_1+x_2^{\wedge}t
$$
不过由于噪声存在，我们估计的R，t未必使上式严格成立，所以我们一般是采取最小二乘来求解，即
$$
\min_{s_1}s_1x_2^{\wedge}Rx_1+x_2^{\wedge}t
$$
有了图像1中特征点的深度，图像2中特征点的深度可以直接根据前面的关系式得到。

#### 实践模块3-三角测量

```C++
//实践模块2-对极几何运动估计中定义的三个函数
//用于特征点匹配
void find_feature_matches(
  const Mat &img_1, const Mat &img_2,
  std::vector<KeyPoint> &keypoints_1,
  std::vector<KeyPoint> &keypoints_2,
  std::vector<DMatch> &matches);
//用于根据匹配信息计算本质矩阵，基础矩阵，单应矩阵，并计算运动估计
void pose_estimation_2d2d(
  const std::vector<KeyPoint> &keypoints_1,
  const std::vector<KeyPoint> &keypoints_2,
  const std::vector<DMatch> &matches,
  Mat &R, Mat &t);
// 像素坐标转相机归一化坐标
Point2f pixel2cam(const Point2d &p, const Mat &K);

//三角测量函数（后面具体分析）
void triangulation(
  const vector<KeyPoint> &keypoint_1,
  const vector<KeyPoint> &keypoint_2,
  const std::vector<DMatch> &matches,
  const Mat &R, const Mat &t,
  vector<Point3d> &points) {
  //第一帧图像的变换矩阵
  Mat T1 = (Mat_<float>(3, 4) <<
    1, 0, 0, 0,
    0, 1, 0, 0,
    0, 0, 1, 0);
  //第二帧图像的变换矩阵
  Mat T2 = (Mat_<float>(3, 4) <<
    R.at<double>(0, 0), R.at<double>(0, 1), R.at<double>(0, 2), t.at<double>(0, 0),
    R.at<double>(1, 0), R.at<double>(1, 1), R.at<double>(1, 2), t.at<double>(1, 0),
    R.at<double>(2, 0), R.at<double>(2, 1), R.at<double>(2, 2), t.at<double>(2, 0)
  );
  //相机的内参矩阵
  Mat K = (Mat_<double>(3, 3) << 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);
  
  //生成归一化平面上的匹配点对
  vector<Point2f> pts_1, pts_2;
  for (DMatch m:matches) {
    pts_1.push_back(pixel2cam(keypoint_1[m.queryIdx].pt, K));
    pts_2.push_back(pixel2cam(keypoint_2[m.trainIdx].pt, K));
  }

  //调用OpenCV的三角测量函数
  Mat pts_4d;
  cv::triangulatePoints(T1, T2, pts_1, pts_2, pts_4d);

  // 将齐次坐标结果转换成非齐次坐标
  for (int i = 0; i < pts_4d.cols; i++) {
    Mat x = pts_4d.col(i);
    x /= x.at<float>(3, 0); // 归一化
    Point3d p(
      x.at<float>(0, 0),
      x.at<float>(1, 0),
      x.at<float>(2, 0)
    );
    points.push_back(p);
  }
}

/// 内联函数，作深度图时用
inline cv::Scalar get_color(float depth) {
  float up_th = 50, low_th = 10, th_range = up_th - low_th;
  if (depth > up_th) depth = up_th;
  if (depth < low_th) depth = low_th;
  return cv::Scalar(255 * depth / th_range, 0, 255 * (1 - depth / th_range));
}



//-- 读取图像
Mat img_1 = imread(argv[1], CV_LOAD_IMAGE_COLOR);
Mat img_2 = imread(argv[2], CV_LOAD_IMAGE_COLOR);

vector<KeyPoint> keypoints_1, keypoints_2;
vector<DMatch> matches;
find_feature_matches(img_1, img_2, keypoints_1, keypoints_2, matches);
cout << "一共找到了" << matches.size() << "组匹配点" << endl;

//-- 估计两张图像间运动
Mat R, t;
pose_estimation_2d2d(keypoints_1, keypoints_2, matches, R, t);

//-- 三角化
vector<Point3d> points;
triangulation(keypoints_1, keypoints_2, matches, R, t, points);

//显示特征点,并打印特征点深度，不同深度以不同的颜色显示
Mat img1_plot = img_1.clone();
Mat img2_plot = img_2.clone();
for (int i = 0; i < matches.size(); i++) {
    // 显示第一个图中特征点的深度
    float depth1 = points[i].z;
    cout << "depth: " << depth1 << endl;
    cv::circle(img1_plot, keypoints_1[matches[i].queryIdx].pt, 2, get_color(depth1), 2);

    // 计算并显示第二个图中特征点的深度
    Mat pt2_trans = R * (Mat_<double>(3, 1) << points[i].x, points[i].y, points[i].z) + t;
    float depth2 = pt2_trans.at<double>(2, 0);
    cv::circle(img2_plot, keypoints_2[matches[i].trainIdx].pt, 2, get_color(depth2), 2);
}
cv::imshow("img 1", img1_plot);
cv::imshow("img 2", img2_plot);
cv::waitKey();
```

经过三角测量，特征点的深度被计算了出来，不过这里的深度仍然是**基于初始化尺度而非真实尺度**的。三角测量是立足于平移的，如果没有平移，连三角测量中的"三角"都不存在，所以，纯旋转会给三角测量带来很大的麻烦。三角测量还有一个矛盾，如下图所示

![06-14](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-14.png)

当移动的t较小时，特征点的不确定性较大，而移动的t较大时，特征点由于视场移动的幅度过大又有可能使匹配点过少。

相信各位读者读到这里都会有这种感觉：**单目相机完成SLAM太费劲了**，因为不能提供原始的深度信息，我们翻山越岭最终也没能得到真实尺度下的里程计和观测，只能构建相同比例的模型（除非初始化时我们给定的标准尺度对应到真实尺度下的标准长度）。事实上，我们确实很少会仅仅使用单目去完成SLAM，我们可以将其与imu融合，亦或是使用RGB-D相机和双目相机。



### 3D-2D:PnP

PnP是求解3D到2D点对运动的方法。它描述了当知道n个3D空间点的真实位置（无论在第一帧相机坐标系还是世界坐标系）及其投影位置时，如何估计相机的位姿。在已知特征点在一张图像中的深度时，最少只需要三对点（还需要额外的一对点验证结果）就可以完成相机运动的估计。PnP问题有很多种求解方法，比如用三对点估计相机运动的P3P，直接线性变换(DLT)，EPnP。除此之外，还可以采用**非线性优化**的方式，构建最小二乘问题求解，在这个模型下构建的最小二乘问题称为**光束法平差**（Bundle Adjustment,BA）。

我个人最推崇光束法平差这一方案，所以接下来将简要介绍直接线性变换(DLT)，P3P,着重介绍光束法平差。

#### 直接线性变换

话不多说，直接开始对3D-2D点对进行数学建模
$$
考虑某个空间点P,其齐次坐标为P=[X,Y,Z,1]^T,注意,这个坐标是在图像I_1坐标系下的\\
现在我们直接考虑图像该空间点投影到I_2的归一化平面上,投影点为x_2=[u_2,v_2,1]^T\\
则有s\begin{bmatrix}u_2\\v_2\\1\end{bmatrix}=
\begin{bmatrix}
t_1&t_2&t_3&t_4\\t_5&t_6&t_7&t_8\\t_9&t_{10}&t_{11}&t_{12}
\end{bmatrix}
\begin{bmatrix}X\\Y\\Z\\1\end{bmatrix}
$$
其中，矩阵t包含了旋转和平移信息。通过像素齐次坐标的最后一行约去s，可以得到两个有关系数矩阵t的约束，t一共有12个自变量，最少需要6对没有退化情况的点就可以求解出矩阵t。再据此对旋转矩阵和平移向量构造即可。

#### P3P

对于如下的三对3D-2D点，A,B,C在世界坐标系下已知真实坐标（可以根据上一帧的位姿和上一帧的针孔模型获得），a,b,c是在新的一帧的投影坐标。本质上，我们只要计算出投影点在相机坐标系下的真实坐标，再根据两个坐标系的变换获得其变换矩阵即可。

![06-15](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-15.png)
$$
首先有相似关系,\triangle Oab\sim OAB,\triangle Obc\sim OBC,\triangle Oac\sim OAC\\
根据余弦定理,
\begin{cases}
OA^2+OB^2-2\cdot OA\cdot OB\cdot \cos(a,b)=AB^2\\
OB^2+OC^2-2\cdot OB\cdot OC\cdot \cos(b,c)=BC^2\\
OA^2+OC^2-2\cdot OA\cdot OC\cdot \cos(a,c)=AC^2\\
\end{cases}\\
我们将所有式子同除以OC^2,并记x=\frac{OA}{OC},y=\frac{OB}{OC},
v=\frac{AB^2}{OC^2},uv=\frac{BC^2}{OC^2},wv=\frac{AC^2}{OC^2}\\
有
\begin{cases}
x^2+y^2-2xy\cos(a,b)-v=0\\
y^2+1-2y\cos(b,c)-uv=0\\
x^2+1-2x\cos(a,c)-wv=0
\end{cases},
在同时消掉v,得到如下的二元二次方程\\
\begin{cases}
(1-u)y^2-ux^2-cos(b,c)y+2uxy\cos(a,b)+1=0\\
(1-w)y^2-wy^2-cos(a,c)x+2wxy\cos(a,b)+1=0
\end{cases}
$$
对最后的二元二次方程进行求解即可。求解后可以获得投影点在相机坐标系下的3D点，在此基础上进行3D-3D点对的匹配即可。求解过程不在此赘述，其解也有四种情况，需要按实际模型取舍。

对于SLAM实际情况下，通常先通过PnP方法估计位姿，再构建最小二乘问题对估计值进行调整。

#### 光束法平差

光束法平差是基于非线性优化的，不同于上述先估计相机位姿，再估计空间点位置的过程，光束法平差将相机位姿和空间点同时作为优化变量进行优化。这是一种非常通用的求解方式。我们还可以用它对PnP或是ICP给出的结果进行优化。

考虑n个三维空间点P（可以是前一帧的相机坐标系也可以是世界坐标系下的）及其投影p，我们要计算相机的位姿R，t，它的李群为T。
$$
设某空间点坐标P_i=[X_i,Y_i,Z_i]^T,投影的像素坐标p_i=[u_i,v_i]^T\\
根据针孔模型,做一次齐次坐标变换,有s_i\begin{bmatrix}u_i\\v_i\\1\end{bmatrix}=KT\begin{bmatrix}X_i\\Y_i\\Z_i\\1\end{bmatrix},
即s_ip_i=KTP_i\\
但实际上上式是有误差的,我们记误差函数e_i=p_i-\frac{1}{s_i}KTP_i\\
并对n个空间点的误差范数的平方求和作为代价函数,可以构建下面的最小二乘问题\\
T^*=arg\min_T\frac{1}{2}\sum_{i=1}^n||p_i-\frac{1}{s_i}KTP_i||
$$
该最小二乘问题实际上是找到一个位姿，使得空间点集通过针孔模型投影到像素平面上的点集与实际在这一帧观测到的点集之间的误差和最小。误差项即是投影位置和观测位置作差，称为**重投影误差**。其示意图如下

![06-16](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-16.png)

优化中我们自然使用李代数而不是李群来进行优化，因为李代数没有李群那样的约束。根据05-非线性优化一讲的内容，我们还需要求出代价函数的一阶导数，即雅可比矩阵，才可以顺利的使用编程工具完成优化过程。首先
$$
e(x+\Delta x)\approx e(x)+J^T\Delta x
$$
当误差维数为像素坐标误差（2维），x为se(3)李代数表示的位姿（6维）时，雅可比矩阵的转置是一个2×6的矩阵。
$$
先记相机坐标系下空间点的坐标P_c=(TP)_{1:3}=[X_c,Y_c,Z_c]^T,这里包含一步齐次坐标到非齐次坐标的转换\\
根据针孔模型:s\begin{bmatrix}u\\v\\1\end{bmatrix}=
\begin{bmatrix}f_x&0&c_x\\0&f_y&c_y\\0&0&1\end{bmatrix}
\begin{bmatrix}X_c\\Y_c\\Z_c\end{bmatrix}\\
消去s:u=f_x\frac{X_c}{Z_c}+c_x,v=f_y\frac{Y_c}{Z_c}+c_y
$$
求投影误差时，我们可以用这里的u,v与观测值作差。现在我们对T进行左乘扰动，求e对扰动的量导数。根据链式法则
$$
\frac{\partial e}{\partial \delta\xi}=
\frac{\partial e}{\partial P_c}\frac{\partial P_c}{\partial \delta\xi}
$$

根据刚才推得的u,v，以及误差项表达式，我们可以得到
$$
\frac{\partial e}{\partial P_c}=
-\begin{bmatrix}
\frac{\partial u}{\partial X_c}&\frac{\partial u}{\partial Y_c}&\frac{\partial u}{\partial Z_c}\\
\frac{\partial v}{\partial X_c}&\frac{\partial v}{\partial Y_c}&\frac{\partial v}{\partial Z_c}
\end{bmatrix}=
-\begin{bmatrix}
\frac{f_x}{Z_c}&0&-\frac{f_xX_c}{Z_c^2}\\
0&\frac{f_y}{Z_c}&-\frac{f_yY_c}{Z_c^2}
\end{bmatrix}
$$
第二项是变换后的点关于李代数的导数，根据左乘扰动模型的结果可以得到
$$
\frac{\partial (TP)}{\partial \delta\xi}=
\begin{bmatrix}I&-(RP+t)^{\wedge}\\0^T&0^T\end{bmatrix}\\
$$
由于我们只取了前三维的非齐次坐标，所以有
$$
\frac{\partial P_c}{\partial \delta\xi}=
\begin{bmatrix}I&-P_c^{\wedge}\end{bmatrix}=
\begin{bmatrix}1&0&0&0&-Z_c&Y_c\\0&1&0&Z_c&0&-X_c\\0&0&1&-Y_c&X_c&0\end{bmatrix}
$$
将得到的这两部分相乘，可以得到
$$
\frac{\partial e}{\partial \delta\xi}=-
\begin{bmatrix}
\frac{f_x}{Z_c}&0&-\frac{f_xX_c}{Z_c^2}&
-\frac{f_xX_cY_c}{Z_c^2}&f_x+\frac{f_xX_c^2}{Z_c^2}&-\frac{f_xY_c}{Z_c}\\
0&\frac{f_y}{Z_c}&-\frac{f_yY_c}{Z_c^2}&
-f_y-\frac{f_yY_c^2}{Z_c^2}&\frac{f_yX_cY_c}{Z_c^2}&-\frac{f_yX_c}{Z_c}
\end{bmatrix}
$$
如果我们构建的最小二乘问题是优化空间点位置，即
$$
P^*=arg\min_P\frac{1}{2}\sum_{i=1}^n||p_i-\frac{1}{s_i}KTP_i||
$$
那么需要的一阶导数（雅可比矩阵）根据链式法则可以写作
$$
\frac{\partial e}{\partial P}=
\frac{\partial e}{\partial P_c}\frac{\partial P_c}{\partial P}
$$
其中第一项我们已经求解了，只需要求解第二项
$$
P_c=RP+t\rightarrow\frac{\partial P_c}{\partial P}=R
$$
从而有
$$
\frac{\partial e}{\partial P}=-\begin{bmatrix}
\frac{f_x}{Z_c}&0&-\frac{f_xX_c}{Z_c^2}\\
0&\frac{f_y}{Z_c}&-\frac{f_yY_c}{Z_c^2}
\end{bmatrix}R
$$

#### 实践模块4-求解PnP

直接分析代码（解释都写在注释中了）

```C++
//-- 读取图像并匹配特征点
Mat img_1 = imread(argv[1], CV_LOAD_IMAGE_COLOR);
Mat img_2 = imread(argv[2], CV_LOAD_IMAGE_COLOR);
assert(img_1.data && img_2.data && "Can not load images!");

vector<KeyPoint> keypoints_1, keypoints_2;
vector<DMatch> matches;
//调用之前的方法，直接获取匹配
find_feature_matches(img_1, img_2, keypoints_1, keypoints_2, matches);
cout << "一共找到了" << matches.size() << "组匹配点" << endl;

// 计算3D点,并生成3D-2D点对
Mat d1 = imread(argv[3], CV_LOAD_IMAGE_UNCHANGED);       // 读取深度图,度图为16位无符号数，单通道图像
Mat K = (Mat_<double>(3, 3) << 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);//内参矩阵
vector<Point3f> pts_3d;
vector<Point2f> pts_2d;
for (DMatch m:matches) {
    //找到匹配点处的深度,注意,深度图与前一帧是同一视场
    ushort d = d1.ptr<unsigned short>(int(keypoints_1[m.queryIdx].pt.y))[int(keypoints_1[m.queryIdx].pt.x)];
    if (d == 0)   // bad depth
    	continue;
    float dd = d / 5000.0;//根据深度图规则，计算出真实深度
    Point2d p1 = pixel2cam(keypoints_1[m.queryIdx].pt, K);//将特征点转换到归一化平面上
    pts_3d.push_back(Point3f(p1.x * dd, p1.y * dd, dd));//由归一化坐标乘上深度，得到第一帧相机坐标系真实坐标
    pts_2d.push_back(keypoints_2[m.trainIdx].pt);//后一帧的像素平面坐标
}

//利用OpenCV函数解PnP问题,得到位姿估计
chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
Mat r, t;
solvePnP(pts_3d, pts_2d, K, Mat(), r, t, false); // 调用OpenCV 的 PnP 求解，可选择EPNP，DLS等方法
Mat R;
cv::Rodrigues(r, R); // r为旋转向量形式，用Rodrigues公式转换为矩阵
chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
cout << "solve pnp in opencv cost time: " << time_used.count() << " seconds." << endl;

cout << "R=" << endl << R << endl;
cout << "t=" << endl << t << endl;

//光束法平差
typedef vector<Eigen::Vector2d, Eigen::aligned_allocator<Eigen::Vector2d>> VecVector2d;
typedef vector<Eigen::Vector3d, Eigen::aligned_allocator<Eigen::Vector3d>> VecVector3d;

//先转换为eigen的矩阵形式
VecVector3d pts_3d_eigen;
VecVector2d pts_2d_eigen;
for (size_t i = 0; i < pts_3d.size(); ++i) {
    pts_3d_eigen.push_back(Eigen::Vector3d(pts_3d[i].x, pts_3d[i].y, pts_3d[i].z));
    pts_2d_eigen.push_back(Eigen::Vector2d(pts_2d[i].x, pts_2d[i].y));
}

//通过自己手写的高斯牛顿法优化以及通过g2o优化
cout << "calling bundle adjustment by gauss newton" << endl;
Sophus::SE3d pose_gn;
t1 = chrono::steady_clock::now();
bundleAdjustmentGaussNewton(pts_3d_eigen, pts_2d_eigen, K, pose_gn);
t2 = chrono::steady_clock::now();
time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
cout << "solve pnp by gauss newton cost time: " << time_used.count() << " seconds." << endl;

cout << "calling bundle adjustment by g2o" << endl;
Sophus::SE3d pose_g2o;
t1 = chrono::steady_clock::now();
bundleAdjustmentG2O(pts_3d_eigen, pts_2d_eigen, K, pose_g2o);
t2 = chrono::steady_clock::now();
time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
cout << "solve pnp by g2o cost time: " << time_used.count() << " seconds." << endl;
```

上面就是主函数的内容，运行逻辑已经很清晰了，现在重点看bundleAdjustmentGaussNewton以及bundleAdjustmentG2O两个函数。

```C++
void bundleAdjustmentGaussNewton(
  const VecVector3d &points_3d,
  const VecVector2d &points_2d,
  const Mat &K,
  Sophus::SE3d &pose) {
  typedef Eigen::Matrix<double, 6, 1> Vector6d;
  const int iterations = 10;
  double cost = 0, lastCost = 0;
  double fx = K.at<double>(0, 0);
  double fy = K.at<double>(1, 1);
  double cx = K.at<double>(0, 2);
  double cy = K.at<double>(1, 2);

  for (int iter = 0; iter < iterations; iter++) {
    //用于保存H=J*J.Transpose()
    Eigen::Matrix<double, 6, 6> H = Eigen::Matrix<double, 6, 6>::Zero();
    Vector6d g = Vector6d::Zero();

    cost = 0;
    // compute cost
    for (int i = 0; i < points_3d.size(); i++) {
      Eigen::Vector3d pc = pose * points_3d[i];//计算当前位姿估计下的相机坐标系下的坐标
      double inv_z = 1.0 / pc[2];//1/z
      double inv_z2 = inv_z * inv_z;//1/z^2
        
      Eigen::Vector2d proj(fx * pc[0] / pc[2] + cx, fy * pc[1] / pc[2] + cy);//投影到像素平面
      Eigen::Vector2d e = points_2d[i] - proj;//重投影误差

      cost += e.squaredNorm();//平方后归一化
      //关于位姿优化的雅可比矩阵，正如我们刚才的推导
      Eigen::Matrix<double, 2, 6> J;
      J << -fx * inv_z,
        0,
        fx * pc[0] * inv_z2,
        fx * pc[0] * pc[1] * inv_z2,
        -fx - fx * pc[0] * pc[0] * inv_z2,
        fx * pc[1] * inv_z,
        0,
        -fy * inv_z,
        fy * pc[1] * inv_z2,
        fy + fy * pc[1] * pc[1] * inv_z2,
        -fy * pc[0] * pc[1] * inv_z2,
        -fy * pc[0] * inv_z;

      H += J.transpose() * J;
      g += -J.transpose() * e;
    }
	
    //求解增量方程
    Vector6d dx;
    dx = H.ldlt().solve(g);

    //如果增量不为NaN或cost收敛,结束迭代过程
    if (isnan(dx[0])) {
      cout << "result is nan!" << endl;
      break;
    }
    if (iter > 0 && cost >= lastCost) {
      cout << "cost: " << cost << ", last cost: " << lastCost << endl;
      break;
    }

    // 左乘更新位姿
    pose = Sophus::SE3d::exp(dx) * pose;
    lastCost = cost;

    cout << "iteration " << iter << " cost=" << std::setprecision(12) << cost << endl;
    if (dx.norm() < 1e-6) {
      // converge
      break;
    }
  }

  cout << "pose by g-n: \n" << pose.matrix() << endl;
}
```

```C++
/// 定义顶点 优化变量维度为6 类型为变换矩阵群Sophus::SE3d()
class VertexPose : public g2o::BaseVertex<6, Sophus::SE3d> {
public:
  EIGEN_MAKE_ALIGNED_OPERATOR_NEW;

  // 定义初值
  virtual void setToOriginImpl() override {
    _estimate = Sophus::SE3d();
  }

  // 定义更新规则
  virtual void oplusImpl(const double *update) override {
    Eigen::Matrix<double, 6, 1> update_eigen;
    update_eigen << update[0], update[1], update[2], update[3], update[4], update[5];
    _estimate = Sophus::SE3d::exp(update_eigen) * _estimate;
  }

  virtual bool read(istream &in) override {}

  virtual bool write(ostream &out) const override {}
};

//定义边 观测变量维度为2 类型为 Eigen::Vector2d,连接的顶点类型为VertexPose
class EdgeProjection : public g2o::BaseUnaryEdge<2, Eigen::Vector2d, VertexPose> {
public:
  EIGEN_MAKE_ALIGNED_OPERATOR_NEW;

  EdgeProjection(const Eigen::Vector3d &pos, const Eigen::Matrix3d &K) : _pos3d(pos), _K(K) {}

  //误差的计算方式
  virtual void computeError() override {
    const VertexPose *v = static_cast<VertexPose *> (_vertices[0]);
    Sophus::SE3d T = v->estimate();//顶点此时的优化变量值
    //误差项计算
    Eigen::Vector3d pos_pixel = _K * (T * _pos3d);
    pos_pixel /= pos_pixel[2];
    _error = _measurement - pos_pixel.head<2>();
  }

  //雅可比矩阵的计算方式
  virtual void linearizeOplus() override {
    const VertexPose *v = static_cast<VertexPose *> (_vertices[0]);
    Sophus::SE3d T = v->estimate();//顶点此时的优化变量值
    Eigen::Vector3d pos_cam = T * _pos3d;//当前位姿估计下的相机坐标系下的坐标
    //计算雅可比矩阵，与我们刚才推导的一样
    double fx = _K(0, 0);
    double fy = _K(1, 1);
    double cx = _K(0, 2);
    double cy = _K(1, 2);
    double X = pos_cam[0];
    double Y = pos_cam[1];
    double Z = pos_cam[2];
    double Z2 = Z * Z;
    _jacobianOplusXi
      << -fx / Z, 0, fx * X / Z2, fx * X * Y / Z2, -fx - fx * X * X / Z2, fx * Y / Z,
      0, -fy / Z, fy * Y / (Z * Z), fy + fy * Y * Y / Z2, -fy * X * Y / Z2, -fy * X / Z;
  }

  virtual bool read(istream &in) override {}

  virtual bool write(ostream &out) const override {}

private:
  Eigen::Vector3d _pos3d;
  Eigen::Matrix3d _K;
};

void bundleAdjustmentG2O(
  const VecVector3d &points_3d,
  const VecVector2d &points_2d,
  const Mat &K,
  Sophus::SE3d &pose) {

  // 构建图优化，先设定g2o
  typedef g2o::BlockSolver<g2o::BlockSolverTraits<6, 3>> BlockSolverType;  // pose is 6, landmark is 3
  typedef g2o::LinearSolverDense<BlockSolverType::PoseMatrixType> LinearSolverType; // 线性求解器类型
  // 梯度下降方法，可以从GN, LM, DogLeg 中选
  auto solver = new g2o::OptimizationAlgorithmGaussNewton(
    g2o::make_unique<BlockSolverType>(g2o::make_unique<LinearSolverType>()));
  g2o::SparseOptimizer optimizer;     // 图模型
  optimizer.setAlgorithm(solver);   // 设置求解器
  optimizer.setVerbose(true);       // 打开调试输出

  // 构造顶点
  VertexPose *vertex_pose = new VertexPose(); // camera vertex_pose
  vertex_pose->setId(0);
  vertex_pose->setEstimate(Sophus::SE3d());
  optimizer.addVertex(vertex_pose);

  // 导入内参矩阵
  Eigen::Matrix3d K_eigen;
  K_eigen <<
          K.at<double>(0, 0), K.at<double>(0, 1), K.at<double>(0, 2),
    K.at<double>(1, 0), K.at<double>(1, 1), K.at<double>(1, 2),
    K.at<double>(2, 0), K.at<double>(2, 1), K.at<double>(2, 2);

  // 构造边
  int index = 1;
  for (size_t i = 0; i < points_2d.size(); ++i) {
    auto p2d = points_2d[i];
    auto p3d = points_3d[i];
    EdgeProjection *edge = new EdgeProjection(p3d, K_eigen);
    edge->setId(index);
    edge->setVertex(0, vertex_pose);
    edge->setMeasurement(p2d);
    edge->setInformation(Eigen::Matrix2d::Identity());
    optimizer.addEdge(edge);
    index++;
  }
  //启动优化
  chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
  optimizer.setVerbose(true);
  optimizer.initializeOptimization();
  optimizer.optimize(10);
  chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
  chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
  cout << "optimization costs time: " << time_used.count() << " seconds." << endl;
  cout << "pose estimated by g2o =\n" << vertex_pose->estimate().matrix() << endl;
  pose = vertex_pose->estimate();
}
```

除此之外，我们还可以用Ceres进行这个最小二乘问题的求解，后续工程章节我会同时使用Ceres进行优化部分的编写。鉴于目前Ceres文档较为齐全，我认为使用Ceres还是更有优势一些，毕竟谷歌做的雷达定位建图的Cartographer使用的就是Ceres，目测性能也不差。

接下来我们来看一下运行结果

![06-17](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-17.png)

![06-18](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-18.png)

可以看出来，通过OpenCV解PnP的时间是手写高斯牛顿法的两倍，手写高斯牛顿法的收敛速度很快。g2o的用时更久一些。从结果来看基本是一致的。对于PnP问题的光束平差法，我们可以自然而然的想到多张图的重投影也可以一起优化，这在后端中将会介绍。



### 3D-3D：ICP

我们最后再介绍3D-3D的位姿估计问题，首先考虑两个已经匹配好的3D点集
$$
P=\{p_1,p_2,...,p_n\}(来自后一帧),P'=\{p_1',p_2',...,p_n'\}(来自前一帧)
$$
现在，我们的目标是找到一个欧式变换R,t，使得下面的误差项范数平方和最小
$$
\sum_{i=1}^n||e_i||_2^2=\sum_{i=1}^n||(p_i-(Rp_i'+t))||_2^2
$$
解决这个问题有两种方法：**SVD方法**以及**非线性优化方法**。在这里我主要介绍非线性优化方法，简要介绍SVD方法。

#### SVD方法

首先，定义两组点的质心，并计算去质心坐标
$$
p=\frac{1}{n}\sum_{i=1}^np_i,\;p'=\frac{1}{n}\sum_{i=1}^np_i'\\
q_i=p_i-p,\;q_i'=p_i'-p'
$$
再根据以下最小二乘问题计算旋转矩阵R
$$
R^*=arg\min_R\frac{1}{2}\sum_{i=1}^n||q_i-Rq_i'||
$$
再根据第二步计算的R求解t
$$
t^*=p-Rp'
$$
具体的解析步骤在此不展开，感兴趣的读者可以自行查阅相关资料。需要说明的是，SVD方法可以直接给出ICP问题的解析解，也就是说我们未必需要采用迭代优化的方式进行求解。

#### 非线性优化方法

构建最小二乘问题
$$
\min_{\xi}\frac{1}{2}\sum_{i=1}^n||p_i-\exp(\xi^{\wedge})p_i'||_2^2
$$
使用李代数左乘扰动模型，得到其一阶导数（雅可比矩阵）
$$
\frac{\partial e}{\partial \delta\xi}=\begin{bmatrix}I&(-Rp+t)^{\wedge}\\0^T&0^T\end{bmatrix}
$$
ICP问题有一个特殊的性质，其有唯一解时必是全局最小值解，尽管其有可能出现无穷多组解。因此，ICP问题的初值并没有那么重要。

#### 实践模块5-求解ICP

直接分析代码（解释都在注释当中）

```C++
//-- 读取图像
Mat img_1 = imread(argv[1], CV_LOAD_IMAGE_COLOR);
Mat img_2 = imread(argv[2], CV_LOAD_IMAGE_COLOR);

vector<KeyPoint> keypoints_1, keypoints_2;
vector<DMatch> matches;
find_feature_matches(img_1, img_2, keypoints_1, keypoints_2, matches);
cout << "一共找到了" << matches.size() << "组匹配点" << endl;

// 建立3D点，与PnP的代码如出一辙
Mat depth1 = imread(argv[3], CV_LOAD_IMAGE_UNCHANGED);       // 深度图为16位无符号数，单通道图像
Mat depth2 = imread(argv[4], CV_LOAD_IMAGE_UNCHANGED);       // 深度图为16位无符号数，单通道图像
Mat K = (Mat_<double>(3, 3) << 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);
vector<Point3f> pts1, pts2;

for (DMatch m:matches) {
    ushort d1 = depth1.ptr<unsigned short>(int(keypoints_1[m.queryIdx].pt.y))[int(keypoints_1[m.queryIdx].pt.x)];
    ushort d2 = depth2.ptr<unsigned short>(int(keypoints_2[m.trainIdx].pt.y))[int(keypoints_2[m.trainIdx].pt.x)];
    if (d1 == 0 || d2 == 0)   // bad depth
        continue;
    Point2d p1 = pixel2cam(keypoints_1[m.queryIdx].pt, K);
    Point2d p2 = pixel2cam(keypoints_2[m.trainIdx].pt, K);
    float dd1 = float(d1) / 5000.0;
    float dd2 = float(d2) / 5000.0;
    pts1.push_back(Point3f(p1.x * dd1, p1.y * dd1, dd1));
    pts2.push_back(Point3f(p2.x * dd2, p2.y * dd2, dd2));
}

cout << "3d-3d pairs: " << pts1.size() << endl;
Mat R, t;
//根据3D-3D点对通过SVD方法解决ICP问题
pose_estimation_3d3d(pts1, pts2, R, t);
cout << "ICP via SVD results: " << endl;
cout << "R = " << R << endl;
cout << "t = " << t << endl;
cout << "R_inv = " << R.t() << endl;
cout << "t_inv = " << -R.t() * t << endl;

cout << "calling bundle adjustment" << endl;
//通过g2o进行非线性优化解决ICP问题
bundleAdjustment(pts1, pts2, R, t);

// 验证计算的位姿变换是否正确
for (int i = 0; i < 5; i++) {
    cout << "p1 = " << pts1[i] << endl;
    cout << "p2 = " << pts2[i] << endl;
    cout << "(R*p2+t) = " <<
        R * (Mat_<double>(3, 1) << pts2[i].x, pts2[i].y, pts2[i].z) + t
        << endl;
    cout << endl;
}
```

SVD方法的具体步骤如下

```C++
void pose_estimation_3d3d(const vector<Point3f> &pts1,
                          const vector<Point3f> &pts2,
                          Mat &R, Mat &t) {
  //去质心
  Point3f p1, p2;     
  int N = pts1.size();
  for (int i = 0; i < N; i++) {
    p1 += pts1[i];
    p2 += pts2[i];
  }
  p1 = Point3f(Vec3f(p1) / N);
  p2 = Point3f(Vec3f(p2) / N);
  vector<Point3f> q1(N), q2(N); 
  for (int i = 0; i < N; i++) {
    q1[i] = pts1[i] - p1;
    q2[i] = pts2[i] - p2;
  }

  // 旋转矩阵R的计算，涉及SVD分解
  Eigen::Matrix3d W = Eigen::Matrix3d::Zero();
  for (int i = 0; i < N; i++) {
    W += Eigen::Vector3d(q1[i].x, q1[i].y, q1[i].z) * Eigen::Vector3d(q2[i].x, q2[i].y, q2[i].z).transpose();
  }
  cout << "W=" << W << endl;

  // SVD分解
  Eigen::JacobiSVD<Eigen::Matrix3d> svd(W, Eigen::ComputeFullU | Eigen::ComputeFullV);
  Eigen::Matrix3d U = svd.matrixU();
  Eigen::Matrix3d V = svd.matrixV();

  cout << "U=" << U << endl;
  cout << "V=" << V << endl;

  Eigen::Matrix3d R_ = U * (V.transpose());
  if (R_.determinant() < 0) {
    R_ = -R_;
  }
  //根据计算出的R来计算t
  Eigen::Vector3d t_ = Eigen::Vector3d(p1.x, p1.y, p1.z) - R_ * Eigen::Vector3d(p2.x, p2.y, p2.z);

  // 转换为cv::Mat格式
  R = (Mat_<double>(3, 3) <<
    R_(0, 0), R_(0, 1), R_(0, 2),
    R_(1, 0), R_(1, 1), R_(1, 2),
    R_(2, 0), R_(2, 1), R_(2, 2)
  );
  t = (Mat_<double>(3, 1) << t_(0, 0), t_(1, 0), t_(2, 0));
}
```

利用g2o的非线性优化方法如下

```C++
/// 定义顶点 与PnP的模型是一致的
class VertexPose : public g2o::BaseVertex<6, Sophus::SE3d> {
public:
  EIGEN_MAKE_ALIGNED_OPERATOR_NEW;

  virtual void setToOriginImpl() override {
    _estimate = Sophus::SE3d();
  }

  /// left multiplication on SE3
  virtual void oplusImpl(const double *update) override {
    Eigen::Matrix<double, 6, 1> update_eigen;
    update_eigen << update[0], update[1], update[2], update[3], update[4], update[5];
    _estimate = Sophus::SE3d::exp(update_eigen) * _estimate;
  }

  virtual bool read(istream &in) override {}

  virtual bool write(ostream &out) const override {}
};

/// 定义边 观测变量为3维，类型Eigen::Vector3d，连接顶点
class EdgeProjectXYZRGBDPoseOnly : public g2o::BaseUnaryEdge<3, Eigen::Vector3d, VertexPose> {
public:
  EIGEN_MAKE_ALIGNED_OPERATOR_NEW;

  EdgeProjectXYZRGBDPoseOnly(const Eigen::Vector3d &point) : _point(point) {}

  //误差计算方式
  virtual void computeError() override {
    const VertexPose *pose = static_cast<const VertexPose *> ( _vertices[0] );
    _error = _measurement - pose->estimate() * _point;
  }

  //雅可比矩阵计算方式
  virtual void linearizeOplus() override {
    VertexPose *pose = static_cast<VertexPose *>(_vertices[0]);
    Sophus::SE3d T = pose->estimate();
    Eigen::Vector3d xyz_trans = T * _point;
    _jacobianOplusXi.block<3, 3>(0, 0) = -Eigen::Matrix3d::Identity();
    _jacobianOplusXi.block<3, 3>(0, 3) = Sophus::SO3d::hat(xyz_trans);
  }

  bool read(istream &in) {}

  bool write(ostream &out) const {}

protected:
  Eigen::Vector3d _point;
};

void bundleAdjustment(
  const vector<Point3f> &pts1,
  const vector<Point3f> &pts2,
  Mat &R, Mat &t) {
  // 构建图优化，先设定g2o
  typedef g2o::BlockSolverX BlockSolverType;
  typedef g2o::LinearSolverDense<BlockSolverType::PoseMatrixType> LinearSolverType; // 线性求解器类型
  // 梯度下降方法，可以从GN, LM, DogLeg 中选
  auto solver = new g2o::OptimizationAlgorithmLevenberg(
    g2o::make_unique<BlockSolverType>(g2o::make_unique<LinearSolverType>()));
  g2o::SparseOptimizer optimizer;     // 图模型
  optimizer.setAlgorithm(solver);   // 设置求解器
  optimizer.setVerbose(true);       // 打开调试输出

  // 添加顶点
  VertexPose *pose = new VertexPose(); // 相机位姿
  pose->setId(0);
  pose->setEstimate(Sophus::SE3d());
  optimizer.addVertex(pose);

  // 添加边
  for (size_t i = 0; i < pts1.size(); i++) {
    EdgeProjectXYZRGBDPoseOnly *edge = new EdgeProjectXYZRGBDPoseOnly(
      Eigen::Vector3d(pts2[i].x, pts2[i].y, pts2[i].z));
    edge->setVertex(0, pose);
    edge->setMeasurement(Eigen::Vector3d(
      pts1[i].x, pts1[i].y, pts1[i].z));
    edge->setInformation(Eigen::Matrix3d::Identity());
    optimizer.addEdge(edge);
  }

  //启动优化
  chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
  optimizer.initializeOptimization();
  optimizer.optimize(10);
  chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
  chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
  cout << "optimization costs time: " << time_used.count() << " seconds." << endl;

  cout << endl << "after optimization:" << endl;
  cout << "T=\n" << pose->estimate().matrix() << endl;

  //转换为 cv::Mat
  Eigen::Matrix3d R_ = pose->estimate().rotationMatrix();
  Eigen::Vector3d t_ = pose->estimate().translation();
  R = (Mat_<double>(3, 3) <<
    R_(0, 0), R_(0, 1), R_(0, 2),
    R_(1, 0), R_(1, 1), R_(1, 2),
    R_(2, 0), R_(2, 1), R_(2, 2)
  );
  t = (Mat_<double>(3, 1) << t_(0, 0), t_(1, 0), t_(2, 0));
}
```

最终，我们来看一下结果。首先是SVD方法的结果

![06-19](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-19.png)

然后再看看非线性优化方法的结果

![06-20](C:\Users\legions\Desktop\research_notes\视觉SLAM十四讲\img\06-20.png)

可以看出两种方法给出的结果是基本一致的，理论上SVD分解的值可以作为真值。



到此，本讲总算是完结了。这一讲结合了三维空间刚体运动，李群与李代数，相机与图像，非线性优化四讲的内容，为解决前端视觉里程计的相机运动估计与空间点位置估计问题，考虑了多种不同传感器给出的观测数据的情况，分别通过对极几何与三角测量解决了2D-2D点对的运动估计和空间点位置估计，通过解决PnP问题解决了3D-2D点对的运动估计问题，通过解决ICP问题解决了3D-3D点对的运动估计问题。本讲包含的知识点多，与前置内容的关系较多，且篇幅较长。如果第一次阅读并没有完全理解，建议进行一定实践后再次阅读。
